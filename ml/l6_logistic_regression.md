# Logistic Regression

Machine learning algorithms can be (roughly) categorized into two categories:

- **Generative** algorithms, that estimate <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i,&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i,&space;y)" title="P(\mathbf{x}_i, y)" /></a> (often they model <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i&space;\vert&space;y)" title="P(\mathbf{x}_i \vert y)" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=P(y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y)" title="P(y)" /></a> separately).
- **Discriminative** algorithms, that model <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x}_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x}_i)" title="P(y \vert \mathbf{x}_i)" /></a>

The Naive Bayes algorithm is **generative**. It models <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i&space;\vert&space;y)" title="P(\mathbf{x}_i \vert y)" /></a> and makes explicit assumptions on its distribution (e.g. multinomial, categorical, Gaussian, ...). The parameters of this distributions are estimated with MLE or MAP. We showed previously that for the Gaussian Naive Bayes <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" title="P(y \vert \mathbf{x}_i)=\frac{1}{1+e^{-y(\mathbf{w}^T \mathbf{x}_i + b)}}" /></a> for <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;\in&space;\{&space;&plus;1,&space;-1&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;\in&space;\{&space;&plus;1,&space;-1&space;\}" title="y \in \{ +1, -1 \}" /></a> for specific vectors <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b" title="b" /></a> that are uniquely determined through the particular choice of <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i&space;\vert&space;y)" title="P(\mathbf{x}_i \vert y)" /></a>.

Logistic Regression is often referred to as the **discriminative** counterpart of Naive Bayes. Here, we model <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x}_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x}_i)" title="P(y \vert \mathbf{x}_i)" /></a> and assume that it takes on exactly this form

<a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" title="P(y \vert \mathbf{x}_i)=\frac{1}{1+e^{-y(\mathbf{w}^T \mathbf{x}_i + b)}}" /></a>.

We make little assumptions on <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i&space;\vert&space;y)" title="P(\mathbf{x}_i \vert y)" /></a>, e.g. it could be Gaussian or Multinomial. Ultimately it doesn't matter, because we estimate the vector <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b" title="b" /></a> directly with MLE or MAP to maximize the conditional likelihood of <a href="https://www.codecogs.com/eqnedit.php?latex=\prod_i&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;;&space;\mathbf{w},&space;b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\prod_i&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;;&space;\mathbf{w},&space;b)" title="\prod_i P(y_i \vert \mathbf{x}_i ; \mathbf{w}, b)" /></a>.

We can absorbed the parameter <a href="https://www.codecogs.com/eqnedit.php?latex=b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b" title="b" /></a> into <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> through an additional constant dimension.

## Maximum Likelihood Estimate (MLE)

In MLE we choose parrameters that **maximize the conditional likelihood**. The conditional data likelihood <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})" title="P(\mathbf{y} \vert X, \mathbf{w})" /></a> is the probability of the observed value <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{y}&space;\in&space;\mathbb{R}^{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{y}&space;\in&space;\mathbb{R}^{n}" title="\mathbf{y} \in \mathbb{R}^{n}" /></a> in the training data conditioned on the feature value <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a>. Note that <a href="https://www.codecogs.com/eqnedit.php?latex=X&space;=&space;[\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_i,&space;\ldots,&space;\mathbf{x}_n]&space;\in&space;\mathbb{R}^{d&space;\times&space;n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X&space;=&space;[\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_i,&space;\ldots,&space;\mathbf{x}_n]&space;\in&space;\mathbb{R}^{d&space;\times&space;n}" title="X = [\mathbf{x}_1, \ldots, \mathbf{x}_i, \ldots, \mathbf{x}_n] \in \mathbb{R}^{d \times n}" /></a>. We choose the parameters that maximize this function and we assume that the <a href="https://www.codecogs.com/eqnedit.php?latex=y_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i" title="y_i" /></a>'s are independent given the input features <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>. So,

<a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})&space;=&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i,&space;\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})&space;=&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i,&space;\mathbf{w})" title="P(\mathbf{y} \vert X, \mathbf{w}) = \prod_{i=1}^{n} P(y_i \vert \mathbf{x}_i, \mathbf{w})" /></a>.

Now if we take the log, we obtain

<a href="https://www.codecogs.com/eqnedit.php?latex=\log&space;\Big(\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i,&space;\mathbf{w})\Big)=-\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\log&space;\Big(\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i,&space;\mathbf{w})\Big)=-\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" title="\log \Big(\prod_{i=1}^{n} P(y_i \vert \mathbf{x}_i, \mathbf{w})\Big)=-\sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i})" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{\mathbf{w}}_{MLE}=\operatorname*{argmax}_{\mathbf{w}}&space;\Big(-&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})\Big)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{\mathbf{w}}_{MLE}=\operatorname*{argmax}_{\mathbf{w}}&space;\Big(-&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})\Big)" title="\hat{\mathbf{w}}_{MLE}=\operatorname*{argmax}_{\mathbf{w}} \Big(- \sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i})\Big)" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\operatorname*{argmin}_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\operatorname*{argmin}_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" title="\operatorname*{argmin}_{\mathbf{w}} \sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i})" /></a>

We need to estimate the parameter <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>. To find the values of the parameters at minimum, we can try to find solutions for <a href="https://www.codecogs.com/eqnedit.php?latex=\nabla_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\nabla_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})&space;=&space;0" title="\nabla_{\mathbf{w}} \sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i}) = 0" /></a>. This equation has no closed form solution, so we use **Gradient Descent** on the **negative log likelihood** <a href="https://www.codecogs.com/eqnedit.php?latex=l(\mathbf{w})=\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\mathbf{w})=\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" title="l(\mathbf{w})=\sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i})" /></a>.








