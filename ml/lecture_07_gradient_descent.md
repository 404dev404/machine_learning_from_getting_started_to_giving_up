# Gradient Descent

We want to minimize a *convex*, *continuous* and *differentiable* loss function <a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(w)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(w)" title="\ell (w)" /></a>. We will discuss two of the most popular "hill-climbing" algorithms, gradient descent and Newton's method.

**Algorithm**:

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;\text{Initialize&space;}&space;\textbf{w}_0&space;\\&space;&&space;\text{Repeat&space;until&space;converge:}&space;\\&space;&&space;\quad&space;\mathbf{w}^{t&plus;1}&space;=&space;\mathbf{w}^t&space;&plus;&space;\mathbf{s}&space;\\&space;&&space;\quad&space;\text{If&space;}&space;\|&space;\mathbf{w}^{t&plus;1}&space;-&space;\mathbf{w}^t&space;\|_2&space;<&space;\epsilon&space;\text{,&space;\textbf{converged}!}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;&&space;\text{Initialize&space;}&space;\textbf{w}_0&space;\\&space;&&space;\text{Repeat&space;until&space;converge:}&space;\\&space;&&space;\quad&space;\mathbf{w}^{t&plus;1}&space;=&space;\mathbf{w}^t&space;&plus;&space;\mathbf{s}&space;\\&space;&&space;\quad&space;\text{If&space;}&space;\|&space;\mathbf{w}^{t&plus;1}&space;-&space;\mathbf{w}^t&space;\|_2&space;<&space;\epsilon&space;\text{,&space;\textbf{converged}!}&space;\end{align*}" title="\begin{align*} & \text{Initialize } \textbf{w}_0 \\ & \text{Repeat until converge:} \\ & \quad \mathbf{w}^{t+1} = \mathbf{w}^t + \mathbf{s} \\ & \quad \text{If } \| \mathbf{w}^{t+1} - \mathbf{w}^t \|_2 < \epsilon \text{, \textbf{converged}!} \end{align*}" /></a>

## Trick: Taylor Expansion

How can you minimize a function <a href="https://www.codecogs.com/eqnedit.php?latex=\ell" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell" title="\ell" /></a> if you don't know much about it? The trick is to assume it is much simpler than it really is. This can be done with Taylor's approximation. Provided that the norm <a href="https://www.codecogs.com/eqnedit.php?latex=\|&space;\mathbf{s}&space;\|_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\|&space;\mathbf{s}&space;\|_2" title="\| \mathbf{s} \|_2" /></a> is small (i.e. <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}&space;&plus;&space;\mathbf{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}&space;&plus;&space;\mathbf{s}" title="\mathbf{w} + \mathbf{s}" /></a> is very close to <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>), we can approximate the function <a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})" title="\ell (\mathbf{w} + \mathbf{s})" /></a> by its first and second derivatives:

<a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})&space;\approx&space;\ell&space;(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}&space;\\&space;\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})&space;\approx&space;\ell&space;(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}&space;&plus;&space;\frac{1}{2}&space;\mathbf{s}^{\top}&space;H&space;(\mathbf{w})&space;\mathbf{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})&space;\approx&space;\ell&space;(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}&space;\\&space;\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})&space;\approx&space;\ell&space;(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}&space;&plus;&space;\frac{1}{2}&space;\mathbf{s}^{\top}&space;H&space;(\mathbf{w})&space;\mathbf{s}" title="\ell (\mathbf{w} + \mathbf{s}) \approx \ell (\mathbf{w}) + g(\mathbf{w})^{\top} \mathbf{s} \\ \ell (\mathbf{w} + \mathbf{s}) \approx \ell (\mathbf{w}) + g(\mathbf{w})^{\top} \mathbf{s} + \frac{1}{2} \mathbf{s}^{\top} H (\mathbf{w}) \mathbf{s}" /></a>

Here, <a href="https://www.codecogs.com/eqnedit.php?latex=g(\mathbf{w})&space;=&space;\nabla&space;\ell&space;(\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?g(\mathbf{w})&space;=&space;\nabla&space;\ell&space;(\mathbf{w})" title="g(\mathbf{w}) = \nabla \ell (\mathbf{w})" /></a> is the gradient and <a href="https://www.codecogs.com/eqnedit.php?latex=H(\mathbf{w})&space;=&space;\nabla^2&space;\ell&space;(\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H(\mathbf{w})&space;=&space;\nabla^2&space;\ell&space;(\mathbf{w})" title="H(\mathbf{w}) = \nabla^2 \ell (\mathbf{w})" /></a> is the Hessian of <a href="https://www.codecogs.com/eqnedit.php?latex=\ell" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell" title="\ell" /></a>. Both approximations are valid if <a href="https://www.codecogs.com/eqnedit.php?latex=\|&space;\mathbf{s}&space;\|_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\|&space;\mathbf{s}&space;\|_2" title="\| \mathbf{s} \|_2" /></a> is small, but the second one assumes that <a href="https://www.codecogs.com/eqnedit.php?latex=\ell" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell" title="\ell" /></a> is *twice* differentiable and is more expensive to compute but also more accurate than only using gradient.

# Gradient Descent: Use the First Order Approximation

In gradient descent we only use the gradient (first order). In other words, we assume that the function <a href="https://www.codecogs.com/eqnedit.php?latex=\ell" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell" title="\ell" /></a> around <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> is linear and behaves like <a href="https://www.codecogs.com/eqnedit.php?latex=\ell(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}" title="\ell(\mathbf{w}) + g(\mathbf{w})^{\top} \mathbf{s}" /></a>. Our goal is to find a vector <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{s}" title="\mathbf{s}" /></a> that minimizes this function. In steepest descent we simply set

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{s}&space;=&space;-&space;\alpha&space;g(\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{s}&space;=&space;-&space;\alpha&space;g(\mathbf{w})" title="\mathbf{s} = - \alpha g(\mathbf{w})" /></a>,

for some small <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha&space;>&space;0" title="\alpha > 0" /></a>. It is straight-forward to prove that in this case <a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})&space;<&space;\ell&space;(\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})&space;<&space;\ell&space;(\mathbf{w})" title="\ell (\mathbf{w} + \mathbf{s}) < \ell (\mathbf{w})" /></a>.

<a href="https://www.codecogs.com/eqnedit.php?latex=\underbrace{\ell&space;(\mathbf{w}&space;&plus;&space;(-\alpha&space;g(\mathbf{w})))}_{\text{after&space;one&space;update}}&space;\approx&space;\ell&space;(\mathbf{w})&space;-&space;\underbrace{\alpha\overbrace{g(\mathbf{w})^{\top}&space;g(\mathbf{w})}^{>0}}_{>0}&space;<&space;\underbrace{\ell&space;(\mathbf{w})}_{\text{before}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\underbrace{\ell&space;(\mathbf{w}&space;&plus;&space;(-\alpha&space;g(\mathbf{w})))}_{\text{after&space;one&space;update}}&space;\approx&space;\ell&space;(\mathbf{w})&space;-&space;\underbrace{\alpha\overbrace{g(\mathbf{w})^{\top}&space;g(\mathbf{w})}^{>0}}_{>0}&space;<&space;\underbrace{\ell&space;(\mathbf{w})}_{\text{before}}" title="\underbrace{\ell (\mathbf{w} + (-\alpha g(\mathbf{w})))}_{\text{after one update}} \approx \ell (\mathbf{w}) - \underbrace{\alpha\overbrace{g(\mathbf{w})^{\top} g(\mathbf{w})}^{>0}}_{>0} < \underbrace{\ell (\mathbf{w})}_{\text{before}}" /></a>

Setting the **learning rate** <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha&space;>&space;0" title="\alpha > 0" /></a> is  a dark art. Only if it is sufficiently small will gradient descent converge. If it is too large the algorithm can easily *diverge* out of control. A safe (but sometimes slow) choice is to set <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha&space;=&space;\frac{t_0}{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha&space;=&space;\frac{t_0}{t}" title="\alpha = \frac{t_0}{t}" /></a>, which guarantees that it will eventually become small enough to converge (for any initial value <a href="https://www.codecogs.com/eqnedit.php?latex=t_0&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?t_0&space;>&space;0" title="t_0 > 0" /></a>).

# Adagrad

One option is to set the step-size adaptively for *every feature*. **Adagrad** keeps a running average of the squared gradient magnitude and sets a small learning rate for features that have large gradients, and a large learning rate for features with small gradients. Setting different learning rates for different features is particularly important if they are of different scale or vary in frequency. For example, word counts can differ a lot across common words and rare words.

**Adagrad Algorithm**
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;\text{Initialize&space;}&space;\mathbf{w}_0&space;\text{&space;and&space;}&space;\mathbf{z}&space;\text{:&space;}&space;\forall&space;d&space;\text{:&space;}&space;w_d^0&space;=&space;0&space;\text{&space;and&space;}&space;z_d&space;=&space;0&space;\\&space;&&space;\text{Repeat&space;until&space;converge:}&space;\\&space;&&space;\quad&space;\mathbf{g}&space;=&space;\frac{\partial&space;f(\mathbf{w})}{\partial&space;\mathbf{w}}&space;\text{&space;\&hash;&space;compute&space;gradient}&space;\\&space;&&space;\quad&space;\forall&space;d&space;\text{:&space;}&space;z_d&space;\leftarrow&space;z_d&space;&plus;&space;g_d^2&space;\\&space;&&space;\quad&space;\forall&space;d&space;\text{:&space;}&space;w_d^{t&plus;1}&space;\leftarrow&space;w_d^t&space;-&space;\alpha&space;\frac{g_d}{\sqrt{z_d&space;&plus;&space;\epsilon}}&space;\\&space;&&space;\quad&space;\text{If&space;}&space;\|&space;\mathbf{w}^{t&plus;1}&space;-&space;\mathbf{w}^{t}&space;\|_2&space;<&space;\delta&space;\text{,&space;\textbf{converged}!&space;\&hash;&space;for&space;some&space;small&space;}&space;\delta&space;>&space;0&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;&&space;\text{Initialize&space;}&space;\mathbf{w}_0&space;\text{&space;and&space;}&space;\mathbf{z}&space;\text{:&space;}&space;\forall&space;d&space;\text{:&space;}&space;w_d^0&space;=&space;0&space;\text{&space;and&space;}&space;z_d&space;=&space;0&space;\\&space;&&space;\text{Repeat&space;until&space;converge:}&space;\\&space;&&space;\quad&space;\mathbf{g}&space;=&space;\frac{\partial&space;f(\mathbf{w})}{\partial&space;\mathbf{w}}&space;\text{&space;\&hash;&space;compute&space;gradient}&space;\\&space;&&space;\quad&space;\forall&space;d&space;\text{:&space;}&space;z_d&space;\leftarrow&space;z_d&space;&plus;&space;g_d^2&space;\\&space;&&space;\quad&space;\forall&space;d&space;\text{:&space;}&space;w_d^{t&plus;1}&space;\leftarrow&space;w_d^t&space;-&space;\alpha&space;\frac{g_d}{\sqrt{z_d&space;&plus;&space;\epsilon}}&space;\\&space;&&space;\quad&space;\text{If&space;}&space;\|&space;\mathbf{w}^{t&plus;1}&space;-&space;\mathbf{w}^{t}&space;\|_2&space;<&space;\delta&space;\text{,&space;\textbf{converged}!&space;\&hash;&space;for&space;some&space;small&space;}&space;\delta&space;>&space;0&space;\end{align*}" title="\begin{align*} & \text{Initialize } \mathbf{w}_0 \text{ and } \mathbf{z} \text{: } \forall d \text{: } w_d^0 = 0 \text{ and } z_d = 0 \\ & \text{Repeat until converge:} \\ & \quad \mathbf{g} = \frac{\partial f(\mathbf{w})}{\partial \mathbf{w}} \text{ \# compute gradient} \\ & \quad \forall d \text{: } z_d \leftarrow z_d + g_d^2 \\ & \quad \forall d \text{: } w_d^{t+1} \leftarrow w_d^t - \alpha \frac{g_d}{\sqrt{z_d + \epsilon}} \\ & \quad \text{If } \| \mathbf{w}^{t+1} - \mathbf{w}^{t} \|_2 < \delta \text{, \textbf{converged}! \# for some small } \delta > 0 \end{align*}" /></a>

# Newton's Method: Use 2nd Order Approximation

Newton's method assumes that the loss <a href="https://www.codecogs.com/eqnedit.php?latex=\ell" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell" title="\ell" /></a> is *twice differentiable* and uses the approximation with Hessian (2nd order Taylor approximation). The **Hessian Matrix** contains all second order partial derivatives and is defined as

<a href="https://www.codecogs.com/eqnedit.php?latex=H(\mathbf{w})&space;=&space;\begin{pmatrix}&space;\frac{\partial^2&space;\ell}{\partial&space;w_1^2}&space;&&space;\frac{\partial^2&space;\ell}{\partial&space;w_1&space;\partial&space;w_2}&space;&&space;\cdots&space;&&space;\frac{\partial^2&space;\ell}{\partial&space;w_1&space;\partial&space;w_n}&space;\\&space;\vdots&space;&&space;\cdots&space;&&space;\cdots&space;&&space;\vdots&space;\\&space;\frac{\partial^2&space;\ell}{\partial&space;w_n&space;\partial&space;w_1}&space;&&space;\cdots&space;&&space;\cdots&space;&&space;\frac{\partial^2&space;\ell}{\partial&space;w_n^2}&space;\end{pmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H(\mathbf{w})&space;=&space;\begin{pmatrix}&space;\frac{\partial^2&space;\ell}{\partial&space;w_1^2}&space;&&space;\frac{\partial^2&space;\ell}{\partial&space;w_1&space;\partial&space;w_2}&space;&&space;\cdots&space;&&space;\frac{\partial^2&space;\ell}{\partial&space;w_1&space;\partial&space;w_n}&space;\\&space;\vdots&space;&&space;\cdots&space;&&space;\cdots&space;&&space;\vdots&space;\\&space;\frac{\partial^2&space;\ell}{\partial&space;w_n&space;\partial&space;w_1}&space;&&space;\cdots&space;&&space;\cdots&space;&&space;\frac{\partial^2&space;\ell}{\partial&space;w_n^2}&space;\end{pmatrix}" title="H(\mathbf{w}) = \begin{pmatrix} \frac{\partial^2 \ell}{\partial w_1^2} & \frac{\partial^2 \ell}{\partial w_1 \partial w_2} & \cdots & \frac{\partial^2 \ell}{\partial w_1 \partial w_n} \\ \vdots & \cdots & \cdots & \vdots \\ \frac{\partial^2 \ell}{\partial w_n \partial w_1} & \cdots & \cdots & \frac{\partial^2 \ell}{\partial w_n^2} \end{pmatrix}" /></a>,

and, because the convexity of <a href="https://www.codecogs.com/eqnedit.php?latex=\ell" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell" title="\ell" /></a>, it is always a symmetric square matrix and positive semi-definite.

*Note*: A symmetric matrix <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{M}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{M}" title="\mathbf{M}" /></a> is positive semi-definite if it has only non-negative eigenvalues or, equivalently, for any vector <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}" title="\mathbf{x}" /></a> we must have <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}^{\top}&space;\mathbf{M}&space;\mathbf{x}&space;\geq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}^{\top}&space;\mathbf{M}&space;\mathbf{x}&space;\geq&space;0" title="\mathbf{x}^{\top} \mathbf{M} \mathbf{x} \geq 0" /></a>.

It follows that the approximation

<a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})&space;\approx&space;\ell&space;(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}&space;&plus;&space;\frac{1}{2}&space;\mathbf{s}^{\top}&space;H(\mathbf{w})&space;\mathbf{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(\mathbf{w}&space;&plus;&space;\mathbf{s})&space;\approx&space;\ell&space;(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}&space;&plus;&space;\frac{1}{2}&space;\mathbf{s}^{\top}&space;H(\mathbf{w})&space;\mathbf{s}" title="\ell (\mathbf{w} + \mathbf{s}) \approx \ell (\mathbf{w}) + g(\mathbf{w})^{\top} \mathbf{s} + \frac{1}{2} \mathbf{s}^{\top} H(\mathbf{w}) \mathbf{s}" /></a>

describes a convex parabola, and we can find its minimum by solving the following optimization problem:

<a href="https://www.codecogs.com/eqnedit.php?latex=\operatorname*{argmin}_{s}&space;\ell&space;(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}&space;&plus;&space;\frac{1}{2}&space;\mathbf{s}^{\top}&space;H(\mathbf{w})&space;\mathbf{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\operatorname*{argmin}_{s}&space;\ell&space;(\mathbf{w})&space;&plus;&space;g(\mathbf{w})^{\top}&space;\mathbf{s}&space;&plus;&space;\frac{1}{2}&space;\mathbf{s}^{\top}&space;H(\mathbf{w})&space;\mathbf{s}" title="\operatorname*{argmin}_{s} \ell (\mathbf{w}) + g(\mathbf{w})^{\top} \mathbf{s} + \frac{1}{2} \mathbf{s}^{\top} H(\mathbf{w}) \mathbf{s}" /></a>

To find the minimum of the objective, we take its first derivative with respect to <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{s}" title="\mathbf{s}" /></a>, equate it with zero, and solve for <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{s}" title="\mathbf{s}" /></a>:

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;g(\mathbf{w})&space;&plus;&space;H(\mathbf{w})&space;\mathbf{s}&space;&&space;=&space;0&space;\\&space;\Rightarrow&space;\mathbf{s}&space;&&space;=&space;-&space;[H(\mathbf{w})]^{-1}&space;g(\mathbf{w})&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;g(\mathbf{w})&space;&plus;&space;H(\mathbf{w})&space;\mathbf{s}&space;&&space;=&space;0&space;\\&space;\Rightarrow&space;\mathbf{s}&space;&&space;=&space;-&space;[H(\mathbf{w})]^{-1}&space;g(\mathbf{w})&space;\end{align*}" title="\begin{align*} g(\mathbf{w}) + H(\mathbf{w}) \mathbf{s} & = 0 \\ \Rightarrow \mathbf{s} & = - [H(\mathbf{w})]^{-1} g(\mathbf{w}) \end{align*}" /></a>.

This choice of <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{s}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{s}" title="\mathbf{s}" /></a> converges extremely fast if the approximation is sufficiently accurate and the resulting step sufficiently small. Otherwise it can diverge. Divergence often happens if the function is flat or almost flat with respect to some dimension. In that case the second derivatives are close to zero, and their inverse becomes very large -- resulting in gigantic steps. Different from gradient descent, here there is no step-size that guarantees that steps are all small and local. As the Taylor approximation is only accurate locally, large steps can move the current estimates far from regions where the Taylor approximation is accurate.


































