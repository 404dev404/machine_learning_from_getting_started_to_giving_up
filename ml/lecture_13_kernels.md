# Kernels

Linear classifiers are great, but what if there exists no linear decision boundary? As it turns out, there is an elegant way to incorporate non-linearities into most linear classifiers.

## Handcrafted Feature Expansion

We can make linear classifiers non-linear by applying basis function (feature transformations) on the input feature vectors. Formally, for a data vector <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}&space;\in&space;\mathbb{R}^d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}&space;\in&space;\mathbb{R}^d" title="\mathbf{x} \in \mathbb{R}^d" /></a>, we apply the transformation <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}&space;\rightarrow&space;\phi&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}&space;\rightarrow&space;\phi&space;(\mathbf{x})" title="\mathbf{x} \rightarrow \phi (\mathbf{x})" /></a> where <a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;(\mathbf{x})&space;\in&space;\mathbb{R}^D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;(\mathbf{x})&space;\in&space;\mathbb{R}^D" title="\phi (\mathbf{x}) \in \mathbb{R}^D" /></a>. Usually <a href="https://www.codecogs.com/eqnedit.php?latex=D&space;\gg&space;d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D&space;\gg&space;d" title="D \gg d" /></a> because we add dimensions that capture non-linear interactions among the original features.

Advantage: It is simple, and your problem stays convex and well behaved. (i.e. you can still use your original gradient descent code, just with the higher dimensional representation)

Disadvantage: <a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;(\mathbf{x})" title="\phi (\mathbf{x})" /></a> might be very high dimensional.

Consider the following example: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}&space;=&space;\begin{pmatrix}&space;x_1&space;\\&space;x_2&space;\\&space;\vdots&space;\\&space;x_d&space;\end{pmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}&space;=&space;\begin{pmatrix}&space;x_1&space;\\&space;x_2&space;\\&space;\vdots&space;\\&space;x_d&space;\end{pmatrix}" title="\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{pmatrix}" /></a>, and define <a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;(\mathbf{x})&space;=&space;\begin{pmatrix}&space;1&space;\\&space;x_1&space;\\&space;\vdots&space;\\&space;x_d&space;\\&space;x_1&space;x_2&space;\\&space;\vdots&space;\\&space;x_{d-1}&space;x_d&space;\\&space;\vdots&space;\\&space;x_1&space;x_2&space;\cdots&space;x_d&space;\end{pmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;(\mathbf{x})&space;=&space;\begin{pmatrix}&space;1&space;\\&space;x_1&space;\\&space;\vdots&space;\\&space;x_d&space;\\&space;x_1&space;x_2&space;\\&space;\vdots&space;\\&space;x_{d-1}&space;x_d&space;\\&space;\vdots&space;\\&space;x_1&space;x_2&space;\cdots&space;x_d&space;\end{pmatrix}" title="\phi (\mathbf{x}) = \begin{pmatrix} 1 \\ x_1 \\ \vdots \\ x_d \\ x_1 x_2 \\ \vdots \\ x_{d-1} x_d \\ \vdots \\ x_1 x_2 \cdots x_d \end{pmatrix}" /></a>

This new representation, <a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;(\mathbf{x})" title="\phi (\mathbf{x})" /></a>, is very expressive and allows for complicated non-linear decision boundaries -- but the dimensionality is extremely high. This makes our algorithm unbearable (and quickly prohibitively) slow.

## The Kernel Trick

### Gradient Descent with Squared Loss

The kernel trick is a way to get around this dilemma by learning a function in the much higher dimensional space, without ever computing a single vector <a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;(\mathbf{x})" title="\phi (\mathbf{x})" /></a> or ever computing the full vector <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>. It is a little magical.

It is based on the following observation: If we use gradient descent with any one of our standard loss function, the gradient is a linear combination of the input samples. For example, let us take a look at the squared loss:

<a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(\mathbf{w})&space;=&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;\mathbf{x}_{i}&space;-&space;y_i)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(\mathbf{w})&space;=&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;\mathbf{x}_{i}&space;-&space;y_i)^2" title="\ell (\mathbf{w}) = \sum_{i=1}^{n} (\mathbf{w}^{\top} \mathbf{x}_{i} - y_i)^2" /></a>

The gradient descent rule, with step-size/learning-rate <a href="https://www.codecogs.com/eqnedit.php?latex=s&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?s&space;>&space;0" title="s > 0" /></a> (we denote this as <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha&space;>&space;0" title="\alpha > 0" /></a> previously), updates <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> over time,

<a href="https://www.codecogs.com/eqnedit.php?latex=w_{t&plus;1}&space;\leftarrow&space;w_t&space;-&space;s(\frac{\partial&space;\ell}{\partial&space;\mathbf{w}})&space;\enspace&space;\text{where}&space;\enspace&space;\frac{\partial&space;\ell}{\partial&space;\mathbf{w}}&space;=&space;\sum_{i=1}^{n}&space;\underbrace{2(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;-&space;y_i)}_{\gamma_i&space;\text{:&space;function&space;of&space;}&space;\mathbf{x}_i,&space;y_i}&space;\mathbf{x}_{i}&space;=&space;\sum_{i=1}^{n}&space;\gamma_i&space;\mathbf{x}_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w_{t&plus;1}&space;\leftarrow&space;w_t&space;-&space;s(\frac{\partial&space;\ell}{\partial&space;\mathbf{w}})&space;\enspace&space;\text{where}&space;\enspace&space;\frac{\partial&space;\ell}{\partial&space;\mathbf{w}}&space;=&space;\sum_{i=1}^{n}&space;\underbrace{2(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;-&space;y_i)}_{\gamma_i&space;\text{:&space;function&space;of&space;}&space;\mathbf{x}_i,&space;y_i}&space;\mathbf{x}_{i}&space;=&space;\sum_{i=1}^{n}&space;\gamma_i&space;\mathbf{x}_{i}" title="w_{t+1} \leftarrow w_t - s(\frac{\partial \ell}{\partial \mathbf{w}}) \enspace \text{where} \enspace \frac{\partial \ell}{\partial \mathbf{w}} = \sum_{i=1}^{n} \underbrace{2(\mathbf{w}^{\top} \mathbf{x}_i - y_i)}_{\gamma_i \text{: function of } \mathbf{x}_i, y_i} \mathbf{x}_{i} = \sum_{i=1}^{n} \gamma_i \mathbf{x}_{i}" /></a>

We will now show that we can express <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> as a linear combination of all input vectors,

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i&space;\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i&space;\mathbf{x}_i" title="\mathbf{w} = \sum_{i=1}^{n} \alpha_i \mathbf{x}_i" /></a>

Since the loss is convex, the final solution is independent of the initialization, and we can initialize <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}_0" title="\mathbf{w}_0" /></a> to be whatever we want. For convenience, let us pick <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}_0&space;=&space;\begin{pmatrix}&space;0&space;\\&space;\vdots&space;\\&space;0&space;\end{pmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}_0&space;=&space;\begin{pmatrix}&space;0&space;\\&space;\vdots&space;\\&space;0&space;\end{pmatrix}" title="\mathbf{w}_0 = \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix}" /></a>. For the initial choice of <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}_0" title="\mathbf{w}_0" /></a>, the linear combination in <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i&space;\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i&space;\mathbf{x}_i" title="\mathbf{w} = \sum_{i=1}^{n} \alpha_i \mathbf{x}_i" /></a> is trivially <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha_1&space;=&space;\cdots&space;=&space;\alpha_n&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha_1&space;=&space;\cdots&space;=&space;\alpha_n&space;=&space;0" title="\alpha_1 = \cdots = \alpha_n = 0" /></a>. We now show that throughout the entire gradient descent optimization such coefficients <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha_1,&space;\ldots,\alpha_n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha_1,&space;\ldots,\alpha_n" title="\alpha_1, \ldots,\alpha_n" /></a> must always exist, as we can re-write the gradient updates entirely in terms of updating the <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha_i" title="\alpha_i" /></a> coefficients:

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;\mathbf{w}_{1}&space;=&space;\mathbf{w}_{0}&space;-&space;s&space;\sum_{i=1}^{n}&space;2(\mathbf{w}_{0}^{\top}&space;\mathbf{x}_{i}&space;-&space;y_i)&space;\mathbf{x}_{i}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^0&space;\mathbf{x}_{i}&space;-&space;s&space;\sum_{i=1}^{n}&space;\gamma_i^0&space;\mathbf{x}_{i}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^1&space;\mathbf{x}_{i}&space;&&&space;(\text{with}\enspace&space;\alpha_i^1&space;=&space;\alpha_i^0&space;-&space;s&space;\gamma_i^0)&space;\\&space;&&space;\mathbf{w}_{2}&space;=&space;\mathbf{w}_{1}&space;-&space;s&space;\sum_{i=1}^{n}&space;2&space;(\mathbf{w}_1^\top&space;\mathbf{x}_i&space;-&space;y_i)&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^1&space;\mathbf{x}_i&space;-&space;s&space;\sum_{i=1}^{n}&space;\gamma_i^1&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^2&space;\mathbf{x}_i&space;&&&space;(\text{with}\enspace&space;\alpha_i^2&space;=&space;\alpha_i^1&space;-&space;s&space;\gamma_i^1)&space;\\&space;&&space;\mathbf{w}_3&space;=&space;\mathbf{w}_2&space;-&space;s&space;\sum_{i=1}^{n}&space;2&space;(\mathbf{w}_2^\top&space;\mathbf{x}_i&space;-&space;y_i)&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^2&space;\mathbf{x}_i&space;-&space;s&space;\sum_{i=1}^{n}&space;\gamma_i^2&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^3&space;\mathbf{x}_i&space;&&&space;(\text{with}\enspace&space;\alpha_i^3&space;=&space;\alpha_i^2&space;-&space;s&space;\gamma_i^2)&space;\\&space;&\ldots&space;\\&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;&&space;\mathbf{w}_{1}&space;=&space;\mathbf{w}_{0}&space;-&space;s&space;\sum_{i=1}^{n}&space;2(\mathbf{w}_{0}^{\top}&space;\mathbf{x}_{i}&space;-&space;y_i)&space;\mathbf{x}_{i}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^0&space;\mathbf{x}_{i}&space;-&space;s&space;\sum_{i=1}^{n}&space;\gamma_i^0&space;\mathbf{x}_{i}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^1&space;\mathbf{x}_{i}&space;&&&space;(\text{with}\enspace&space;\alpha_i^1&space;=&space;\alpha_i^0&space;-&space;s&space;\gamma_i^0)&space;\\&space;&&space;\mathbf{w}_{2}&space;=&space;\mathbf{w}_{1}&space;-&space;s&space;\sum_{i=1}^{n}&space;2&space;(\mathbf{w}_1^\top&space;\mathbf{x}_i&space;-&space;y_i)&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^1&space;\mathbf{x}_i&space;-&space;s&space;\sum_{i=1}^{n}&space;\gamma_i^1&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^2&space;\mathbf{x}_i&space;&&&space;(\text{with}\enspace&space;\alpha_i^2&space;=&space;\alpha_i^1&space;-&space;s&space;\gamma_i^1)&space;\\&space;&&space;\mathbf{w}_3&space;=&space;\mathbf{w}_2&space;-&space;s&space;\sum_{i=1}^{n}&space;2&space;(\mathbf{w}_2^\top&space;\mathbf{x}_i&space;-&space;y_i)&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^2&space;\mathbf{x}_i&space;-&space;s&space;\sum_{i=1}^{n}&space;\gamma_i^2&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^3&space;\mathbf{x}_i&space;&&&space;(\text{with}\enspace&space;\alpha_i^3&space;=&space;\alpha_i^2&space;-&space;s&space;\gamma_i^2)&space;\\&space;&\ldots&space;\\&space;\end{align*}" title="\begin{align*} & \mathbf{w}_{1} = \mathbf{w}_{0} - s \sum_{i=1}^{n} 2(\mathbf{w}_{0}^{\top} \mathbf{x}_{i} - y_i) \mathbf{x}_{i} = \sum_{i=1}^{n} \alpha_i^0 \mathbf{x}_{i} - s \sum_{i=1}^{n} \gamma_i^0 \mathbf{x}_{i} = \sum_{i=1}^{n} \alpha_i^1 \mathbf{x}_{i} && (\text{with}\enspace \alpha_i^1 = \alpha_i^0 - s \gamma_i^0) \\ & \mathbf{w}_{2} = \mathbf{w}_{1} - s \sum_{i=1}^{n} 2 (\mathbf{w}_1^\top \mathbf{x}_i - y_i) \mathbf{x}_i = \sum_{i=1}^{n} \alpha_i^1 \mathbf{x}_i - s \sum_{i=1}^{n} \gamma_i^1 \mathbf{x}_i = \sum_{i=1}^{n} \alpha_i^2 \mathbf{x}_i && (\text{with}\enspace \alpha_i^2 = \alpha_i^1 - s \gamma_i^1) \\ & \mathbf{w}_3 = \mathbf{w}_2 - s \sum_{i=1}^{n} 2 (\mathbf{w}_2^\top \mathbf{x}_i - y_i) \mathbf{x}_i = \sum_{i=1}^{n} \alpha_i^2 \mathbf{x}_i - s \sum_{i=1}^{n} \gamma_i^2 \mathbf{x}_i = \sum_{i=1}^{n} \alpha_i^3 \mathbf{x}_i && (\text{with}\enspace \alpha_i^3 = \alpha_i^2 - s \gamma_i^2) \\ &\ldots \\ \end{align*}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;\mathbf{w}_{t}&space;=&space;\mathbf{w}_{t-1}&space;-&space;s&space;\sum_{i=1}^{n}&space;2&space;(\mathbf{w}_{t-1}^{\top}&space;\mathbf{x}_i&space;-&space;y_i)&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^{t-1}&space;\mathbf{x}_i&space;-&space;s&space;\sum_{i=1}^{n}&space;\gamma_i^{t-1}&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^t&space;\mathbf{x}_i&space;&&&space;(\text{with}\enspace&space;\alpha_i^t&space;=&space;\alpha_i^{t-1}&space;-&space;s&space;\gamma_{i}^{t-1})&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;&&space;\mathbf{w}_{t}&space;=&space;\mathbf{w}_{t-1}&space;-&space;s&space;\sum_{i=1}^{n}&space;2&space;(\mathbf{w}_{t-1}^{\top}&space;\mathbf{x}_i&space;-&space;y_i)&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^{t-1}&space;\mathbf{x}_i&space;-&space;s&space;\sum_{i=1}^{n}&space;\gamma_i^{t-1}&space;\mathbf{x}_i&space;=&space;\sum_{i=1}^{n}&space;\alpha_i^t&space;\mathbf{x}_i&space;&&&space;(\text{with}\enspace&space;\alpha_i^t&space;=&space;\alpha_i^{t-1}&space;-&space;s&space;\gamma_{i}^{t-1})&space;\end{align*}" title="\begin{align*} & \mathbf{w}_{t} = \mathbf{w}_{t-1} - s \sum_{i=1}^{n} 2 (\mathbf{w}_{t-1}^{\top} \mathbf{x}_i - y_i) \mathbf{x}_i = \sum_{i=1}^{n} \alpha_i^{t-1} \mathbf{x}_i - s \sum_{i=1}^{n} \gamma_i^{t-1} \mathbf{x}_i = \sum_{i=1}^{n} \alpha_i^t \mathbf{x}_i && (\text{with}\enspace \alpha_i^t = \alpha_i^{t-1} - s \gamma_{i}^{t-1}) \end{align*}" /></a>

Formally, the argument is by induction. <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> is trivially a linear combination of our training vectors for <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}_0" title="\mathbf{w}_0" /></a> (base case). If we apply the inductive hypothesis for <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}_t" title="\mathbf{w}_t" /></a> it follows for <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}_{t&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}_{t&plus;1}" title="\mathbf{w}_{t+1}" /></a>.

The update-rule for <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha_i^t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha_i^t" title="\alpha_i^t" /></a> is thus

<a href="https://www.codecogs.com/eqnedit.php?latex=\alpha_i^t&space;=&space;\alpha_i^{t-1}&space;-&space;s&space;\gamma_i^{t-1}&space;\text{,&space;and&space;we&space;have&space;}&space;\alpha_i^t&space;=&space;s&space;\sum_{r=0}^{t-1}&space;\gamma_i^{r}." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha_i^t&space;=&space;\alpha_i^{t-1}&space;-&space;s&space;\gamma_i^{t-1}&space;\text{,&space;and&space;we&space;have&space;}&space;\alpha_i^t&space;=&space;s&space;\sum_{r=0}^{t-1}&space;\gamma_i^{r}." title="\alpha_i^t = \alpha_i^{t-1} - s \gamma_i^{t-1} \text{, and we have } \alpha_i^t = s \sum_{r=0}^{t-1} \gamma_i^{r}." /></a>

In other words, we can perform the entire gradient descent update rule without ever expressing <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> explicitly. We just keep track of the <a href="https://www.codecogs.com/eqnedit.php?latex=n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n" title="n" /></a> coefficients <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha_1,&space;\ldots,&space;\alpha_n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha_1,&space;\ldots,&space;\alpha_n" title="\alpha_1, \ldots, \alpha_n" /></a>. Now that <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> can be written as a linear combination of the training set, we can also express the inner-product of <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> with any input <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> purely in terms of inner-products between training inputs:

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}^{\top}&space;\mathbf{x}_{j}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i&space;\mathbf{x}_i^\top&space;\mathbf{x}_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}^{\top}&space;\mathbf{x}_{j}&space;=&space;\sum_{i=1}^{n}&space;\alpha_i&space;\mathbf{x}_i^\top&space;\mathbf{x}_j" title="\mathbf{w}^{\top} \mathbf{x}_{j} = \sum_{i=1}^{n} \alpha_i \mathbf{x}_i^\top \mathbf{x}_j" /></a>

Consequently, we can also re-write the squared-loss from <a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(\mathbf{w})&space;=&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;\mathbf{x}_{i}&space;-&space;y_i)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(\mathbf{w})&space;=&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;\mathbf{x}_{i}&space;-&space;y_i)^2" title="\ell (\mathbf{w}) = \sum_{i=1}^{n} (\mathbf{w}^{\top} \mathbf{x}_{i} - y_i)^2" /></a> entirely in terms of inner-product between training inputs:

<a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(\alpha)&space;=&space;\sum_{i=1}^{n}&space;\Big(&space;\sum_{j=1}^{n}&space;\alpha_j&space;\mathbf{x}_j^{\top}&space;\mathbf{x}_i&space;-&space;y_i&space;\Big)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(\alpha)&space;=&space;\sum_{i=1}^{n}&space;\Big(&space;\sum_{j=1}^{n}&space;\alpha_j&space;\mathbf{x}_j^{\top}&space;\mathbf{x}_i&space;-&space;y_i&space;\Big)^2" title="\ell (\alpha) = \sum_{i=1}^{n} \Big( \sum_{j=1}^{n} \alpha_j \mathbf{x}_j^{\top} \mathbf{x}_i - y_i \Big)^2" /></a>

During test-time we also only need these coefficients to make a prediction on a test-input <a href="https://www.codecogs.com/eqnedit.php?latex=x_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_t" title="x_t" /></a>, and can write the entire classifier in terms of inner-products between the test point and training points:

<a href="https://www.codecogs.com/eqnedit.php?latex=h(\mathbf{x}_t)&space;=&space;\mathbf{w}^{\top}&space;\mathbf{x}_t&space;=&space;\sum_{j=1}^{n}&space;\alpha_j&space;\mathbf{x}_j^{\top}&space;\mathbf{x}_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\mathbf{x}_t)&space;=&space;\mathbf{w}^{\top}&space;\mathbf{x}_t&space;=&space;\sum_{j=1}^{n}&space;\alpha_j&space;\mathbf{x}_j^{\top}&space;\mathbf{x}_t" title="h(\mathbf{x}_t) = \mathbf{w}^{\top} \mathbf{x}_t = \sum_{j=1}^{n} \alpha_j \mathbf{x}_j^{\top} \mathbf{x}_t" /></a>

Do you notice a theme? The only information we ever need in order to learn a hyper-plane classifier with the squared-loss is inner-products between all pairs of data vectors.

## Inner-Product Computation

Let's go back to the previous example, <a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;(\mathbf{x})&space;=&space;\begin{pmatrix}&space;1&space;\\&space;x_1&space;\\&space;\vdots&space;\\&space;x_d&space;\\&space;x_1&space;x_2&space;\\&space;\vdots&space;\\&space;x_{d-1}&space;x_d&space;\\&space;\vdots&space;\\&space;x_1&space;x_2&space;\cdots&space;x_d&space;\end{pmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;(\mathbf{x})&space;=&space;\begin{pmatrix}&space;1&space;\\&space;x_1&space;\\&space;\vdots&space;\\&space;x_d&space;\\&space;x_1&space;x_2&space;\\&space;\vdots&space;\\&space;x_{d-1}&space;x_d&space;\\&space;\vdots&space;\\&space;x_1&space;x_2&space;\cdots&space;x_d&space;\end{pmatrix}" title="\phi (\mathbf{x}) = \begin{pmatrix} 1 \\ x_1 \\ \vdots \\ x_d \\ x_1 x_2 \\ \vdots \\ x_{d-1} x_d \\ \vdots \\ x_1 x_2 \cdots x_d \end{pmatrix}" /></a>.

The inner product <a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;(\mathbf{x})^{\top}&space;\phi&space;(\mathbf{z})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;(\mathbf{x})^{\top}&space;\phi&space;(\mathbf{z})" title="\phi (\mathbf{x})^{\top} \phi (\mathbf{z})" /></a> can be formulated as:

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\phi&space;(\mathbf{x})^{\top}&space;\phi&space;(\mathbf{z})&space;&&space;=&space;1&space;\cdot&space;1&space;&plus;&space;x_1&space;z_1&space;&plus;&space;x_2&space;z_2&space;&plus;&space;\cdots&space;&plus;&space;x_1&space;x_2&space;z_1&space;z_2&space;&plus;&space;\cdots&space;&plus;&space;x_1&space;\cdots&space;x_d&space;z_1&space;\cdots&space;z_d&space;\\&space;&&space;=&space;\prod_{k=1}^{d}&space;(1&space;&plus;&space;x_k&space;z_k)&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;\phi&space;(\mathbf{x})^{\top}&space;\phi&space;(\mathbf{z})&space;&&space;=&space;1&space;\cdot&space;1&space;&plus;&space;x_1&space;z_1&space;&plus;&space;x_2&space;z_2&space;&plus;&space;\cdots&space;&plus;&space;x_1&space;x_2&space;z_1&space;z_2&space;&plus;&space;\cdots&space;&plus;&space;x_1&space;\cdots&space;x_d&space;z_1&space;\cdots&space;z_d&space;\\&space;&&space;=&space;\prod_{k=1}^{d}&space;(1&space;&plus;&space;x_k&space;z_k)&space;\end{align*}" title="\begin{align*} \phi (\mathbf{x})^{\top} \phi (\mathbf{z}) & = 1 \cdot 1 + x_1 z_1 + x_2 z_2 + \cdots + x_1 x_2 z_1 z_2 + \cdots + x_1 \cdots x_d z_1 \cdots z_d \\ & = \prod_{k=1}^{d} (1 + x_k z_k) \end{align*}" /></a>

The sum of <a href="https://www.codecogs.com/eqnedit.php?latex=2^d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?2^d" title="2^d" /></a> terms becomes the product of <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> terms. We can compute the inner-product from the above formula in time <a href="https://www.codecogs.com/eqnedit.php?latex=O(d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(d)" title="O(d)" /></a> instead of <a href="https://www.codecogs.com/eqnedit.php?latex=O(2^d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(2^d)" title="O(2^d)" /></a>! We define the function

<a href="https://www.codecogs.com/eqnedit.php?latex=\underbrace{\mathbf{k}&space;(\mathbf{x}_i&space;,&space;\mathbf{x}_j)}_{\text{this&space;is&space;called&space;the&space;}\textbf{kernel&space;function}}&space;=&space;\phi&space;(\mathbf{x}_i)^{\top}&space;\phi&space;(\mathbf{x}_j)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\underbrace{\mathbf{k}&space;(\mathbf{x}_i&space;,&space;\mathbf{x}_j)}_{\text{this&space;is&space;called&space;the&space;}\textbf{kernel&space;function}}&space;=&space;\phi&space;(\mathbf{x}_i)^{\top}&space;\phi&space;(\mathbf{x}_j)" title="\underbrace{\mathbf{k} (\mathbf{x}_i , \mathbf{x}_j)}_{\text{this is called the }\textbf{kernel function}} = \phi (\mathbf{x}_i)^{\top} \phi (\mathbf{x}_j)" /></a>

With a finite training set of <a href="https://www.codecogs.com/eqnedit.php?latex=n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n" title="n" /></a> samples, inner products are often pre-computed and stored in a Kernel Matrix:

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}_{ij}&space;=&space;\phi&space;(\mathbf{x}_i)^\top&space;\phi&space;(\mathbf{x}_j)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}_{ij}&space;=&space;\phi&space;(\mathbf{x}_i)^\top&space;\phi&space;(\mathbf{x}_j)" title="\mathbf{K}_{ij} = \phi (\mathbf{x}_i)^\top \phi (\mathbf{x}_j)" /></a>

If we store the matrix <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}" title="\mathbf{K}" /></a>, we only need to do simple inner-product look-ups and low-dimensional computations throughout the gradient descent algorithm. The final classifier becomes:

<a href="https://www.codecogs.com/eqnedit.php?latex=h(\mathbf{x}_t)&space;=&space;\sum_{j=1}^{n}&space;\alpha_j&space;\mathbf{k}&space;(\mathbf{x}_j&space;,&space;\mathbf{x}_t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\mathbf{x}_t)&space;=&space;\sum_{j=1}^{n}&space;\alpha_j&space;\mathbf{k}&space;(\mathbf{x}_j&space;,&space;\mathbf{x}_t)" title="h(\mathbf{x}_t) = \sum_{j=1}^{n} \alpha_j \mathbf{k} (\mathbf{x}_j , \mathbf{x}_t)" /></a>

During training in the new high dimensional space of <a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;(\mathbf{x})" title="\phi (\mathbf{x})" /></a> we want to compute <a href="https://www.codecogs.com/eqnedit.php?latex=\gamma_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\gamma_i" title="\gamma_i" /></a> through kernels, without ever computing any <a href="https://www.codecogs.com/eqnedit.php?latex=\phi&space;(\mathbf{x}_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi&space;(\mathbf{x}_i)" title="\phi (\mathbf{x}_i)" /></a> or even <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>. We previously established that <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}&space;=&space;\sum_{j=1}^{n}&space;\alpha_j&space;\phi&space;(\mathbf{x}_j)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}&space;=&space;\sum_{j=1}^{n}&space;\alpha_j&space;\phi&space;(\mathbf{x}_j)" title="\mathbf{w} = \sum_{j=1}^{n} \alpha_j \phi (\mathbf{x}_j)" /></a>, and <a href="https://www.codecogs.com/eqnedit.php?latex=\gamma_i&space;=&space;2&space;(\mathbf{w}^{\top}&space;\phi&space;(\mathbf{x}_i)&space;-&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\gamma_i&space;=&space;2&space;(\mathbf{w}^{\top}&space;\phi&space;(\mathbf{x}_i)&space;-&space;y_i)" title="\gamma_i = 2 (\mathbf{w}^{\top} \phi (\mathbf{x}_i) - y_i)" /></a>. It follows that <a href="https://www.codecogs.com/eqnedit.php?latex=\gamma_i&space;=&space;2&space;\big((\sum_{j=1}^{n}&space;\alpha_j&space;\mathbf{K}_{ij})&space;-&space;j_i&space;\big)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\gamma_i&space;=&space;2&space;\big((\sum_{j=1}^{n}&space;\alpha_j&space;\mathbf{K}_{ij})&space;-&space;j_i&space;\big)" title="\gamma_i = 2 \big((\sum_{j=1}^{n} \alpha_j \mathbf{K}_{ij}) - j_i \big)" /></a>. The gradient update in iteration <a href="https://www.codecogs.com/eqnedit.php?latex=t&plus;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?t&plus;1" title="t+1" /></a> becomes

<a href="https://www.codecogs.com/eqnedit.php?latex=\alpha_i^{t&plus;1}&space;\leftarrow&space;\alpha_i^{t}&space;-&space;2s&space;\big(&space;(\sum_{j=1}^{n}&space;\alpha_j^t&space;\mathbf{K}_{ij}&space;)&space;-&space;y_i&space;\big)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha_i^{t&plus;1}&space;\leftarrow&space;\alpha_i^{t}&space;-&space;2s&space;\big(&space;(\sum_{j=1}^{n}&space;\alpha_j^t&space;\mathbf{K}_{ij}&space;)&space;-&space;y_i&space;\big)" title="\alpha_i^{t+1} \leftarrow \alpha_i^{t} - 2s \big( (\sum_{j=1}^{n} \alpha_j^t \mathbf{K}_{ij} ) - y_i \big)" /></a>

As we have <a href="https://www.codecogs.com/eqnedit.php?latex=n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n" title="n" /></a> such updates to do, the amount of work per gradient update in the transformed space is <a href="https://www.codecogs.com/eqnedit.php?latex=O(n^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(n^2)" title="O(n^2)" /></a> -- far better than <a href="https://www.codecogs.com/eqnedit.php?latex=O(2^d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(2^d)" title="O(2^d)" /></a>.

## General Kernels

Below are some popular kernel functions:

**Linear**: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;\mathbf{x}^{\top}&space;\mathbf{z}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;\mathbf{x}^{\top}&space;\mathbf{z}" title="\mathbf{K} (\mathbf{x}, \mathbf{z}) = \mathbf{x}^{\top} \mathbf{z}" /></a>

(The linear kernel is equivalent to just using a good old linear classifier -- but it can be faster to use a kernel matrix if the dimensionality <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> of the data is high.)

**Polynomial**: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;(1&space;&plus;&space;\mathbf{x}^{\top}&space;\mathbf{z})^d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;(1&space;&plus;&space;\mathbf{x}^{\top}&space;\mathbf{z})^d" title="\mathbf{K} (\mathbf{x}, \mathbf{z}) = (1 + \mathbf{x}^{\top} \mathbf{z})^d" /></a>

**Radial Basis Function (RBF) (a.k.a. Gaussian Kernel)**: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;e^{\frac{-\|&space;\mathbf{x}&space;-&space;\mathbf{z}&space;\|^2}{\sigma^2}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;e^{\frac{-\|&space;\mathbf{x}&space;-&space;\mathbf{z}&space;\|^2}{\sigma^2}}" title="\mathbf{K} (\mathbf{x}, \mathbf{z}) = e^{\frac{-\| \mathbf{x} - \mathbf{z} \|^2}{\sigma^2}}" /></a>

The RBF kernel is the most popular Kernel! It is a **Universal Approximator**!! Its corresponding feature vector is infinite dimensional and cannot be computed. However, very effective low dimensional approximations exist.

**Exponential Kernel**: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;e^{\frac{-\|&space;\mathbf{x}&space;-&space;\mathbf{z}&space;\|}{2\sigma^2}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;e^{\frac{-\|&space;\mathbf{x}&space;-&space;\mathbf{z}&space;\|}{2\sigma^2}}" title="\mathbf{K} (\mathbf{x}, \mathbf{z}) = e^{\frac{-\| \mathbf{x} - \mathbf{z} \|}{2\sigma^2}}" /></a>

**Laplacian Kernel**: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;e^{\frac{-\vert&space;\mathbf{x}&space;-&space;\mathbf{z}&space;\vert}{\sigma}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;e^{\frac{-\vert&space;\mathbf{x}&space;-&space;\mathbf{z}&space;\vert}{\sigma}}" title="\mathbf{K} (\mathbf{x}, \mathbf{z}) = e^{\frac{-\vert \mathbf{x} - \mathbf{z} \vert}{\sigma}}" /></a>

**Sigmoid Kernel**: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;\text{tanh}&space;(\mathbf{a}&space;\mathbf{x}^{\top}&space;&plus;&space;c)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}&space;(\mathbf{x},&space;\mathbf{z})&space;=&space;\text{tanh}&space;(\mathbf{a}&space;\mathbf{x}^{\top}&space;&plus;&space;c)" title="\mathbf{K} (\mathbf{x}, \mathbf{z}) = \text{tanh} (\mathbf{a} \mathbf{x}^{\top} + c)" /></a>

### Kernel Functions

Can any function <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}(\cdot,&space;\cdot)&space;\rightarrow&space;\mathcal{R}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}(\cdot,&space;\cdot)&space;\rightarrow&space;\mathcal{R}" title="\mathbf{K}(\cdot, \cdot) \rightarrow \mathcal{R}" /></a> be used as a kernel?

No, the matrix <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}&space;(\mathbf{x}_i&space;,&space;\mathbf{x}_j)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}&space;(\mathbf{x}_i&space;,&space;\mathbf{x}_j)" title="\mathbf{K} (\mathbf{x}_i , \mathbf{x}_j)" /></a> has to correspond to real inner-products after some transformation <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}&space;\rightarrow&space;\phi&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}&space;\rightarrow&space;\phi&space;(\mathbf{x})" title="\mathbf{x} \rightarrow \phi (\mathbf{x})" /></a>. This is the case if and only if <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}" title="\mathbf{K}" /></a> is *positive semi-definite*.

**Definition**: A matrix <a href="https://www.codecogs.com/eqnedit.php?latex=A&space;\in&space;\mathbb{R}^{n&space;\times&space;n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?A&space;\in&space;\mathbb{R}^{n&space;\times&space;n}" title="A \in \mathbb{R}^{n \times n}" /></a> is positive semi-definite iff <a href="https://www.codecogs.com/eqnedit.php?latex=\forall&space;\mathbf{q}&space;\in&space;\mathbb{R}^{n},&space;\mathbf{q}^{\top}&space;\mathbf{A}\mathbf{q}&space;\geq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\forall&space;\mathbf{q}&space;\in&space;\mathbb{R}^{n},&space;\mathbf{q}^{\top}&space;\mathbf{A}\mathbf{q}&space;\geq&space;0" title="\forall \mathbf{q} \in \mathbb{R}^{n}, \mathbf{q}^{\top} \mathbf{A}\mathbf{q} \geq 0" /></a>.

Remember <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}_{ij}&space;=&space;\phi&space;(\mathbf{x}_i)^{\top}&space;\phi&space;(\mathbf{x}_j)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}_{ij}&space;=&space;\phi&space;(\mathbf{x}_i)^{\top}&space;\phi&space;(\mathbf{x}_j)" title="\mathbf{K}_{ij} = \phi (\mathbf{x}_i)^{\top} \phi (\mathbf{x}_j)" /></a>. So <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}&space;=&space;\Phi^{\top}&space;\Phi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}&space;=&space;\Phi^{\top}&space;\Phi" title="\mathbf{K} = \Phi^{\top} \Phi" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=\Phi&space;=&space;[\phi&space;(\mathbf{x}_1),&space;\ldots,&space;\phi&space;(\mathbf{x}_n)]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Phi&space;=&space;[\phi&space;(\mathbf{x}_1),&space;\ldots,&space;\phi&space;(\mathbf{x}_n)]" title="\Phi = [\phi (\mathbf{x}_1), \ldots, \phi (\mathbf{x}_n)]" /></a>. It follows that <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{K}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{K}" title="\mathbf{K}" /></a> is positive semi-definite, because <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{q}^{\top}&space;\mathbf{K}&space;\mathbf{q}&space;=&space;(\Phi^{\top}&space;\mathbf{q})^2&space;\geq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{q}^{\top}&space;\mathbf{K}&space;\mathbf{q}&space;=&space;(\Phi^{\top}&space;\mathbf{q})^2&space;\geq&space;0" title="\mathbf{q}^{\top} \mathbf{K} \mathbf{q} = (\Phi^{\top} \mathbf{q})^2 \geq 0" /></a>. Inversely, if any matrix <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{A}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{A}" title="\mathbf{A}" /></a> is positive semi-definite, it can be decomposed as <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{A}&space;=&space;\Phi^{\top}&space;\Phi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{A}&space;=&space;\Phi^{\top}&space;\Phi" title="\mathbf{A} = \Phi^{\top} \Phi" /></a> for some realization of <a href="https://www.codecogs.com/eqnedit.php?latex=\Phi" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Phi" title="\Phi" /></a>.
