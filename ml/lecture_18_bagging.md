# Bagging

Also known as Bootstrap Aggregating (Breiman 96). Bagging is an **ensemble** method.

## Bagging Reduces Variance

Remember the Bias / Variance decomposition:

<a href="https://www.codecogs.com/eqnedit.php?latex=\underbrace{\mathbb{E}[(h_D&space;(x)&space;-&space;y)^2]}_{\text{Error}}&space;=&space;\underbrace{\mathbb{E}[(h_D&space;(x)&space;-&space;\bar{h}&space;(x)&space;)^2]}_{\text{Variance}}&space;&plus;&space;\underbrace{\mathbb{E}[(\bar{h}&space;(x)&space;-&space;\bar{y}&space;(x))^2]}_{\text{Bias}}&space;&plus;&space;\underbrace{\mathbb{E}[(\bar{y}&space;(x)&space;-&space;y(x))^2]}_{\text{Noise}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\underbrace{\mathbb{E}[(h_D&space;(x)&space;-&space;y)^2]}_{\text{Error}}&space;=&space;\underbrace{\mathbb{E}[(h_D&space;(x)&space;-&space;\bar{h}&space;(x)&space;)^2]}_{\text{Variance}}&space;&plus;&space;\underbrace{\mathbb{E}[(\bar{h}&space;(x)&space;-&space;\bar{y}&space;(x))^2]}_{\text{Bias}}&space;&plus;&space;\underbrace{\mathbb{E}[(\bar{y}&space;(x)&space;-&space;y(x))^2]}_{\text{Noise}}" title="\underbrace{\mathbb{E}[(h_D (x) - y)^2]}_{\text{Error}} = \underbrace{\mathbb{E}[(h_D (x) - \bar{h} (x) )^2]}_{\text{Variance}} + \underbrace{\mathbb{E}[(\bar{h} (x) - \bar{y} (x))^2]}_{\text{Bias}} + \underbrace{\mathbb{E}[(\bar{y} (x) - y(x))^2]}_{\text{Noise}}" /></a>

Our goal is to reduce the variance term: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbb{E}[(h_D&space;(x)&space;-&space;\bar{h}&space;(x))^2]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbb{E}[(h_D&space;(x)&space;-&space;\bar{h}&space;(x))^2]" title="\mathbb{E}[(h_D (x) - \bar{h} (x))^2]" /></a>.

For this, we want <a href="https://www.codecogs.com/eqnedit.php?latex=h_D&space;\rightarrow&space;\bar{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h_D&space;\rightarrow&space;\bar{h}" title="h_D \rightarrow \bar{h}" /></a>.

### Weak Law of Large Numbers

The weak law of large numbers says (roughly) for i.i.d. random variables <a href="https://www.codecogs.com/eqnedit.php?latex=x_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i" title="x_i" /></a> with mean <a href="https://www.codecogs.com/eqnedit.php?latex=\bar{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bar{x}" title="\bar{x}" /></a>, we have,

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{m}&space;\sum_{i=1}^{m}&space;x_i&space;\rightarrow&space;\bar{x}&space;\text{&space;as&space;}&space;m&space;\rightarrow&space;\infty" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{m}&space;\sum_{i=1}^{m}&space;x_i&space;\rightarrow&space;\bar{x}&space;\text{&space;as&space;}&space;m&space;\rightarrow&space;\infty" title="\frac{1}{m} \sum_{i=1}^{m} x_i \rightarrow \bar{x} \text{ as } m \rightarrow \infty" /></a>

Apply this to classifiers: Assume we have <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> training sets <a href="https://www.codecogs.com/eqnedit.php?latex=D_1,&space;D_2,&space;\ldots&space;,&space;D_n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_1,&space;D_2,&space;\ldots&space;,&space;D_n" title="D_1, D_2, \ldots , D_n" /></a> drawn from <a href="https://www.codecogs.com/eqnedit.php?latex=P^m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P^m" title="P^m" /></a>. Train a classifier on each one and average result:

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{h}&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;h_{D_i}&space;\rightarrow&space;\bar{h}&space;\text{&space;as&space;}&space;m&space;\rightarrow&space;\infty" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{h}&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;h_{D_i}&space;\rightarrow&space;\bar{h}&space;\text{&space;as&space;}&space;m&space;\rightarrow&space;\infty" title="\hat{h} = \frac{1}{m} \sum_{i=1}^{m} h_{D_i} \rightarrow \bar{h} \text{ as } m \rightarrow \infty" /></a>

We refer to such an average of multiple classifiers as an **ensemble** of classifiers.

*Good news*: If <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{h}&space;\rightarrow&space;\bar{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{h}&space;\rightarrow&space;\bar{h}" title="\hat{h} \rightarrow \bar{h}" /></a> the variance component of the error must also vanish, i.e. 

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathbb{E}[(\hat{h}(x)&space;-&space;\bar{h}(x))^2]&space;\rightarrow&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbb{E}[(\hat{h}(x)&space;-&space;\bar{h}(x))^2]&space;\rightarrow&space;0" title="\mathbb{E}[(\hat{h}(x) - \bar{h}(x))^2] \rightarrow 0" /></a>

*Problem*: We don't have <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> data sets <a href="https://www.codecogs.com/eqnedit.php?latex=D_1&space;,&space;\ldots&space;,&space;D_m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_1&space;,&space;\ldots&space;,&space;D_m" title="D_1 , \ldots , D_m" /></a>, we only have <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>.

### Solution: Bagging (Bootstrap Aggregating)

Simulate drawing from <a href="https://www.codecogs.com/eqnedit.php?latex=P" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P" title="P" /></a> by drawing uniformly with replacement from the set <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>. i.e. let <a href="https://www.codecogs.com/eqnedit.php?latex=Q(X,&space;Y&space;\vert&space;D)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q(X,&space;Y&space;\vert&space;D)" title="Q(X, Y \vert D)" /></a> be a probability distribution that picks a training sample <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_i&space;,&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_i&space;,&space;y_i)" title="(\mathbf{x}_i , y_i)" /></a> from <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a> uniformly at random. More formally, <a href="https://www.codecogs.com/eqnedit.php?latex=Q((\mathbf{x}_i&space;,&space;y_i)&space;\vert&space;D)&space;=&space;\frac{1}{n}&space;\qquad&space;\forall&space;(\mathbf{x}_i&space;,&space;y_i)&space;\in&space;D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q((\mathbf{x}_i&space;,&space;y_i)&space;\vert&space;D)&space;=&space;\frac{1}{n}&space;\qquad&space;\forall&space;(\mathbf{x}_i&space;,&space;y_i)&space;\in&space;D" title="Q((\mathbf{x}_i , y_i) \vert D) = \frac{1}{n} \qquad \forall (\mathbf{x}_i , y_i) \in D" /></a> with <a href="https://www.codecogs.com/eqnedit.php?latex=n&space;=&space;\vert&space;D&space;\vert" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n&space;=&space;\vert&space;D&space;\vert" title="n = \vert D \vert" /></a>. We sample the set <a href="https://www.codecogs.com/eqnedit.php?latex=D_i&space;\sim&space;Q^n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_i&space;\sim&space;Q^n" title="D_i \sim Q^n" /></a>, i.e. <a href="https://www.codecogs.com/eqnedit.php?latex=\vert&space;D_i&space;\vert&space;=&space;n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vert&space;D_i&space;\vert&space;=&space;n" title="\vert D_i \vert = n" /></a>, and <a href="https://www.codecogs.com/eqnedit.php?latex=D_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_i" title="D_i" /></a> is picked *with replacement* from <a href="https://www.codecogs.com/eqnedit.php?latex=Q&space;\vert&space;D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q&space;\vert&space;D" title="Q \vert D" /></a>.

**Q**: What is <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbb{E}[\vert&space;D&space;\cap&space;D_i&space;\vert]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbb{E}[\vert&space;D&space;\cap&space;D_i&space;\vert]" title="\mathbb{E}[\vert D \cap D_i \vert]" /></a>?

Bagged classifier: <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{h}_D&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;h_{D_i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{h}_D&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;h_{D_i}" title="\hat{h}_D = \frac{1}{m} \sum_{i=1}^{m} h_{D_i}" /></a>

Notice: <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{h}_D&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;h_{D_i}&space;\nrightarrow&space;\bar{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{h}_D&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;h_{D_i}&space;\nrightarrow&space;\bar{h}" title="\hat{h}_D = \frac{1}{m} \sum_{i=1}^{m} h_{D_i} \nrightarrow \bar{h}" /></a> (cannot use W.L.L.N here, W.L.L.N only works for i.i.d. samples).

However, in practice bagging still reduces variance very effectively.

#### Analysis

Although we cannot prove that the new samples are i.i.d., we can show that they are drawn from the original distribution <a href="https://www.codecogs.com/eqnedit.php?latex=P" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P" title="P" /></a>. Assume <a href="https://www.codecogs.com/eqnedit.php?latex=P" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P" title="P" /></a> is discrete, with <a href="https://www.codecogs.com/eqnedit.php?latex=P&space;(X&space;=&space;x_i)&space;=&space;p_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P&space;(X&space;=&space;x_i)&space;=&space;p_i" title="P (X = x_i) = p_i" /></a> over some set <a href="https://www.codecogs.com/eqnedit.php?latex=\Omega&space;=&space;x_1&space;,&space;\ldots&space;,&space;x_N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Omega&space;=&space;x_1&space;,&space;\ldots&space;,&space;x_N" title="\Omega = x_1 , \ldots , x_N" /></a> (<a href="https://www.codecogs.com/eqnedit.php?latex=N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?N" title="N" /></a> very large) (let's ignore the label for now for simplicity)

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;Q(X&space;=&space;x_i)&space;&&space;=&space;\underbrace{\sum_{k=1}^{n}&space;\begin{pmatrix}&space;n&space;\\&space;k&space;\end{pmatrix}&space;p_i^k&space;(1&space;-&space;p_i)^{n-k}&space;}_{\text{Probability&space;that&space;there&space;are&space;}&space;k&space;\text{&space;copies&space;of&space;}&space;x_i&space;\text{&space;in&space;}&space;D}&space;\underbrace{\frac{k}{n}}_{\text{Probability&space;of&space;picking&space;one&space;of&space;these&space;copies}}&space;\\&space;&&space;=&space;\frac{1}{n}&space;\underbrace{\sum_{k=1}^{n}&space;\begin{pmatrix}&space;n&space;\\&space;k&space;\end{pmatrix}&space;p_i^k&space;(1&space;-&space;p_i)^{n&space;-&space;k}&space;k&space;}_{\text{Expected&space;value&space;of&space;binomial&space;distribution&space;with&space;parameter&space;}&space;p_i&space;\text{&space;that&space;}&space;\mathbb{E}[\mathbb{B}&space;(p_i&space;,&space;n)]&space;=&space;n&space;p_i}&space;\\&space;&&space;=&space;\frac{1}{n}&space;n&space;p_i&space;\\&space;&&space;=&space;p_i&space;\leftarrow&space;\textit{TATAAA}\text{!!&space;Each&space;data&space;set&space;}&space;D_l'&space;\text{&space;is&space;drawn&space;from&space;}&space;P&space;\text{,&space;but&space;not&space;independently.}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;Q(X&space;=&space;x_i)&space;&&space;=&space;\underbrace{\sum_{k=1}^{n}&space;\begin{pmatrix}&space;n&space;\\&space;k&space;\end{pmatrix}&space;p_i^k&space;(1&space;-&space;p_i)^{n-k}&space;}_{\text{Probability&space;that&space;there&space;are&space;}&space;k&space;\text{&space;copies&space;of&space;}&space;x_i&space;\text{&space;in&space;}&space;D}&space;\underbrace{\frac{k}{n}}_{\text{Probability&space;of&space;picking&space;one&space;of&space;these&space;copies}}&space;\\&space;&&space;=&space;\frac{1}{n}&space;\underbrace{\sum_{k=1}^{n}&space;\begin{pmatrix}&space;n&space;\\&space;k&space;\end{pmatrix}&space;p_i^k&space;(1&space;-&space;p_i)^{n&space;-&space;k}&space;k&space;}_{\text{Expected&space;value&space;of&space;binomial&space;distribution&space;with&space;parameter&space;}&space;p_i&space;\text{&space;that&space;}&space;\mathbb{E}[\mathbb{B}&space;(p_i&space;,&space;n)]&space;=&space;n&space;p_i}&space;\\&space;&&space;=&space;\frac{1}{n}&space;n&space;p_i&space;\\&space;&&space;=&space;p_i&space;\leftarrow&space;\textit{TATAAA}\text{!!&space;Each&space;data&space;set&space;}&space;D_l'&space;\text{&space;is&space;drawn&space;from&space;}&space;P&space;\text{,&space;but&space;not&space;independently.}&space;\end{align*}" title="\begin{align*} Q(X = x_i) & = \underbrace{\sum_{k=1}^{n} \begin{pmatrix} n \\ k \end{pmatrix} p_i^k (1 - p_i)^{n-k} }_{\text{Probability that there are } k \text{ copies of } x_i \text{ in } D} \underbrace{\frac{k}{n}}_{\text{Probability of picking one of these copies}} \\ & = \frac{1}{n} \underbrace{\sum_{k=1}^{n} \begin{pmatrix} n \\ k \end{pmatrix} p_i^k (1 - p_i)^{n - k} k }_{\text{Expected value of binomial distribution with parameter } p_i \text{ that } \mathbb{E}[\mathbb{B} (p_i , n)] = n p_i} \\ & = \frac{1}{n} n p_i \\ & = p_i \leftarrow \textit{TATAAA}\text{!! Each data set } D_l' \text{ is drawn from } P \text{, but not independently.} \end{align*}" /></a>

There is a simple intuitive argument why <a href="https://www.codecogs.com/eqnedit.php?latex=Q(X&space;=&space;x_i)&space;=&space;P(X&space;=&space;x_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q(X&space;=&space;x_i)&space;=&space;P(X&space;=&space;x_i)" title="Q(X = x_i) = P(X = x_i)" /></a>. So far we assumed that you draw <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a> from <a href="https://www.codecogs.com/eqnedit.php?latex=P^n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P^n" title="P^n" /></a> and then <a href="https://www.codecogs.com/eqnedit.php?latex=Q" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q" title="Q" /></a> picks a sample from <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>. However, you don't have to do it in that order. You can also view sampling from <a href="https://www.codecogs.com/eqnedit.php?latex=Q" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q" title="Q" /></a> in reverse order: Consider that you first use <a href="https://www.codecogs.com/eqnedit.php?latex=Q" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q" title="Q" /></a> to reserve a "spot" in <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>, i.e. a number from <a href="https://www.codecogs.com/eqnedit.php?latex=1,&space;\ldots,&space;n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?1,&space;\ldots,&space;n" title="1, \ldots, n" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i" title="i" /></a> means that you sampled the <a href="https://www.codecogs.com/eqnedit.php?latex=i^{th}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i^{th}" title="i^{th}" /></a> data point in <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>. So far you only have the slot, <a href="https://www.codecogs.com/eqnedit.php?latex=i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i" title="i" /></a>, and you still need to fill it with a data point <a href="https://www.codecogs.com/eqnedit.php?latex=(x_i&space;,&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(x_i&space;,&space;y_i)" title="(x_i , y_i)" /></a>. You do this by sampling <a href="https://www.codecogs.com/eqnedit.php?latex=(x_i&space;,&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(x_i&space;,&space;y_i)" title="(x_i , y_i)" /></a> from <a href="https://www.codecogs.com/eqnedit.php?latex=P" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P" title="P" /></a>. It is now obvious that which slot you picked doesn't really matter, so we have <a href="https://www.codecogs.com/eqnedit.php?latex=Q(X=x)=P(X=x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q(X=x)=P(X=x)" title="Q(X=x)=P(X=x)" /></a>.

#### Bagging Summarized

1. Sample <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> data sets <a href="https://www.codecogs.com/eqnedit.php?latex=D_1,&space;\ldots&space;,&space;D_m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_1,&space;\ldots&space;,&space;D_m" title="D_1, \ldots , D_m" /></a> from <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a> with replacement.
2. For each <a href="https://www.codecogs.com/eqnedit.php?latex=D_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_j" title="D_j" /></a> train a classifier <a href="https://www.codecogs.com/eqnedit.php?latex=h_j&space;()" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h_j&space;()" title="h_j ()" /></a>
3. The final classifier is <a href="https://www.codecogs.com/eqnedit.php?latex=h&space;(\mathbf{x})&space;=&space;\frac{1}{m}&space;\sum_{j=1}^{m}&space;h_j&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h&space;(\mathbf{x})&space;=&space;\frac{1}{m}&space;\sum_{j=1}^{m}&space;h_j&space;(\mathbf{x})" title="h (\mathbf{x}) = \frac{1}{m} \sum_{j=1}^{m} h_j (\mathbf{x})" /></a>

In practice larger <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> results in a better ensemble, however at some point you will obtain diminishing returns. Note that setting <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> unnecessarily high will only slow down your classifier but will **not** increase the error of your classifier.

## Advantages of Bagging

- Easy to implement
- Reduces variance, so has a strong beneficial effect on high variance classifiers
- As the prediction is an average of many classifiers, you obtain a mean score and *variance*. Latter can be interpreted as the uncertainty of the prediction. Especially in regression tasks, such uncertainties are otherwise hard to obtain. For example, imagine the prediction of a house price is $300,000. If a buyer wants to decide how much to offer, it would be very valuable to know if this prediction has standard deviation +-$10,000 or +-$50,000.
- Bagging provides an *unbiased* estimate of the test error, which we refer to as the *out-of-bag-error*. The idea is that each training point was not picked and all the data sets <a href="https://www.codecogs.com/eqnedit.php?latex=D_k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_k" title="D_k" /></a>. If we average the classifiers <a href="https://www.codecogs.com/eqnedit.php?latex=h_k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h_k" title="h_k" /></a> of all such data sets, we obtain a classifier (with a slightly smaller <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a>) that was not trained on <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_i&space;,&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_i&space;,&space;y_i)" title="(\mathbf{x}_i , y_i)" /></a> ever and it is therefore equivalent to a test sample. If we compute the error of all these classifiers, we obtain an estimate of the true test error. The beauty is that we can do this without reducing the training set. We just run bagging as it is intended and obtain this so called out-of-bag error for free.

More formally, for each training point <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_i&space;,&space;y_i)&space;\in&space;D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_i&space;,&space;y_i)&space;\in&space;D" title="(\mathbf{x}_i , y_i) \in D" /></a> let <a href="https://www.codecogs.com/eqnedit.php?latex=S_i&space;=&space;\{&space;k&space;\vert&space;(\mathbf{x}_i&space;,&space;y_i)&space;\notin&space;D_k&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?S_i&space;=&space;\{&space;k&space;\vert&space;(\mathbf{x}_i&space;,&space;y_i)&space;\notin&space;D_k&space;\}" title="S_i = \{ k \vert (\mathbf{x}_i , y_i) \notin D_k \}" /></a> -- in other words <a href="https://www.codecogs.com/eqnedit.php?latex=S_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?S_i" title="S_i" /></a> is a set of all the training sets <a href="https://www.codecogs.com/eqnedit.php?latex=D_k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_k" title="D_k" /></a>, which do not contain <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_k&space;,&space;y_k)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_k&space;,&space;y_k)" title="(\mathbf{x}_k , y_k)" /></a>. Let the averaged classifier over all these data sets be

<a href="https://www.codecogs.com/eqnedit.php?latex=\tilde{h}_i&space;(\mathbf{x})&space;=&space;\frac{1}{\vert&space;S_i&space;\vert}&space;\sum_{k&space;\in&space;S_i}&space;h_k&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\tilde{h}_i&space;(\mathbf{x})&space;=&space;\frac{1}{\vert&space;S_i&space;\vert}&space;\sum_{k&space;\in&space;S_i}&space;h_k&space;(\mathbf{x})" title="\tilde{h}_i (\mathbf{x}) = \frac{1}{\vert S_i \vert} \sum_{k \in S_i} h_k (\mathbf{x})" /></a>

The out-of-bag-error becomes simply the average error/loss that all these classifier yield

<a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon_{\text{OOB}}&space;=&space;\frac{1}{n}&space;\sum_{(\mathbf{x}_i&space;,&space;y_i)&space;\in&space;D}&space;l(\tilde{h}_i&space;(\mathbf{x}_i),&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon_{\text{OOB}}&space;=&space;\frac{1}{n}&space;\sum_{(\mathbf{x}_i&space;,&space;y_i)&space;\in&space;D}&space;l(\tilde{h}_i&space;(\mathbf{x}_i),&space;y_i)" title="\epsilon_{\text{OOB}} = \frac{1}{n} \sum_{(\mathbf{x}_i , y_i) \in D} l(\tilde{h}_i (\mathbf{x}_i), y_i)" /></a>

This is an estimate of the test error, because for each training point we used the subset of classifiers that never saw that training point during training. If <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> is sufficiently large, the fact that we take out some classifiers has no significant effect and the estimate is pretty reliable.

## Random Forest

One of the most famous and useful bagged algorithms is the **Random Forest**! A Random Forest is essentially nothing else but bagged decision trees, with a slightly modified splitting criteria.

The algorithm works as follows:

1. Sample <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> data sets <a href="https://www.codecogs.com/eqnedit.php?latex=D_1,&space;\ldots,&space;D_m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_1,&space;\ldots,&space;D_m" title="D_1, \ldots, D_m" /></a> from <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a> with replacement.
2. For each <a href="https://www.codecogs.com/eqnedit.php?latex=D_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_j" title="D_j" /></a> train a full decision tree <a href="https://www.codecogs.com/eqnedit.php?latex=h_j&space;()" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h_j&space;()" title="h_j ()" /></a> (max-depth=<a href="https://www.codecogs.com/eqnedit.php?latex=\infty" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\infty" title="\infty" /></a>) with one small modification: before each split randomly subsample <a href="https://www.codecogs.com/eqnedit.php?latex=k&space;\leq&space;d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k&space;\leq&space;d" title="k \leq d" /></a> features (without replacement) and only consider these for your split. (This further increases the variance of the trees.)
3. The final classifier is <a href="https://www.codecogs.com/eqnedit.php?latex=h(\mathbf{x})&space;=&space;\frac{1}{m}&space;\sum_{j=1}^{m}&space;h_j&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\mathbf{x})&space;=&space;\frac{1}{m}&space;\sum_{j=1}^{m}&space;h_j&space;(\mathbf{x})" title="h(\mathbf{x}) = \frac{1}{m} \sum_{j=1}^{m} h_j (\mathbf{x})" /></a>.

The Random Forest is one of the best, most popular and easiest to use out-of-the-box classifier. There are two reasons for this:

- The RF only has two hyper-parameters, <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a>. It is extremely *insensitive* to both of these. A good choice for <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a> is <a href="https://www.codecogs.com/eqnedit.php?latex=k&space;=&space;\sqrt{d}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k&space;=&space;\sqrt{d}" title="k = \sqrt{d}" /></a> (where <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> denotes the number of features). You can set <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> as large as you can afford.
- Decision trees do not require a lot of preprocessing. For example, the features can be of different scale, magnitude, or shape. This can be highly advantageous in scenarios with heterogeneous data, for example the medical settings where features could be things like *blood pressure*, *age*, *gender*, ..., each of which is recorded in completely different units.

Useful variants of Random Forests:

- Split each training set into two partitions <a href="https://www.codecogs.com/eqnedit.php?latex=D_l&space;=&space;D_l^A&space;\cup&space;D_l^B" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_l&space;=&space;D_l^A&space;\cup&space;D_l^B" title="D_l = D_l^A \cup D_l^B" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=D_l^A&space;\cap&space;D_l^B&space;=&space;\varnothing" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_l^A&space;\cap&space;D_l^B&space;=&space;\varnothing" title="D_l^A \cap D_l^B = \varnothing" /></a>. Build the tree on <a href="https://www.codecogs.com/eqnedit.php?latex=D_l^A" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_l^A" title="D_l^A" /></a> and estimate the leaf labels on <a href="https://www.codecogs.com/eqnedit.php?latex=D_l^B" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_l^B" title="D_l^B" /></a>. You must stop splitting if a leaf has only a single point in <a href="https://www.codecogs.com/eqnedit.php?latex=D_l^B" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_l^B" title="D_l^B" /></a> in it. This has the advantage that each tree and also the RF classifier become *consistent*.
- Do not grow each tree to its full depth, instead prune based on the leave out samples. This can further improve your bias/variance trade-off.
