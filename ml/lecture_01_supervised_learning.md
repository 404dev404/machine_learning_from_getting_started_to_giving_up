# Supervised Learning

## Introduction

The goal in supervised learning is to make *predictions from data*. For example, one popular application of supervised learning is email filtering. Here, an email (the data instance) needs to be classified as *spam* or *not-spam*. Following the approach of traditional computer science, one might be tempted to write a carefully designed program that follows some rules to decide if an email is spam or not. Although such a program might work reasonably well for a while, it has significant drawbacks. As email spam changes it would have to be rewritten. Spammers could attempt to reverse engineer the software and design messages that circumvent it. And even if it is successful, it could probably not easily be applied to different languages. Machine learning uses a different approach to generate a program that can make predictions from data. Instead of programming it by hand it is *learned* from past data. This process works if we have data instances for which we know exactly what the right prediction would have been. For example past data might be user-annotated as spam or not-spam. A machine learning algorithm can utilize such data to learn a program, a *classifier*, to predict the correct *label* of each annotated data instance. Other successful applications of machine learning include web-search ranking (predict which web-page the user will click on based on his/her search query), placing of online advertisements (predict the expected revenue of an ad, when placed on a homepage, which is seen by a specific user), visual object recognition (predict which object is an image -- e.g. a camera mounted on a self-driving car), face-detection (predict if an image patch contains a human face or not).

## Setup

Let us formalize the supervised machine learning setup. Our training data comes in pairs of inputs <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x},&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x},&space;y)" title="(\mathbf{x}, y)" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}&space;\in&space;\mathcal{R}^d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}&space;\in&space;\mathcal{R}^d" title="\mathbf{x} \in \mathcal{R}^d" /></a> is the input instance and <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a> is its label. The entire training data is denoted as

<a href="https://www.codecogs.com/eqnedit.php?latex=D=\{&space;(\mathbf{x}_1,&space;y_1),&space;\ldots,&space;(\mathbf{x}_n,&space;y_n)&space;\}&space;\subseteq&space;\mathcal{R}^d&space;\times&space;\mathcal{C}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D=\{&space;(\mathbf{x}_1,&space;y_1),&space;\ldots,&space;(\mathbf{x}_n,&space;y_n)&space;\}&space;\subseteq&space;\mathcal{R}^d&space;\times&space;\mathcal{C}" title="D=\{ (\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n) \} \subseteq \mathcal{R}^d \times \mathcal{C}" /></a>

where:

- <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{R}^d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{R}^d" title="\mathcal{R}^d" /></a> is the d-dimensional feature space
- <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> is the input vector of the <a href="https://www.codecogs.com/eqnedit.php?latex=i^{th}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i^{th}" title="i^{th}" /></a> sample
- <a href="https://www.codecogs.com/eqnedit.php?latex=y_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i" title="y_i" /></a> is the label of the <a href="https://www.codecogs.com/eqnedit.php?latex=i^{th}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i^{th}" title="i^{th}" /></a> sample
- <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{C}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{C}" title="\mathcal{C}" /></a> is the label space

The data point <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_i,&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_i,&space;y_i)" title="(\mathbf{x}_i, y_i)" /></a> are drawn from some (unknown) distribution <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{P}(X,&space;Y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{P}(X,&space;Y)" title="\mathcal{P}(X, Y)" /></a>. Ultimately we would like to learn a function <a href="https://www.codecogs.com/eqnedit.php?latex=h" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h" title="h" /></a> such that for a new pair <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x},&space;y)&space;\sim&space;\mathcal{P}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x},&space;y)&space;\sim&space;\mathcal{P}" title="(\mathbf{x}, y) \sim \mathcal{P}" /></a>, we have <a href="https://www.codecogs.com/eqnedit.php?latex=h(\mathbf{x})=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\mathbf{x})=y" title="h(\mathbf{x})=y" /></a> with high probability (or <a href="https://www.codecogs.com/eqnedit.php?latex=h(\mathbf{x})\approx&space;y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\mathbf{x})\approx&space;y" title="h(\mathbf{x})\approx y" /></a>). We will get to this later. For now let us go through some examples of <a href="https://www.codecogs.com/eqnedit.php?latex=X" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X" title="X" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=Y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y" title="Y" /></a>.

### Examples of Label Spaces

There are multiple scenarios for the label space <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{C}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{C}" title="\mathcal{C}" /></a>:

| | | |
| - | - | - |
| Binary classification | <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{C}=\{&space;0,&space;1&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{C}=\{&space;0,&space;1&space;\}" title="\mathcal{C}=\{ 0, 1 \}" /></a> or <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{C}=\{&space;-1,&space;&plus;1&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{C}=\{&space;-1,&space;&plus;1&space;\}" title="\mathcal{C}=\{ -1, +1 \}" /></a> | E.g. spam filtering. An email is either spam (<a href="https://www.codecogs.com/eqnedit.php?latex=&plus;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?&plus;1" title="+1" /></a>), or not (<a href="https://www.codecogs.com/eqnedit.php?latex=-1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-1" title="-1" /></a>). |
| Multi-class classification | <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{C}=\{&space;1,&space;2,&space;\ldots,&space;K&space;\}&space;(K&space;\geq&space;2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{C}=\{&space;1,&space;2,&space;\ldots,&space;K&space;\}&space;(K&space;\geq&space;2)" title="\mathcal{C}=\{ 1, 2, \ldots, K \} (K \geq 2)" /></a> | E.g. face classification. A person can be exactly one of <a href="https://www.codecogs.com/eqnedit.php?latex=K" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K" title="K" /></a> identities (e.g., 1="Barack Obama", 2="George W. Bush", etc.) |
| Regression | <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{C}=\mathbb{R}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{C}=\mathbb{R}" title="\mathcal{C}=\mathbb{R}" /></a> | E.g. predict future temperature or the height of a person. |

## Examples of Feature Vector

We call <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> a feature vector. Each one of its <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> dimensions is a features describing the <a href="https://www.codecogs.com/eqnedit.php?latex=i^{th}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i^{th}" title="i^{th}" /></a> sample. Let us look at some examples:

- Patient Data in a hospital. <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i&space;=&space;(x_i^1,&space;x_i^2,&space;\ldots,&space;x_i^d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i&space;=&space;(x_i^1,&space;x_i^2,&space;\ldots,&space;x_i^d)" title="\mathbf{x}_i = (x_i^1, x_i^2, \ldots, x_i^d)" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=x_i^1=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i^1=0" title="x_i^1=0" /></a> or <a href="https://www.codecogs.com/eqnedit.php?latex=1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?1" title="1" /></a>, may refer to the patient <a href="https://www.codecogs.com/eqnedit.php?latex=i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i" title="i" /></a>'s gender, <a href="https://www.codecogs.com/eqnedit.php?latex=x_i^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i^2" title="x_i^2" /></a> could be the height of patient <a href="https://www.codecogs.com/eqnedit.php?latex=i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i" title="i" /></a> in cm, and <a href="https://www.codecogs.com/eqnedit.php?latex=x_i^3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i^3" title="x_i^3" /></a> may be his/her in years, etc. In this case, <a href="https://www.codecogs.com/eqnedit.php?latex=d&space;\leq&space;100" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d&space;\leq&space;100" title="d \leq 100" /></a> and the feature vector is dense, i.e., the number of nonzero coordinates in <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> is large relative to <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a>.
- Text document in bag-of-words format. <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i=(x_i^1,&space;x_i^2,&space;\ldots,&space;x_i^d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i=(x_i^1,&space;x_i^2,&space;\ldots,&space;x_i^d)" title="\mathbf{x}_i=(x_i^1, x_i^2, \ldots, x_i^d)" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=x_i^{\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i^{\alpha}" title="x_i^{\alpha}" /></a> is the number of occurrences of the <a href="https://www.codecogs.com/eqnedit.php?latex={\alpha}^{th}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?{\alpha}^{th}" title="{\alpha}^{th}" /></a> word in a dictionary in document <a href="https://www.codecogs.com/eqnedit.php?latex=i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i" title="i" /></a> (often referred to as term frequencies). In this case, <a href="https://www.codecogs.com/eqnedit.php?latex=d&space;\sim&space;100,000&space;-&space;10M" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d&space;\sim&space;100,000&space;-&space;10M" title="d \sim 100,000 - 10M" /></a> and the feature vector is sparse, i.e., <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> consists of mostly zeros. A common way to avoid the use of a dictionary is to use feature hashing instead of directly hash any string to a dimension index (the advantage is that no dictionary is needed, but a minor disadvantage can be that multiple words are hashed into the same dimension.) A popular inprovement over bag-of-words features is TF-IDF, which down-scales common words and highlights rare words.
- Images. Here, the features typically represent pixel values. <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i&space;=&space;(x_i^1,&space;x_i^2,&space;\ldots,&space;x_i^{3k})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i&space;=&space;(x_i^1,&space;x_i^2,&space;\ldots,&space;x_i^{3k})" title="\mathbf{x}_i = (x_i^1, x_i^2, \ldots, x_i^{3k})" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=x_i^{3j-2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i^{3j-2}" title="x_i^{3j-2}" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=x_i^{3j-1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i^{3j-1}" title="x_i^{3j-1}" /></a>, and <a href="https://www.codecogs.com/eqnedit.php?latex=x_i^{3j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_i^{3j}" title="x_i^{3j}" /></a> refer to the red, green, and blue values of the <a href="https://www.codecogs.com/eqnedit.php?latex=j^{th}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?j^{th}" title="j^{th}" /></a> pixel in the image. In this case, <a href="https://www.codecogs.com/eqnedit.php?latex=d&space;\sim&space;100,000&space;-&space;10M" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d&space;\sim&space;100,000&space;-&space;10M" title="d \sim 100,000 - 10M" /></a> and the feature fector is dense. A <a href="https://www.codecogs.com/eqnedit.php?latex=7MP" target="_blank"><img src="https://latex.codecogs.com/gif.latex?7MP" title="7MP" /></a> camera results in <a href="https://www.codecogs.com/eqnedit.php?latex=7M&space;\times&space;3&space;=&space;21M" target="_blank"><img src="https://latex.codecogs.com/gif.latex?7M&space;\times&space;3&space;=&space;21M" title="7M \times 3 = 21M" /></a> features.

## Hypothesis Classes and No Free Lunch

Before we can find a function <a href="https://www.codecogs.com/eqnedit.php?latex=h" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h" title="h" /></a>, we must specify what type of function it is that we are looking for. It could be an artificial neural network, a decision tree or many other types of classifiers. We call the set of possible functions the *hypothesis class*. By specifying the hypothesis class, we are encoding important assumptions about the type of problem we are trying to learn. The *No Free Lunch Theorem* states that every successful ML algorithm must make assumptions. This also means that there is no single ML algorithm that works for every setting.

## Loss Functions

There are typically two steps involved in learning a hypothesis function <a href="https://www.codecogs.com/eqnedit.php?latex=h()" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h()" title="h()" /></a>. First, we select the type of machine learning algorithm that we think is appropriate for this particular learning problem. This defines the hypothesis class <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{H}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{H}" title="\mathcal{H}" /></a>, i.e. the set of functions we can possibly learn. The second step is to find the best function within this class, <a href="https://www.codecogs.com/eqnedit.php?latex=h&space;\in&space;\mathcal{H}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h&space;\in&space;\mathcal{H}" title="h \in \mathcal{H}" /></a>. This second step is the actual learning process and often, but not always, involves an optimization problem. Essentially, we try to find a function <a href="https://www.codecogs.com/eqnedit.php?latex=h" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h" title="h" /></a> within the hypothesis class that makes the fewest mistakes within our training data. (If there is not a single function we typically try to choose the "simplest" by some notion of simplicity.) How can we find the best function? For this we need some way to evaluate what it means for one function to be better than another. This is where the loss function (aka risk function) comes in. A loss function evaluates a hypothesis <a href="https://www.codecogs.com/eqnedit.php?latex=h&space;\in&space;\mathcal{H}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h&space;\in&space;\mathcal{H}" title="h \in \mathcal{H}" /></a> on our training data and tells us how bad it is. The higher the loss, the worse it is -- a loss of zero means it makes perfect predictions. It is common practice to normalize the loss by the total number of training samples, <a href="https://www.codecogs.com/eqnedit.php?latex=n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n" title="n" /></a>, so that the output can be interpreted as the average loss per sample (and is independent of <a href="https://www.codecogs.com/eqnedit.php?latex=n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n" title="n" /></a>).

### Examples:

#### Zero-One Loss:

The simplest loss function is the zero-one loss. It literally counts how many mistakes an hypothesis function <a href="https://www.codecogs.com/eqnedit.php?latex=h" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h" title="h" /></a> makes on the training set. For every single example it suffers a loss of 1 if it is mispredicted, and 0 otherwise. The normalized zero-one loss returns the fraction of misclassified training samples, also often referred to as the training error. The zero-one loss is often used to evaluate classifiers in multi-class/binary classification settings but rarely useful to guide optimization procedures because the function is non-differentiable and non-continuous. Formally, the zero-one loss can be stated has:

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{L}_{0/1}(h)&space;=&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;\delta_{h(\mathbf{x}_i)&space;\neq&space;y_i},&space;\enspace&space;where&space;\enspace&space;\delta_{h(\mathbf{x}_i)&space;\neq&space;y_i}&space;=&space;\left\{&space;\begin{array}{ll}&space;1,&space;\enspace&space;if&space;\enspace&space;h(\mathbf{x}_i)&space;\neq&space;y_i\\&space;0,&space;\enspace&space;otherwise.&space;\end{array}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{L}_{0/1}(h)&space;=&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;\delta_{h(\mathbf{x}_i)&space;\neq&space;y_i},&space;\enspace&space;where&space;\enspace&space;\delta_{h(\mathbf{x}_i)&space;\neq&space;y_i}&space;=&space;\left\{&space;\begin{array}{ll}&space;1,&space;\enspace&space;if&space;\enspace&space;h(\mathbf{x}_i)&space;\neq&space;y_i\\&space;0,&space;\enspace&space;otherwise.&space;\end{array}\right." title="\mathcal{L}_{0/1}(h) = \frac{1}{n} \sum_{i=1}^{n} \delta_{h(\mathbf{x}_i) \neq y_i}, \enspace where \enspace \delta_{h(\mathbf{x}_i) \neq y_i} = \left\{ \begin{array}{ll} 1, \enspace if \enspace h(\mathbf{x}_i) \neq y_i\\ 0, \enspace otherwise. \end{array}\right." /></a>

This loss function returns the *error rate* on this data set <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>. For every example that the classifier misclassifiies (i.e. get wrong) a loss of 1 is suffered, whereas correctly classified samples lead to 0 loss.

#### Squared Loss:

The squared loss function is typically used in regression settings. It iterates over all training samples and suffers the loss <a href="https://www.codecogs.com/eqnedit.php?latex=(h(\mathbf{x}_i)-y_i)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(h(\mathbf{x}_i)-y_i)^2" title="(h(\mathbf{x}_i)-y_i)^2" /></a>. The squaring has two effects: (i) the loss suffered is always nonnegative; (ii) the loss suffered grows quadratically with the absolute mispredicted amount. The latter property encourages no predictions to be really far off (or the penalty would be so large that a different hypothesis function is likely better suited). On the flipside, if a prediction is very close to be correct, the square will be tiny and little attention will be given to that example to obtain zero error. For example, if <a href="https://www.codecogs.com/eqnedit.php?latex=\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert&space;=&space;0.001" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert&space;=&space;0.001" title="\vert h(\mathbf{x}_i) - y_i \vert = 0.001" /></a> the squared loss will be even smaller, <a href="https://www.codecogs.com/eqnedit.php?latex=0.000001" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0.000001" title="0.000001" /></a>, and will likely never be fully corrected. If, given an input <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}" title="\mathbf{x}" /></a>, the label <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a> is probabilistic according to some distribution <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x})" title="P(y \vert \mathbf{x})" /></a> then the optimal prediction to minimize the squared loss is to predict the expected value, i.e. <a href="https://www.codecogs.com/eqnedit.php?latex=h(\mathbf{x})=\mathbb{E}_{P(y&space;\vert&space;\mathbf{x})}[y]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\mathbf{x})=\mathbb{E}_{P(y&space;\vert&space;\mathbf{x})}[y]" title="h(\mathbf{x})=\mathbb{E}_{P(y \vert \mathbf{x})}[y]" /></a>. Formally the squared loss is:

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{L}_{sq}(h)=\frac{1}{n}\sum_{i=1}^{n}&space;(h(\mathbf{x}_i)&space;-&space;y_i)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{L}_{sq}(h)=\frac{1}{n}\sum_{i=1}^{n}&space;(h(\mathbf{x}_i)&space;-&space;y_i)^2" title="\mathcal{L}_{sq}(h)=\frac{1}{n}\sum_{i=1}^{n} (h(\mathbf{x}_i) - y_i)^2" /></a>.

#### Absolute Loss:

Similar to the squared loss, the absolute loss function is also typically used in regression settings. It suffers the penalties <a href="https://www.codecogs.com/eqnedit.php?latex=\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert" title="\vert h(\mathbf{x}_i) - y_i \vert" /></a>. Because the suffered loss grows linearly with the mispredictions it is more suitable for noisy data (when some mispredictions are unavoidable and shouldn't dominate the loss). If, given an input <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}" title="\mathbf{x}" /></a>, the label <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a> is probabilistic according to some distribution <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x})" title="P(y \vert \mathbf{x})" /></a> then the optimal prediction to minimize the absolute loss is to predict the median value, i.e. <a href="https://www.codecogs.com/eqnedit.php?latex=h(\mathbf{x})&space;=&space;MEDIAN_{P(y&space;\vert&space;\mathbf{x})}&space;[y]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\mathbf{x})&space;=&space;MEDIAN_{P(y&space;\vert&space;\mathbf{x})}&space;[y]" title="h(\mathbf{x}) = MEDIAN_{P(y \vert \mathbf{x})} [y]" /></a>. Formally, the absolute loss can be stated as:

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{L}_{abs}(h)=\frac{1}{n}&space;\sum_{i=1}^{n}&space;\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{L}_{abs}(h)=\frac{1}{n}&space;\sum_{i=1}^{n}&space;\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert" title="\mathcal{L}_{abs}(h)=\frac{1}{n} \sum_{i=1}^{n} \vert h(\mathbf{x}_i) - y_i \vert" /></a>.

## Generalization:

Given a loss function, we can then attempt to find the function <a href="https://www.codecogs.com/eqnedit.php?latex=h" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h" title="h" /></a> that minimizes the loss:

<a href="https://www.codecogs.com/eqnedit.php?latex=h&space;=&space;\operatorname*{argmin}_{h&space;\in&space;\mathcal{H}}&space;\mathcal{L}(h)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h&space;=&space;\operatorname*{argmin}_{h&space;\in&space;\mathcal{H}}&space;\mathcal{L}(h)" title="h = \operatorname*{argmin}_{h \in \mathcal{H}} \mathcal{L}(h)" /></a>

A big part of machine learning focuses on the question, how to do this minimization efficiently.

If you find a function <a href="https://www.codecogs.com/eqnedit.php?latex=h(&space;\cdot&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(&space;\cdot&space;)" title="h( \cdot )" /></a> wwith low loss on your data <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>, how do you know whether it will still get examples right that are not in <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>?

Bad example: "memorizer" <a href="https://www.codecogs.com/eqnedit.php?latex=h(\cdot)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\cdot)" title="h(\cdot)" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=h(\mathbf{x})&space;=&space;\left\{&space;\begin{array}{ll}&space;y_i,&space;\enspace&space;if&space;\enspace&space;\exists&space;(\mathbf{x}_i,&space;y_i)&space;\in&space;D,&space;\enspace&space;s.t.,&space;\enspace&space;\mathbf{x}=\mathbf{x}_i&space;,&space;\\&space;0,&space;\enspace&space;otherwise.&space;\end{array}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\mathbf{x})&space;=&space;\left\{&space;\begin{array}{ll}&space;y_i,&space;\enspace&space;if&space;\enspace&space;\exists&space;(\mathbf{x}_i,&space;y_i)&space;\in&space;D,&space;\enspace&space;s.t.,&space;\enspace&space;\mathbf{x}=\mathbf{x}_i&space;,&space;\\&space;0,&space;\enspace&space;otherwise.&space;\end{array}\right." title="h(\mathbf{x}) = \left\{ \begin{array}{ll} y_i, \enspace if \enspace \exists (\mathbf{x}_i, y_i) \in D, \enspace s.t., \enspace \mathbf{x}=\mathbf{x}_i , \\ 0, \enspace otherwise. \end{array}\right." /></a>

For this <a href="https://www.codecogs.com/eqnedit.php?latex=h(\cdot)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\cdot)" title="h(\cdot)" /></a>, we get <a href="https://www.codecogs.com/eqnedit.php?latex=0\%" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0\%" title="0\%" /></a> error on the training data <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>, but does horribly with samples not in <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a>, i.e., there's the overfitting issue with this function.

## Train / Test Splits

To resolve the overfitting issue, we usually *split* <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a> into three subsets: <a href="https://www.codecogs.com/eqnedit.php?latex=D_{TR}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{TR}" title="D_{TR}" /></a>, as the training data, <a href="https://www.codecogs.com/eqnedit.php?latex=D_{VA}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{VA}" title="D_{VA}" /></a>, as the validation data, and <a href="https://www.codecogs.com/eqnedit.php?latex=D_{TE}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{TE}" title="D_{TE}" /></a>, as the test data. Usually, they are split into a proportion of <a href="https://www.codecogs.com/eqnedit.php?latex=80\%" target="_blank"><img src="https://latex.codecogs.com/gif.latex?80\%" title="80\%" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=10\%" target="_blank"><img src="https://latex.codecogs.com/gif.latex?10\%" title="10\%" /></a>, and <a href="https://www.codecogs.com/eqnedit.php?latex=10\%" target="_blank"><img src="https://latex.codecogs.com/gif.latex?10\%" title="10\%" /></a>. Then, we choose <a href="https://www.codecogs.com/eqnedit.php?latex=h(\cdot)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\cdot)" title="h(\cdot)" /></a> based on <a href="https://www.codecogs.com/eqnedit.php?latex=D_{TR}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{TR}" title="D_{TR}" /></a>, and evaluate <a href="https://www.codecogs.com/eqnedit.php?latex=h(\cdot)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\cdot)" title="h(\cdot)" /></a> on <a href="https://www.codecogs.com/eqnedit.php?latex=D_{TE}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{TE}" title="D_{TE}" /></a>.

*Quiz*: Why do we need <a href="https://www.codecogs.com/eqnedit.php?latex=D_{VA}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{VA}" title="D_{VA}" /></a>?

<a href="https://www.codecogs.com/eqnedit.php?latex=D_{VA}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{VA}" title="D_{VA}" /></a> is used to check whether the <a href="https://www.codecogs.com/eqnedit.php?latex=h(\cdot)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\cdot)" title="h(\cdot)" /></a> obtained from <a href="https://www.codecogs.com/eqnedit.php?latex=D_{TR}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{TR}" title="D_{TR}" /></a> suffers from the overfitting issue. <a href="https://www.codecogs.com/eqnedit.php?latex=h(\cdot)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\cdot)" title="h(\cdot)" /></a> will need to be validated on <a href="https://www.codecogs.com/eqnedit.php?latex=D_{VA}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{VA}" title="D_{VA}" /></a>, if the loss is too large, <a href="https://www.codecogs.com/eqnedit.php?latex=h(\cdot)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\cdot)" title="h(\cdot)" /></a> will get revised based on <a href="https://www.codecogs.com/eqnedit.php?latex=D_{TR}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{TR}" title="D_{TR}" /></a>, and validated again on <a href="https://www.codecogs.com/eqnedit.php?latex=D_{VA}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{VA}" title="D_{VA}" /></a>. This process will keep going back and forth until it gives low loss on <a href="https://www.codecogs.com/eqnedit.php?latex=D_{VA}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{VA}" title="D_{VA}" /></a>. Here's a trade-off between the sizes of <a href="https://www.codecogs.com/eqnedit.php?latex=D_{TR}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{TR}" title="D_{TR}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=D_{VA}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{VA}" title="D_{VA}" /></a>: the training results will be better for a larger <a href="https://www.codecogs.com/eqnedit.php?latex=D_{TR}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{TR}" title="D_{TR}" /></a>, but the validation will be more reliable (less noisy) if <a href="https://www.codecogs.com/eqnedit.php?latex=D_{VA}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{VA}" title="D_{VA}" /></a> is larger.

### How to Split the Data?

You have to be very careful when you split the data in Train, Validation, Test. The test set must simulate a real test scenario, i.e. you want to simulate the setting that you will encounter in real life. For example, if you want to train an email spam filter, you train a system on past data to predict if future email is spam. Here it is important to split train / test temporally -- so that you strictly predict the future from the past. If there is no such thing as a temporal component, it is often best to split uniformly at random. Definitely never split alphabetically, or by feature values.

- *By time*, if the data is temporally collected.
    
    In general, if the data has a temporal component, we *must* split it by time.

- *Uniformly at random*, if (and, in general, only if) the data is *i.i.d.*.
    
    The test error (or testing loss) approximates the true generalization error/loss.

## Putting Everything Together:

We train our classifier by minimizing the training loss:

**Learning**: <a href="https://www.codecogs.com/eqnedit.php?latex=h^{\ast}(\cdot)=\operatorname*{argmin}_{h(\cdot)&space;\in&space;\mathcal{H}}&space;\frac{1}{\vert&space;D_{TR}&space;\vert}&space;\sum_{(\mathbf{x},&space;y)&space;\in&space;D_{TR}}&space;\ell(\mathbf{x},&space;y&space;\vert&space;h(\cdot))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{\ast}(\cdot)=\operatorname*{argmin}_{h(\cdot)&space;\in&space;\mathcal{H}}&space;\frac{1}{\vert&space;D_{TR}&space;\vert}&space;\sum_{(\mathbf{x},&space;y)&space;\in&space;D_{TR}}&space;\ell(\mathbf{x},&space;y&space;\vert&space;h(\cdot))" title="h^{\ast}(\cdot)=\operatorname*{argmin}_{h(\cdot) \in \mathcal{H}} \frac{1}{\vert D_{TR} \vert} \sum_{(\mathbf{x}, y) \in D_{TR}} \ell(\mathbf{x}, y \vert h(\cdot))" /></a>,

where <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{H}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{H}" title="\mathcal{H}" /></a> is the hypothetical class (i.e., the set of all possible classifier <a href="https://www.codecogs.com/eqnedit.php?latex=h(\cdot)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\cdot)" title="h(\cdot)" /></a>). In other words, we are trying to find a hypothesis <a href="https://www.codecogs.com/eqnedit.php?latex=h" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h" title="h" /></a> which would have performed well on the past/known data.

We evaluate our classifier on the testing loss:

**Evaluation**: <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon_{TE}&space;=&space;\frac{1}{\vert&space;D_{TE}&space;\vert}&space;\sum_{(\mathbf{x},&space;y)&space;\in&space;D_{TE}}&space;\ell&space;(\mathbf{x},&space;y&space;\vert&space;h^{\ast}(\cdot))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon_{TE}&space;=&space;\frac{1}{\vert&space;D_{TE}&space;\vert}&space;\sum_{(\mathbf{x},&space;y)&space;\in&space;D_{TE}}&space;\ell&space;(\mathbf{x},&space;y&space;\vert&space;h^{\ast}(\cdot))" title="\epsilon_{TE} = \frac{1}{\vert D_{TE} \vert} \sum_{(\mathbf{x}, y) \in D_{TE}} \ell (\mathbf{x}, y \vert h^{\ast}(\cdot))" /></a>.

If the samples are drawn i.i.d. from the same distribution <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{P}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{P}" title="\mathcal{P}" /></a>, then the testing loss is an unbiased estimator of the true **generalization loss**:

**Generalization**: <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon&space;=&space;\mathbb{E}_{(\mathbf{x},&space;y)&space;\sim&space;\mathcal{P}}&space;[\ell&space;(\mathbf{x},&space;y&space;\vert&space;h^{\ast}&space;(\cdot))]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon&space;=&space;\mathbb{E}_{(\mathbf{x},&space;y)&space;\sim&space;\mathcal{P}}&space;[\ell&space;(\mathbf{x},&space;y&space;\vert&space;h^{\ast}&space;(\cdot))]" title="\epsilon = \mathbb{E}_{(\mathbf{x}, y) \sim \mathcal{P}} [\ell (\mathbf{x}, y \vert h^{\ast} (\cdot))]" /></a>.

*Quiz*: Why does <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon_{TE}&space;\rightarrow&space;\epsilon" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon_{TE}&space;\rightarrow&space;\epsilon" title="\epsilon_{TE} \rightarrow \epsilon" /></a> as <a href="https://www.codecogs.com/eqnedit.php?latex=\vert&space;D_{TE}&space;\vert&space;\rightarrow&space;&plus;&space;\infty" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vert&space;D_{TE}&space;\vert&space;\rightarrow&space;&plus;&space;\infty" title="\vert D_{TE} \vert \rightarrow + \infty" /></a>? This is due to the weak law of large numbers, which says that the empirical average of data drawn from a distribution converges to its mean.

*No free lunch*. Every ML algorithm has to make assumptions on which hypothesis class <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{H}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{H}" title="\mathcal{H}" /></a> should you choose? This choice depends on the data, and encodes *your assumptions* about the data set/distribution <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{P}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{P}" title="\mathcal{P}" /></a>. Clearly, there's no one perfect <a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{H}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{H}" title="\mathcal{H}" /></a> for all problems.

*Example*. Assume that <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_1,&space;y_1)&space;=&space;(1,&space;1)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_1,&space;y_1)&space;=&space;(1,&space;1)" title="(\mathbf{x}_1, y_1) = (1, 1)" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_2,&space;y_2)&space;=&space;(2,&space;2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_2,&space;y_2)&space;=&space;(2,&space;2)" title="(\mathbf{x}_2, y_2) = (2, 2)" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_3,&space;y_3)&space;=&space;(3,&space;3)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_3,&space;y_3)&space;=&space;(3,&space;3)" title="(\mathbf{x}_3, y_3) = (3, 3)" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_4,&space;y_4)&space;=&space;(4,&space;4)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_4,&space;y_4)&space;=&space;(4,&space;4)" title="(\mathbf{x}_4, y_4) = (4, 4)" /></a>, and <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x}_5,&space;y_5)&space;=&space;(5,&space;5)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x}_5,&space;y_5)&space;=&space;(5,&space;5)" title="(\mathbf{x}_5, y_5) = (5, 5)" /></a>.

Question: what is the value of <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a> if <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}=2.5" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}=2.5" title="\mathbf{x}=2.5" /></a>? Well, it is utterly *impossible* to know the answer without assumptions. The most common assumption of ML algorithms is that the function to be approximated is locally smooth.
