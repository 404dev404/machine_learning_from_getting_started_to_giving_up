# Gaussian Processes

## Properties of Multivariate Gaussian Distributions

We first review the definition and properties of Gaussian distribution:

A Gaussian random variable <a href="https://www.codecogs.com/eqnedit.php?latex=X&space;\sim&space;\mathcal{N}&space;(\mu&space;,&space;\Sigma)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X&space;\sim&space;\mathcal{N}&space;(\mu&space;,&space;\Sigma)" title="X \sim \mathcal{N} (\mu , \Sigma)" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=\mu" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu" title="\mu" /></a> is the mean and <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma" title="\Sigma" /></a> is the covariance matrix has the following probability density function:

<a href="https://www.codecogs.com/eqnedit.php?latex=P(x;&space;\mu&space;,&space;\Sigma)&space;=&space;\frac{1}{(2\pi)^{\frac{d}{2}}&space;\vert&space;\Sigma&space;\vert}&space;e^{-\frac{1}{2}&space;\big(&space;(x&space;-&space;\mu)^{\top}&space;\Sigma^{-1}&space;(x&space;-&space;\mu)&space;\big)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(x;&space;\mu&space;,&space;\Sigma)&space;=&space;\frac{1}{(2\pi)^{\frac{d}{2}}&space;\vert&space;\Sigma&space;\vert}&space;e^{-\frac{1}{2}&space;\big(&space;(x&space;-&space;\mu)^{\top}&space;\Sigma^{-1}&space;(x&space;-&space;\mu)&space;\big)}" title="P(x; \mu , \Sigma) = \frac{1}{(2\pi)^{\frac{d}{2}} \vert \Sigma \vert} e^{-\frac{1}{2} \big( (x - \mu)^{\top} \Sigma^{-1} (x - \mu) \big)}" /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=\vert&space;\Sigma&space;\vert" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vert&space;\Sigma&space;\vert" title="\vert \Sigma \vert" /></a> is the determinant of <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma" title="\Sigma" /></a>.

The Gaussian distribution occurs very often in real world data. This is for a good reason: **the Central Limit Theorem (CLT)**. The CLT states that the arithmetic mean of <a href="https://www.codecogs.com/eqnedit.php?latex=m&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m&space;>&space;0" title="m > 0" /></a> samples is approximately normal distributed -- independent of the original sample distribution (provided it has finite mean and variance).

#### Once Gaussian Always Gaussian

Let Gaussian random variable <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;\begin{bmatrix}&space;y_A&space;\\&space;y_B&space;\end{bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;\begin{bmatrix}&space;y_A&space;\\&space;y_B&space;\end{bmatrix}" title="y = \begin{bmatrix} y_A \\ y_B \end{bmatrix}" /></a>, mean <a href="https://www.codecogs.com/eqnedit.php?latex=\mu&space;=&space;\begin{bmatrix}&space;\mu_A&space;\\&space;\mu_B&space;\end{bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu&space;=&space;\begin{bmatrix}&space;\mu_A&space;\\&space;\mu_B&space;\end{bmatrix}" title="\mu = \begin{bmatrix} \mu_A \\ \mu_B \end{bmatrix}" /></a> and covariance matrix <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma&space;=&space;\begin{bmatrix}&space;\Sigma_{AA},&space;\Sigma_{AB}&space;\\&space;\Sigma_{BA},&space;\Sigma_{BB}&space;\end{bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma&space;=&space;\begin{bmatrix}&space;\Sigma_{AA},&space;\Sigma_{AB}&space;\\&space;\Sigma_{BA},&space;\Sigma_{BB}&space;\end{bmatrix}" title="\Sigma = \begin{bmatrix} \Sigma_{AA}, \Sigma_{AB} \\ \Sigma_{BA}, \Sigma_{BB} \end{bmatrix}" /></a>. We have the following properties:

1. **Normalization**:

<a href="https://www.codecogs.com/eqnedit.php?latex=\int_y&space;p&space;(y&space;;&space;\mu&space;,&space;\Sigma)&space;dy&space;=&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\int_y&space;p&space;(y&space;;&space;\mu&space;,&space;\Sigma)&space;dy&space;=&space;1" title="\int_y p (y ; \mu , \Sigma) dy = 1" /></a>

2. **Marginalization**: The marginal distribution <a href="https://www.codecogs.com/eqnedit.php?latex=p&space;(&space;y_A&space;)&space;=&space;\int_{y_B}&space;p&space;(y_A&space;,&space;y_B;&space;\mu&space;,&space;\Sigma)&space;d&space;y_B" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p&space;(&space;y_A&space;)&space;=&space;\int_{y_B}&space;p&space;(y_A&space;,&space;y_B;&space;\mu&space;,&space;\Sigma)&space;d&space;y_B" title="p ( y_A ) = \int_{y_B} p (y_A , y_B; \mu , \Sigma) d y_B" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=p&space;(&space;y_B&space;)&space;=&space;\int_{y_A}&space;p&space;(y_A&space;,&space;y_B;&space;\mu&space;,&space;\Sigma)&space;d&space;y_A" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p&space;(&space;y_B&space;)&space;=&space;\int_{y_A}&space;p&space;(y_A&space;,&space;y_B;&space;\mu&space;,&space;\Sigma)&space;d&space;y_A" title="p ( y_B ) = \int_{y_A} p (y_A , y_B; \mu , \Sigma) d y_A" /></a> are Gaussian:

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;y_A&space;\sim&space;\mathcal{N}&space;(\mu_A&space;,&space;\Sigma_{AA})&space;\\&space;&&space;y_B&space;\sim&space;\mathcal{N}&space;(\mu_B&space;,&space;\Sigma_{BB})&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;&&space;y_A&space;\sim&space;\mathcal{N}&space;(\mu_A&space;,&space;\Sigma_{AA})&space;\\&space;&&space;y_B&space;\sim&space;\mathcal{N}&space;(\mu_B&space;,&space;\Sigma_{BB})&space;\end{align*}" title="\begin{align*} & y_A \sim \mathcal{N} (\mu_A , \Sigma_{AA}) \\ & y_B \sim \mathcal{N} (\mu_B , \Sigma_{BB}) \end{align*}" /></a>

3. **Summation**: If <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;\sim&space;\mathcal{N}&space;(\mu&space;,&space;\Sigma)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;\sim&space;\mathcal{N}&space;(\mu&space;,&space;\Sigma)" title="y \sim \mathcal{N} (\mu , \Sigma)" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=y'&space;\sim&space;\mathcal{N}&space;(\mu'&space;,&space;\Sigma')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y'&space;\sim&space;\mathcal{N}&space;(\mu'&space;,&space;\Sigma')" title="y' \sim \mathcal{N} (\mu' , \Sigma')" /></a>, then

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;&plus;&space;y'&space;\sim&space;\mathcal{N}&space;(\mu&space;&plus;&space;\mu'&space;,&space;\Sigma&space;&plus;&space;\Sigma')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;&plus;&space;y'&space;\sim&space;\mathcal{N}&space;(\mu&space;&plus;&space;\mu'&space;,&space;\Sigma&space;&plus;&space;\Sigma')" title="y + y' \sim \mathcal{N} (\mu + \mu' , \Sigma + \Sigma')" /></a>

4. **Conditioning**: The conditional distribution of <a href="https://www.codecogs.com/eqnedit.php?latex=y_A" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_A" title="y_A" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=y_B" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_B" title="y_B" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=p&space;(y_A&space;\vert&space;y_B)&space;=&space;\frac{p&space;(y_A&space;,&space;y_B&space;;&space;\mu&space;,&space;\Sigma)}{&space;\int_{y_A}&space;p&space;(y_A&space;,&space;y_B&space;;&space;\mu&space;,&space;\Sigma)&space;d&space;y_A&space;}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p&space;(y_A&space;\vert&space;y_B)&space;=&space;\frac{p&space;(y_A&space;,&space;y_B&space;;&space;\mu&space;,&space;\Sigma)}{&space;\int_{y_A}&space;p&space;(y_A&space;,&space;y_B&space;;&space;\mu&space;,&space;\Sigma)&space;d&space;y_A&space;}" title="p (y_A \vert y_B) = \frac{p (y_A , y_B ; \mu , \Sigma)}{ \int_{y_A} p (y_A , y_B ; \mu , \Sigma) d y_A }" /></a>

is also Gaussian:

<a href="https://www.codecogs.com/eqnedit.php?latex=y_A&space;\vert&space;y_B&space;=&space;y_B&space;\sim&space;\mathcal{N}&space;(\mu_A&space;&plus;&space;\Sigma_{AB}&space;\Sigma_{BB}^{-1}&space;(y_B&space;-&space;\mu_B),&space;\Sigma_{AA}&space;-&space;\Sigma_{AB}&space;\Sigma_{BB}^{-1}&space;\Sigma_{BA})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_A&space;\vert&space;y_B&space;=&space;y_B&space;\sim&space;\mathcal{N}&space;(\mu_A&space;&plus;&space;\Sigma_{AB}&space;\Sigma_{BB}^{-1}&space;(y_B&space;-&space;\mu_B),&space;\Sigma_{AA}&space;-&space;\Sigma_{AB}&space;\Sigma_{BB}^{-1}&space;\Sigma_{BA})" title="y_A \vert y_B = y_B \sim \mathcal{N} (\mu_A + \Sigma_{AB} \Sigma_{BB}^{-1} (y_B - \mu_B), \Sigma_{AA} - \Sigma_{AB} \Sigma_{BB}^{-1} \Sigma_{BA})" /></a>

This property will be useful in deriving Gaussian Process predictions.

## Gaussian Process Regression

### Posterior Predictive Distribution

Consider a regression problem(s):

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;y&space;&&space;=&space;f(\mathbf{x})&space;&plus;&space;\epsilon&space;\\&space;y&space;&&space;=&space;\mathbf{w}^{\top}&space;\mathbf{x}&space;&plus;&space;\epsilon&space;&&&space;\text{(OLS&space;and&space;ridge&space;regression)}&space;\\&space;y&space;&&space;=&space;\mathbf{w}^{\top}&space;\phi&space;(\mathbf{x})&space;&plus;&space;\epsilon&space;&&&space;\text{(kernel&space;ridge&space;regression)}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;y&space;&&space;=&space;f(\mathbf{x})&space;&plus;&space;\epsilon&space;\\&space;y&space;&&space;=&space;\mathbf{w}^{\top}&space;\mathbf{x}&space;&plus;&space;\epsilon&space;&&&space;\text{(OLS&space;and&space;ridge&space;regression)}&space;\\&space;y&space;&&space;=&space;\mathbf{w}^{\top}&space;\phi&space;(\mathbf{x})&space;&plus;&space;\epsilon&space;&&&space;\text{(kernel&space;ridge&space;regression)}&space;\end{align*}" title="\begin{align*} y & = f(\mathbf{x}) + \epsilon \\ y & = \mathbf{w}^{\top} \mathbf{x} + \epsilon && \text{(OLS and ridge regression)} \\ y & = \mathbf{w}^{\top} \phi (\mathbf{x}) + \epsilon && \text{(kernel ridge regression)} \end{align*}" /></a>

Recall our goal to estimate probabilities from data. So far, we have developed OLS and (kernel) ridge regression as a solution for regression problems. Those solutions give us a predictive model for **one** particular parameter <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>.

In general, the posterior predictive distribution is

<a href="https://www.codecogs.com/eqnedit.php?latex=P(Y&space;\vert&space;D,&space;X)&space;=&space;\int_{\mathbf{w}}&space;P(Y,&space;\mathbf{w}&space;\vert&space;D,&space;X)&space;d&space;\mathbf{w}&space;=&space;\int_{\mathbf{w}}&space;P(Y&space;\vert&space;\mathbf{w},&space;D,&space;X)&space;P(\mathbf{w}&space;\vert&space;D)&space;d&space;\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(Y&space;\vert&space;D,&space;X)&space;=&space;\int_{\mathbf{w}}&space;P(Y,&space;\mathbf{w}&space;\vert&space;D,&space;X)&space;d&space;\mathbf{w}&space;=&space;\int_{\mathbf{w}}&space;P(Y&space;\vert&space;\mathbf{w},&space;D,&space;X)&space;P(\mathbf{w}&space;\vert&space;D)&space;d&space;\mathbf{w}" title="P(Y \vert D, X) = \int_{\mathbf{w}} P(Y, \mathbf{w} \vert D, X) d \mathbf{w} = \int_{\mathbf{w}} P(Y \vert \mathbf{w}, D, X) P(\mathbf{w} \vert D) d \mathbf{w}" /></a>

Unfortunately, the above is *often intractable* in closed form. However, for the special case of having a Gaussian likelihood and prior (those are the ridge regression assumptions), this expression is Gaussian and we can derive its mean and covariance. So,

<a href="https://www.codecogs.com/eqnedit.php?latex=P(y_{\ast}&space;\vert&space;D,&space;\mathbf{x})&space;\sim&space;\mathcal{N}&space;(\mu_{y_{\ast}&space;\vert&space;D}&space;,&space;\Sigma_{y_{\ast}&space;\vert&space;D})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y_{\ast}&space;\vert&space;D,&space;\mathbf{x})&space;\sim&space;\mathcal{N}&space;(\mu_{y_{\ast}&space;\vert&space;D}&space;,&space;\Sigma_{y_{\ast}&space;\vert&space;D})" title="P(y_{\ast} \vert D, \mathbf{x}) \sim \mathcal{N} (\mu_{y_{\ast} \vert D} , \Sigma_{y_{\ast} \vert D})" /></a>

where

<a href="https://www.codecogs.com/eqnedit.php?latex=\mu_{y_{\ast}&space;\vert&space;D}&space;=&space;K_{\ast}^{\top}&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu_{y_{\ast}&space;\vert&space;D}&space;=&space;K_{\ast}^{\top}&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;y" title="\mu_{y_{\ast} \vert D} = K_{\ast}^{\top} (K + \sigma^2 I)^{-1} y" /></a>

and

<a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_{y_{\ast}&space;\vert&space;D}&space;=&space;K_{\ast&space;\ast}&space;-&space;K_{\ast}^{\top}&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;K_{\ast}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_{y_{\ast}&space;\vert&space;D}&space;=&space;K_{\ast&space;\ast}&space;-&space;K_{\ast}^{\top}&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;K_{\ast}" title="\Sigma_{y_{\ast} \vert D} = K_{\ast \ast} - K_{\ast}^{\top} (K + \sigma^2 I)^{-1} K_{\ast}" /></a>

So, instead of doing MAP (as in ridge regression) let's model the entire distribution and let's forget about <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> and the kernel trick by modelling <a href="https://www.codecogs.com/eqnedit.php?latex=f" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f" title="f" /></a> directly (instead of <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a>)!

## Gaussian Process -- Definition

**Problem**: <a href="https://www.codecogs.com/eqnedit.php?latex=f" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f" title="f" /></a> is an *infinite dimensional function*! But, the multivariate Gaussian distributions is for *finite dimensional random vectors*.

**Definition**: A GP is a (potentially infinite) collection of random variables (RV) such that the joint distribution of every finite subset of RVs is multivariate Gaussian:

<a href="https://www.codecogs.com/eqnedit.php?latex=f&space;\sim&space;\text{GP}(\mu,&space;k)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f&space;\sim&space;\text{GP}(\mu,&space;k)" title="f \sim \text{GP}(\mu, k)" /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=\mu&space;(\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu&space;(\mathbf{x})" title="\mu (\mathbf{x})" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=k(\mathbf{x},&space;\mathbf{x}')" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k(\mathbf{x},&space;\mathbf{x}')" title="k(\mathbf{x}, \mathbf{x}')" /></a> are the mean resp. covariance function! Now, in order to model the predictive distribution <a href="https://www.codecogs.com/eqnedit.php?latex=P(f_{\ast}&space;\vert&space;\mathbf{x}_{\ast}&space;,&space;D)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(f_{\ast}&space;\vert&space;\mathbf{x}_{\ast}&space;,&space;D)" title="P(f_{\ast} \vert \mathbf{x}_{\ast} , D)" /></a> we can use a Bayesian approach by using a **GP prior**: <a href="https://www.codecogs.com/eqnedit.php?latex=P(f&space;\vert&space;\mathbf{x})&space;\sim&space;\mathcal{N}(\mu&space;,&space;\Sigma)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(f&space;\vert&space;\mathbf{x})&space;\sim&space;\mathcal{N}(\mu&space;,&space;\Sigma)" title="P(f \vert \mathbf{x}) \sim \mathcal{N}(\mu , \Sigma)" /></a> and condition it on the training data <a href="https://www.codecogs.com/eqnedit.php?latex=D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D" title="D" /></a> to model the joint distribution of <a href="https://www.codecogs.com/eqnedit.php?latex=f&space;=&space;f&space;(X)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f&space;=&space;f&space;(X)" title="f = f (X)" /></a> (vector of training observations) and <a href="https://www.codecogs.com/eqnedit.php?latex=f_{\ast}&space;=&space;f&space;(\mathbf{x}_{\ast})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_{\ast}&space;=&space;f&space;(\mathbf{x}_{\ast})" title="f_{\ast} = f (\mathbf{x}_{\ast})" /></a> (prediction at test input).

## Gaussian Process Regression (GPR)

We assume that, before we observe the training labels, the labels are drawn from the zero-mean prior Gaussian distribution:

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{bmatrix}&space;y_1&space;\\&space;y_2&space;\\&space;\vdots&space;\\&space;y_n&space;\\&space;y_t&space;\end{bmatrix}&space;\sim&space;\mathcal{N}&space;(0,&space;\Sigma)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{bmatrix}&space;y_1&space;\\&space;y_2&space;\\&space;\vdots&space;\\&space;y_n&space;\\&space;y_t&space;\end{bmatrix}&space;\sim&space;\mathcal{N}&space;(0,&space;\Sigma)" title="\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \\ y_t \end{bmatrix} \sim \mathcal{N} (0, \Sigma)" /></a>

W.I.o.g. zero-mean is always possible by subtracting the sample mean. All training and test labels are drawn from an <a href="https://www.codecogs.com/eqnedit.php?latex=(n&plus;m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(n&plus;m)" title="(n+m)" /></a>-dimension Gaussian distribution, where <a href="https://www.codecogs.com/eqnedit.php?latex=n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n" title="n" /></a> is the number of training points, <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> is the number of testing points.
Note that, the real training labels, <a href="https://www.codecogs.com/eqnedit.php?latex=y_1,\ldots,y_n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_1,\ldots,y_n" title="y_1,\ldots,y_n" /></a>, we observe are samples of <a href="https://www.codecogs.com/eqnedit.php?latex=Y_1,\ldots,Y_n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y_1,\ldots,Y_n" title="Y_1,\ldots,Y_n" /></a>.

Whether this distribution gives us meaningful distribution or not depends on how we choose the covariance matrix <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma" title="\Sigma" /></a>. We consider the following properties of <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma" title="\Sigma" /></a>:

1. <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_{ij}&space;=&space;E&space;\big(&space;(Y_i&space;-&space;\mu_i)&space;(Y_j&space;-&space;\mu_j)&space;\big)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_{ij}&space;=&space;E&space;\big(&space;(Y_i&space;-&space;\mu_i)&space;(Y_j&space;-&space;\mu_j)&space;\big)" title="\Sigma_{ij} = E \big( (Y_i - \mu_i) (Y_j - \mu_j) \big)" /></a>.
2. <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma" title="\Sigma" /></a> is always positive semi-definite.
3. <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_{ii}&space;=&space;\text{Var}(Y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_{ii}&space;=&space;\text{Var}(Y_i)" title="\Sigma_{ii} = \text{Var}(Y_i)" /></a>, thus <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_{ii}&space;\geq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_{ii}&space;\geq&space;0" title="\Sigma_{ii} \geq 0" /></a>.
4. If <a href="https://www.codecogs.com/eqnedit.php?latex=Y_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y_i" title="Y_i" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=Y_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y_j" title="Y_j" /></a> are very independent, i.e. <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> is very different from <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_j" title="\mathbf{x}_j" /></a>, then <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_{ij}&space;=&space;\Sigma_{ji}&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_{ij}&space;=&space;\Sigma_{ji}&space;=&space;0" title="\Sigma_{ij} = \Sigma_{ji} = 0" /></a>.
5. If <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> is similar to <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_j" title="\mathbf{x}_j" /></a>, then <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_{ij}&space;=&space;\Sigma_{ji}&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_{ij}&space;=&space;\Sigma_{ji}&space;>&space;0" title="\Sigma_{ij} = \Sigma_{ji} > 0" /></a>.

We can observe that this is very similar from the kernel matrix in SVMs. Therefore, we can simply let <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_{ij}&space;=&space;K&space;(\mathbf{x}_i&space;,&space;\mathbf{x}_j)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_{ij}&space;=&space;K&space;(\mathbf{x}_i&space;,&space;\mathbf{x}_j)" title="\Sigma_{ij} = K (\mathbf{x}_i , \mathbf{x}_j)" /></a>. For example, if we use RBF kernel (a.k.a. "squared exponential kernel"), then

<a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_{ij}&space;=&space;\tau&space;e^{\frac{-\|&space;\mathbf{x}_i&space;-&space;\mathbf{x}_j&space;\|^2}{\sigma^2}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_{ij}&space;=&space;\tau&space;e^{\frac{-\|&space;\mathbf{x}_i&space;-&space;\mathbf{x}_j&space;\|^2}{\sigma^2}}" title="\Sigma_{ij} = \tau e^{\frac{-\| \mathbf{x}_i - \mathbf{x}_j \|^2}{\sigma^2}}" /></a>

If we use polynomial kernel, then <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_{ij}&space;=&space;\tau&space;(1&space;&plus;&space;\mathbf{x}_i^{\top}&space;\mathbf{x}_j)^d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_{ij}&space;=&space;\tau&space;(1&space;&plus;&space;\mathbf{x}_i^{\top}&space;\mathbf{x}_j)^d" title="\Sigma_{ij} = \tau (1 + \mathbf{x}_i^{\top} \mathbf{x}_j)^d" /></a>.

Thus, we can decompose <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma" title="\Sigma" /></a> as <a href="https://www.codecogs.com/eqnedit.php?latex=\begin{pmatrix}&space;K&space;&&space;K_{\ast}&space;\\&space;K_{\ast}^{\top}&space;&&space;K_{\ast\ast}&space;\end{pmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{pmatrix}&space;K&space;&&space;K_{\ast}&space;\\&space;K_{\ast}^{\top}&space;&&space;K_{\ast\ast}&space;\end{pmatrix}" title="\begin{pmatrix} K & K_{\ast} \\ K_{\ast}^{\top} & K_{\ast\ast} \end{pmatrix}" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=K" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K" title="K" /></a> is the training kernel matrix, <a href="https://www.codecogs.com/eqnedit.php?latex=K_{\ast}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K_{\ast}" title="K_{\ast}" /></a> is the training-testing kernel matrix, <a href="https://www.codecogs.com/eqnedit.php?latex=K_{\ast}^{\top}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K_{\ast}^{\top}" title="K_{\ast}^{\top}" /></a> is the testing-training kernel matrix and <a href="https://www.codecogs.com/eqnedit.php?latex=K_{\ast\ast}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K_{\ast\ast}" title="K_{\ast\ast}" /></a> is the testing kernel matrix. The conditional distribution of (noise-free) values of the latent function <a href="https://www.codecogs.com/eqnedit.php?latex=f" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f" title="f" /></a> can be written as:

<a href="https://www.codecogs.com/eqnedit.php?latex=f_{\ast}&space;\vert&space;(Y_1&space;=&space;y_1,&space;\ldots&space;,&space;Y_n&space;=&space;y_n&space;,&space;\mathbf{x}_1&space;,&space;\ldots,&space;\mathbf{x}_n&space;,&space;\mathbf{x}_t)&space;\sim&space;\mathcal{N}&space;(K_{\ast}^{\top}&space;K^{-1}&space;y&space;,&space;K_{\ast\ast}&space;-&space;K_{\ast}^{\top}&space;K^{-1}&space;K_{\ast})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_{\ast}&space;\vert&space;(Y_1&space;=&space;y_1,&space;\ldots&space;,&space;Y_n&space;=&space;y_n&space;,&space;\mathbf{x}_1&space;,&space;\ldots,&space;\mathbf{x}_n&space;,&space;\mathbf{x}_t)&space;\sim&space;\mathcal{N}&space;(K_{\ast}^{\top}&space;K^{-1}&space;y&space;,&space;K_{\ast\ast}&space;-&space;K_{\ast}^{\top}&space;K^{-1}&space;K_{\ast})" title="f_{\ast} \vert (Y_1 = y_1, \ldots , Y_n = y_n , \mathbf{x}_1 , \ldots, \mathbf{x}_n , \mathbf{x}_t) \sim \mathcal{N} (K_{\ast}^{\top} K^{-1} y , K_{\ast\ast} - K_{\ast}^{\top} K^{-1} K_{\ast})" /></a>

where the kernel matrices <a href="https://www.codecogs.com/eqnedit.php?latex=K_{\ast}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K_{\ast}" title="K_{\ast}" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=K_{\ast\ast}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K_{\ast\ast}" title="K_{\ast\ast}" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=K" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K" title="K" /></a> are functions of <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_1,\ldots,\mathbf{x}_n,\mathbf{x}_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_1,\ldots,\mathbf{x}_n,\mathbf{x}_t" title="\mathbf{x}_1,\ldots,\mathbf{x}_n,\mathbf{x}_t" /></a>.

### Additive Gaussian Noise

In many applications the observed labels can be noisy. If we assume this noise is independent and zero-mean Gaussian, then we observe <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{Y}_i&space;=&space;f_i&space;&plus;&space;\epsilon_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{Y}_i&space;=&space;f_i&space;&plus;&space;\epsilon_i" title="\hat{Y}_i = f_i + \epsilon_i" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=f_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_i" title="f_i" /></a> is the true (unobserved=latent) target and the noise is denoted by <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon_i&space;\sim&space;\mathcal{N}(0,&space;\sigma^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon_i&space;\sim&space;\mathcal{N}(0,&space;\sigma^2)" title="\epsilon_i \sim \mathcal{N}(0, \sigma^2)" /></a>. In this case the new covariance matrix becomes <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{\Sigma}&space;=&space;\Sigma&space;&plus;&space;\sigma^2&space;I" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{\Sigma}&space;=&space;\Sigma&space;&plus;&space;\sigma^2&space;I" title="\hat{\Sigma} = \Sigma + \sigma^2 I" /></a>. We can derive this fact first for the off-diagonal terms where <a href="https://www.codecogs.com/eqnedit.php?latex=i&space;\neq&space;j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i&space;\neq&space;j" title="i \neq j" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{\Sigma}_{ij}&space;=&space;\mathbb{E}&space;[(f_i&space;&plus;&space;\epsilon_i)&space;(f_j&space;&plus;&space;\epsilon_j)]&space;=&space;\mathbb{E}[f_i&space;f_j]&space;&plus;&space;\mathbb{E}[f_i]\mathbb{E}[\epsilon_j]&space;&plus;&space;\mathbb{E}[f_j]\mathbb{E}[\epsilon_i]&space;&plus;&space;\mathbb{E}[\epsilon_i]\mathbb{E}[\epsilon_j]&space;=&space;\mathbb{E}[f_i&space;f_j]&space;=&space;\Sigma_{ij}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{\Sigma}_{ij}&space;=&space;\mathbb{E}&space;[(f_i&space;&plus;&space;\epsilon_i)&space;(f_j&space;&plus;&space;\epsilon_j)]&space;=&space;\mathbb{E}[f_i&space;f_j]&space;&plus;&space;\mathbb{E}[f_i]\mathbb{E}[\epsilon_j]&space;&plus;&space;\mathbb{E}[f_j]\mathbb{E}[\epsilon_i]&space;&plus;&space;\mathbb{E}[\epsilon_i]\mathbb{E}[\epsilon_j]&space;=&space;\mathbb{E}[f_i&space;f_j]&space;=&space;\Sigma_{ij}" title="\hat{\Sigma}_{ij} = \mathbb{E} [(f_i + \epsilon_i) (f_j + \epsilon_j)] = \mathbb{E}[f_i f_j] + \mathbb{E}[f_i]\mathbb{E}[\epsilon_j] + \mathbb{E}[f_j]\mathbb{E}[\epsilon_i] + \mathbb{E}[\epsilon_i]\mathbb{E}[\epsilon_j] = \mathbb{E}[f_i f_j] = \Sigma_{ij}" /></a>

as <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbb{E}[\epsilon_i]&space;=&space;\mathbb{E}[\epsilon_j]&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbb{E}[\epsilon_i]&space;=&space;\mathbb{E}[\epsilon_j]&space;=&space;0" title="\mathbb{E}[\epsilon_i] = \mathbb{E}[\epsilon_j] = 0" /></a> and where we use the fact that <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon_i" title="\epsilon_i" /></a> is independent from all other random variables. For the diagonal entries of <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma" title="\Sigma" /></a>, i.e. the case where <a href="https://www.codecogs.com/eqnedit.php?latex=i=j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i=j" title="i=j" /></a>, we obtain

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{\Sigma}_{ii}&space;=&space;\mathbb{E}&space;[(f_i&space;&plus;&space;\epsilon_i)^2]&space;=&space;\mathbb{E}[f_i^2]&space;&plus;&space;2&space;\mathbb{E}[f_i]\mathbb{E}[\epsilon_i]&space;&plus;&space;\mathbb{E}[\epsilon_i^2]&space;=&space;\mathbb{E}[f_i&space;f_j]&space;&plus;&space;\mathbb{E}[\epsilon_i^2]&space;=&space;\Sigma_{ij}&space;&plus;&space;\sigma^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{\Sigma}_{ii}&space;=&space;\mathbb{E}&space;[(f_i&space;&plus;&space;\epsilon_i)^2]&space;=&space;\mathbb{E}[f_i^2]&space;&plus;&space;2&space;\mathbb{E}[f_i]\mathbb{E}[\epsilon_i]&space;&plus;&space;\mathbb{E}[\epsilon_i^2]&space;=&space;\mathbb{E}[f_i&space;f_j]&space;&plus;&space;\mathbb{E}[\epsilon_i^2]&space;=&space;\Sigma_{ij}&space;&plus;&space;\sigma^2" title="\hat{\Sigma}_{ii} = \mathbb{E} [(f_i + \epsilon_i)^2] = \mathbb{E}[f_i^2] + 2 \mathbb{E}[f_i]\mathbb{E}[\epsilon_i] + \mathbb{E}[\epsilon_i^2] = \mathbb{E}[f_i f_j] + \mathbb{E}[\epsilon_i^2] = \Sigma_{ij} + \sigma^2" /></a>

because <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbb{E}[\epsilon_i^2]&space;=&space;\sigma^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbb{E}[\epsilon_i^2]&space;=&space;\sigma^2" title="\mathbb{E}[\epsilon_i^2] = \sigma^2" /></a>, which denotes the variance of <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon_i" title="\epsilon_i" /></a>.

Plugging this updated covariance matrix into the Gaussian Process posterior distribution leads to

<a href="https://www.codecogs.com/eqnedit.php?latex=Y_{\ast}&space;\vert&space;(Y_1&space;=&space;y_1&space;,&space;\ldots&space;,&space;Y_n&space;=&space;y_n&space;,&space;\mathbf{x}_1&space;,&space;\ldots&space;,&space;\mathbf{x}_n)&space;\sim&space;\mathcal{N}&space;\big(&space;K_{\ast}^{\top}&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;y,&space;K_{\ast\ast}&space;&plus;&space;\sigma^2&space;I&space;-&space;K_{\ast}^{\top}&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;K_{\ast}&space;\big)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y_{\ast}&space;\vert&space;(Y_1&space;=&space;y_1&space;,&space;\ldots&space;,&space;Y_n&space;=&space;y_n&space;,&space;\mathbf{x}_1&space;,&space;\ldots&space;,&space;\mathbf{x}_n)&space;\sim&space;\mathcal{N}&space;\big(&space;K_{\ast}^{\top}&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;y,&space;K_{\ast\ast}&space;&plus;&space;\sigma^2&space;I&space;-&space;K_{\ast}^{\top}&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;K_{\ast}&space;\big)" title="Y_{\ast} \vert (Y_1 = y_1 , \ldots , Y_n = y_n , \mathbf{x}_1 , \ldots , \mathbf{x}_n) \sim \mathcal{N} \big( K_{\ast}^{\top} (K + \sigma^2 I)^{-1} y, K_{\ast\ast} + \sigma^2 I - K_{\ast}^{\top} (K + \sigma^2 I)^{-1} K_{\ast} \big)" /></a>

In practice the above equation is often more stable because the matrix <a href="https://www.codecogs.com/eqnedit.php?latex=(K&space;&plus;&space;\sigma^2&space;I)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(K&space;&plus;&space;\sigma^2&space;I)" title="(K + \sigma^2 I)" /></a> is always invertible if <a href="https://www.codecogs.com/eqnedit.php?latex=\sigma^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma^2" title="\sigma^2" /></a> is sufficiently large. So, for **predictions** we can use the posterior mean and additionally we get the predictive variance as measure of confidence or (un)certainty about the point prediction.

## Marginal Likelihood and Hyper-Parameter Learning

Even though GPR is a non-parametric model, the covariance function <a href="https://www.codecogs.com/eqnedit.php?latex=K" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K" title="K" /></a> has parameters. Those parameters are so-called hyper-parameters and they need to be learned (=estimated) from the training data. Let <a href="https://www.codecogs.com/eqnedit.php?latex=\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta" title="\theta" /></a> be the vector of all parameters of <a href="https://www.codecogs.com/eqnedit.php?latex=K" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K" title="K" /></a>, then <a href="https://www.codecogs.com/eqnedit.php?latex=K&space;=&space;K_{\theta}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K&space;=&space;K_{\theta}" title="K = K_{\theta}" /></a> and we can learn <a href="https://www.codecogs.com/eqnedit.php?latex=\theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta" title="\theta" /></a> by maximizing the marginal likelihood <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;X&space;,&space;\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;X&space;,&space;\theta)" title="P(y \vert X , \theta)" /></a>.

Once you have the marginal likelihood and its derivatives you can use any out-of-the-box solver such as (stochastic) gradient descent, or conjugate gradient descent (Caution: minimize **negative** log marginal likelihood). Note that the marginal likelihood is not a convex function in its parameters and the solution is most likely a local minima / maxima. To make this procedure more robust, you can rerun your optimization algorithm with different initializations and pick the lowest/highest return value.

## Covariance Functions -- The Heart of the GP Model

GPs gain a lot of their predictive power by selecting the *right* covariance/kernel function. Selecting the covariance function is the model selection process in the GP learning phase. There are three different ways to come up with a good covariance function:

- Expert knowledge (awesome to have -- difficult to get)
- Bayesian model selection (more possibly analytically intractable integrals!!)
- Cross-validation (time-consuming -- but simple to implement)

One popular and powerful covariance function is the RBF with different length scales for each feature dimension.

<a href="https://www.codecogs.com/eqnedit.php?latex=k(\mathbf{x},\mathbf{x}')&space;=&space;\sigma_{f}^2&space;e^{-\frac{1}{2}&space;(\mathbf{x}&space;-&space;\mathbf{x}')^{\top}&space;M&space;(\mathbf{x}&space;-&space;\mathbf{x}')}&space;&plus;&space;\sigma_n^2&space;\delta_{\mathbf{x},\mathbf{x}'}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k(\mathbf{x},\mathbf{x}')&space;=&space;\sigma_{f}^2&space;e^{-\frac{1}{2}&space;(\mathbf{x}&space;-&space;\mathbf{x}')^{\top}&space;M&space;(\mathbf{x}&space;-&space;\mathbf{x}')}&space;&plus;&space;\sigma_n^2&space;\delta_{\mathbf{x},\mathbf{x}'}" title="k(\mathbf{x},\mathbf{x}') = \sigma_{f}^2 e^{-\frac{1}{2} (\mathbf{x} - \mathbf{x}')^{\top} M (\mathbf{x} - \mathbf{x}')} + \sigma_n^2 \delta_{\mathbf{x},\mathbf{x}'}" /></a>

with <a href="https://www.codecogs.com/eqnedit.php?latex=M&space;=&space;\text{diag}(\ell)^{-2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?M&space;=&space;\text{diag}(\ell)^{-2}" title="M = \text{diag}(\ell)^{-2}" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=\sigma_f^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma_f^2" title="\sigma_f^2" /></a> is the signal variance, <a href="https://www.codecogs.com/eqnedit.php?latex=\ell" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell" title="\ell" /></a> is a vector of length-scale parameters and <a href="https://www.codecogs.com/eqnedit.php?latex=\sigma_n^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma_n^2" title="\sigma_n^2" /></a> is the noise variance.

## Summary

Gaussian Process Regression has the following properties:

- GPs are an elegant and powerful ML method
- We get a measure of (un)certainty for the predictions for free.
- GPs work very well for regression problems with small training data set sizes.
- Running time <a href="https://www.codecogs.com/eqnedit.php?latex=O(n^3)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(n^3)" title="O(n^3)" /></a> <a href="https://www.codecogs.com/eqnedit.php?latex=\leftarrow" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\leftarrow" title="\leftarrow" /></a> matrix inversion (get slow when <a href="https://www.codecogs.com/eqnedit.php?latex=n&space;\gg&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n&space;\gg&space;0" title="n \gg 0" /></a>)<a href="https://www.codecogs.com/eqnedit.php?latex=\Rightarrow" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Rightarrow" title="\Rightarrow" /></a> use sparse GPs for large <a href="https://www.codecogs.com/eqnedit.php?latex=n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n" title="n" /></a>.
- GPs are a little bit more involved for classification (non-Gaussian likelihood).
- We can model non-Gaussian likelihoods in regression and do approximate inference for e.g., count data (Poisson distribution)
- GP implementations: GPyTorch, GPML (MATLAB), GPys, pyGPs, and scikit-learn (Python)

## Application: Bayesian Global Optimization

A nice applications of GP regression is Bayesian Global Optimization. Here, the goal is to optimize the hyper-parameters of a machine learning algorithm to do well on a fixed validation data set. Imagine you have <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> hyper-parameters to tune, then your data set consists of <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a>-dimensional vectors <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i&space;\in&space;\mathcal{R}^d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i&space;\in&space;\mathcal{R}^d" title="\mathbf{x}_i \in \mathcal{R}^d" /></a>, where each training point represents a particular hyper-parameter setting and the labels <a href="https://www.codecogs.com/eqnedit.php?latex=y_i&space;\in&space;\mathcal{R}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i&space;\in&space;\mathcal{R}" title="y_i \in \mathcal{R}" /></a> represents the validation error. *Don't get confused, this time vectors <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> correspond to hyper-parameter settings and not data*. For example, in the case of an SVM with polynomial kernel you have two hyper-parameters: the regularization constant <a href="https://www.codecogs.com/eqnedit.php?latex=C" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C" title="C" /></a> (also often <a href="https://www.codecogs.com/eqnedit.php?latex=\lambda" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda" title="\lambda" /></a>) and the polynomial power <a href="https://www.codecogs.com/eqnedit.php?latex=p" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p" title="p" /></a>. The first dimension of <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> may correspond to a value for <a href="https://www.codecogs.com/eqnedit.php?latex=C" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C" title="C" /></a> and the second dimension may correspond to a value of <a href="https://www.codecogs.com/eqnedit.php?latex=p" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p" title="p" /></a>. Initially you train your classifier under a few random hyper-parameter settings and evaluate the classifier on the validation set. This gives you <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_1&space;,&space;\ldots&space;,&space;\mathbf{x}_m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_1&space;,&space;\ldots&space;,&space;\mathbf{x}_m" title="\mathbf{x}_1 , \ldots , \mathbf{x}_m" /></a> with labels <a href="https://www.codecogs.com/eqnedit.php?latex=y_1&space;,&space;\ldots&space;,&space;y_m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_1&space;,&space;\ldots&space;,&space;y_m" title="y_1 , \ldots , y_m" /></a>. You can now train a Gaussian Process to predict the validation error <a href="https://www.codecogs.com/eqnedit.php?latex=y_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_t" title="y_t" /></a> at any new hyper-parameter setting <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_t" title="\mathbf{x}_t" /></a>. In fact, you obtain a mean prediction <a href="https://www.codecogs.com/eqnedit.php?latex=y_t&space;=&space;h(\mathbf{x}_t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_t&space;=&space;h(\mathbf{x}_t)" title="y_t = h(\mathbf{x}_t)" /></a> and its variance <a href="https://www.codecogs.com/eqnedit.php?latex=v(\mathbf{x}_t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v(\mathbf{x}_t)" title="v(\mathbf{x}_t)" /></a>. If you have more time you can now explore a new point. The most promising point is the one with the lowest lower confidence bound, i.e.

<a href="https://www.codecogs.com/eqnedit.php?latex=\operatorname*{argmin}_{\mathbf{x}_t}&space;h(\mathbf{x}_t)&space;-&space;\kappa&space;\sqrt{v(\mathbf{x}_t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\operatorname*{argmin}_{\mathbf{x}_t}&space;h(\mathbf{x}_t)&space;-&space;\kappa&space;\sqrt{v(\mathbf{x}_t)}" title="\operatorname*{argmin}_{\mathbf{x}_t} h(\mathbf{x}_t) - \kappa \sqrt{v(\mathbf{x}_t)}" /></a>

The constant <a href="https://www.codecogs.com/eqnedit.php?latex=\kappa&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\kappa&space;>&space;0" title="\kappa > 0" /></a> trades off how much you want to *explore* points that may be good just because you are very uncertain (variance is high) or how much you want to *exploit* your knowledge about the current best point and refine the best settings found so far. A small <a href="https://www.codecogs.com/eqnedit.php?latex=\kappa" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\kappa" title="\kappa" /></a> leads to more exploitation, whereas a large <a href="https://www.codecogs.com/eqnedit.php?latex=\kappa" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\kappa" title="\kappa" /></a> explores new hyper-parameter settings more aggressively.

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;\textbf{Algorithm:}&space;\mathcal{A},&space;m,&space;n,&space;\kappa&space;\\&space;&&space;\text{For&space;}&space;i&space;=&space;1&space;\text{&space;to&space;}&space;m&space;\\&space;&&space;\qquad&space;\text{sample&space;}&space;\mathbf{x}_i&space;\text{&space;randomly}&space;&&&space;\text{e.g.&space;sample&space;uniformly&space;with&space;reasonable&space;range}&space;\\&space;&&space;\qquad&space;y_i&space;=&space;\mathcal{A}(\mathbf{x}_i)&space;&&&space;\text{compute&space;validation&space;error}&space;\\&space;&&space;\text{EndFor}&space;\\&space;&&space;\text{For&space;}&space;i=m&plus;1&space;\text{&space;to&space;}&space;n&space;\\&space;&&space;\qquad&space;\text{Update&space;kernel&space;}&space;K&space;\text{&space;based&space;on&space;}&space;\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_{i-1}&space;\\&space;&&space;\qquad&space;\mathbf{x}_i&space;=&space;\operatorname*{argmin}_{\mathbf{x}_t}&space;K_t^\top&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;y&space;-&space;\kappa&space;\sqrt{K_{tt}&space;&plus;&space;\sigma^2&space;I&space;-&space;K_t^\top&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;K_t}&space;\\&space;&&space;\qquad&space;y_i&space;=&space;\mathcal{A}(\mathbf{x}_i)&space;&&&space;\text{compute&space;validation&space;error}&space;\\&space;&&space;\text{EndFor}&space;\\&space;&&space;i_{\text{best}}&space;=&space;\operatorname*{argmin}_i&space;\{&space;y_1,\ldots,y_n&space;\}&space;&&&space;\text{find&space;best&space;hyper-parameter&space;setting&space;explored}&space;\\&space;&&space;\text{Return&space;}&space;\mathbf{x}_{i_{\text{best}}}&space;&&&space;\text{return&space;best&space;hyper-parameter&space;setting&space;explored}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;&&space;\textbf{Algorithm:}&space;\mathcal{A},&space;m,&space;n,&space;\kappa&space;\\&space;&&space;\text{For&space;}&space;i&space;=&space;1&space;\text{&space;to&space;}&space;m&space;\\&space;&&space;\qquad&space;\text{sample&space;}&space;\mathbf{x}_i&space;\text{&space;randomly}&space;&&&space;\text{e.g.&space;sample&space;uniformly&space;with&space;reasonable&space;range}&space;\\&space;&&space;\qquad&space;y_i&space;=&space;\mathcal{A}(\mathbf{x}_i)&space;&&&space;\text{compute&space;validation&space;error}&space;\\&space;&&space;\text{EndFor}&space;\\&space;&&space;\text{For&space;}&space;i=m&plus;1&space;\text{&space;to&space;}&space;n&space;\\&space;&&space;\qquad&space;\text{Update&space;kernel&space;}&space;K&space;\text{&space;based&space;on&space;}&space;\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_{i-1}&space;\\&space;&&space;\qquad&space;\mathbf{x}_i&space;=&space;\operatorname*{argmin}_{\mathbf{x}_t}&space;K_t^\top&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;y&space;-&space;\kappa&space;\sqrt{K_{tt}&space;&plus;&space;\sigma^2&space;I&space;-&space;K_t^\top&space;(K&space;&plus;&space;\sigma^2&space;I)^{-1}&space;K_t}&space;\\&space;&&space;\qquad&space;y_i&space;=&space;\mathcal{A}(\mathbf{x}_i)&space;&&&space;\text{compute&space;validation&space;error}&space;\\&space;&&space;\text{EndFor}&space;\\&space;&&space;i_{\text{best}}&space;=&space;\operatorname*{argmin}_i&space;\{&space;y_1,\ldots,y_n&space;\}&space;&&&space;\text{find&space;best&space;hyper-parameter&space;setting&space;explored}&space;\\&space;&&space;\text{Return&space;}&space;\mathbf{x}_{i_{\text{best}}}&space;&&&space;\text{return&space;best&space;hyper-parameter&space;setting&space;explored}&space;\end{align*}" title="\begin{align*} & \textbf{Algorithm:} \mathcal{A}, m, n, \kappa \\ & \text{For } i = 1 \text{ to } m \\ & \qquad \text{sample } \mathbf{x}_i \text{ randomly} && \text{e.g. sample uniformly with reasonable range} \\ & \qquad y_i = \mathcal{A}(\mathbf{x}_i) && \text{compute validation error} \\ & \text{EndFor} \\ & \text{For } i=m+1 \text{ to } n \\ & \qquad \text{Update kernel } K \text{ based on } \mathbf{x}_1, \ldots, \mathbf{x}_{i-1} \\ & \qquad \mathbf{x}_i = \operatorname*{argmin}_{\mathbf{x}_t} K_t^\top (K + \sigma^2 I)^{-1} y - \kappa \sqrt{K_{tt} + \sigma^2 I - K_t^\top (K + \sigma^2 I)^{-1} K_t} \\ & \qquad y_i = \mathcal{A}(\mathbf{x}_i) && \text{compute validation error} \\ & \text{EndFor} \\ & i_{\text{best}} = \operatorname*{argmin}_i \{ y_1,\ldots,y_n \} && \text{find best hyper-parameter setting explored} \\ & \text{Return } \mathbf{x}_{i_{\text{best}}} && \text{return best hyper-parameter setting explored} \end{align*}" /></a>
