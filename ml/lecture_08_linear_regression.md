# Linear Regression

## Assumptions

**Data Assumption**: <a href="https://www.codecogs.com/eqnedit.php?latex=y_i&space;\in&space;\mathbb{R}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i&space;\in&space;\mathbb{R}" title="y_i \in \mathbb{R}" /></a>

**Model Assumption**: <a href="https://www.codecogs.com/eqnedit.php?latex=y_i&space;=&space;\mathbf{w}^{\top}&space;\mathbf{x}_i&space;&plus;&space;\epsilon_i&space;\text{&space;where&space;}&space;\epsilon_i&space;\sim&space;N(0,&space;\sigma^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i&space;=&space;\mathbf{w}^{\top}&space;\mathbf{x}_i&space;&plus;&space;\epsilon_i&space;\text{&space;where&space;}&space;\epsilon_i&space;\sim&space;N(0,&space;\sigma^2)" title="y_i = \mathbf{w}^{\top} \mathbf{x}_i + \epsilon_i \text{ where } \epsilon_i \sim N(0, \sigma^2)" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\Rightarrow&space;y_i&space;\vert&space;\mathbf{x}_i&space;\sim&space;N(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;,&space;\sigma^2)&space;\Rightarrow&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})&space;=&space;\frac{1}{\sqrt{2\pi&space;\sigma^2}}&space;e^{-\frac{(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2}{2\sigma^2}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Rightarrow&space;y_i&space;\vert&space;\mathbf{x}_i&space;\sim&space;N(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;,&space;\sigma^2)&space;\Rightarrow&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})&space;=&space;\frac{1}{\sqrt{2\pi&space;\sigma^2}}&space;e^{-\frac{(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2}{2\sigma^2}}" title="\Rightarrow y_i \vert \mathbf{x}_i \sim N(\mathbf{w}^{\top} \mathbf{x}_i , \sigma^2) \Rightarrow P(y_i \vert \mathbf{x}_i , \mathbf{w}) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(\mathbf{x}_i^{\top} \mathbf{w} - y_i)^2}{2\sigma^2}}" /></a>

In words, we assume that the data is drawn from a "line" <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}^{\top}&space;\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}^{\top}&space;\mathbf{x}" title="\mathbf{w}^{\top} \mathbf{x}" /></a> through the origin (one can always add a bias / offset through an additional dimension, similar to the *Perceptron*. For each data point with features <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a>, the label <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a> is drawn from a Gaussian with mean <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}^{\top}\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}^{\top}\mathbf{x}_i" title="\mathbf{w}^{\top}\mathbf{x}_i" /></a> and variance <a href="https://www.codecogs.com/eqnedit.php?latex=\sigma^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma^2" title="\sigma^2" /></a>. Our task is to estimate the slope <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> from the data.

## Estimating with MLE

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\mathbf{w}&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;P(y_1,&space;\mathbf{x}_1,&space;\ldots,&space;y_n,&space;\mathbf{x}_n&space;\vert&space;\mathbf{w})&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\prod_{i=1}^{n}&space;P(y_i,&space;\mathbf{x}_i&space;\vert&space;\mathbf{w})&space;&&&space;\text{(Because&space;of&space;independence)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})&space;P(\mathbf{x}_i&space;\vert&space;\mathbf{w})&space;&&&space;\text{(Chain&space;rule&space;of&space;probability)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})&space;P(\mathbf{x}_i)&space;&&&space;\text{(}&space;\mathbf{x}_i&space;\text{&space;is&space;independent&space;of&space;}&space;\mathbf{w}&space;\text{)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})&space;&&&space;\text{(}&space;P(\mathbf{x}_i)&space;\text{&space;is&space;a&space;constant&space;---&space;can&space;be&space;dropped)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;[P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})]&space;&&&space;\text{log&space;is&space;a&space;monotonic&space;function}&space;\\&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;\mathbf{w}&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;P(y_1,&space;\mathbf{x}_1,&space;\ldots,&space;y_n,&space;\mathbf{x}_n&space;\vert&space;\mathbf{w})&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\prod_{i=1}^{n}&space;P(y_i,&space;\mathbf{x}_i&space;\vert&space;\mathbf{w})&space;&&&space;\text{(Because&space;of&space;independence)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})&space;P(\mathbf{x}_i&space;\vert&space;\mathbf{w})&space;&&&space;\text{(Chain&space;rule&space;of&space;probability)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})&space;P(\mathbf{x}_i)&space;&&&space;\text{(}&space;\mathbf{x}_i&space;\text{&space;is&space;independent&space;of&space;}&space;\mathbf{w}&space;\text{)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})&space;&&&space;\text{(}&space;P(\mathbf{x}_i)&space;\text{&space;is&space;a&space;constant&space;---&space;can&space;be&space;dropped)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;[P(y_i&space;\vert&space;\mathbf{x}_i&space;,&space;\mathbf{w})]&space;&&&space;\text{log&space;is&space;a&space;monotonic&space;function}&space;\\&space;\end{align*}" title="\begin{align*} \mathbf{w} & = \operatorname*{argmax}_{\mathbf{w}} P(y_1, \mathbf{x}_1, \ldots, y_n, \mathbf{x}_n \vert \mathbf{w}) \\ & = \operatorname*{argmax}_{\mathbf{w}} \prod_{i=1}^{n} P(y_i, \mathbf{x}_i \vert \mathbf{w}) && \text{(Because of independence)} \\ & = \operatorname*{argmax}_{\mathbf{w}} \prod_{i=1}^{n} P(y_i \vert \mathbf{x}_i , \mathbf{w}) P(\mathbf{x}_i \vert \mathbf{w}) && \text{(Chain rule of probability)} \\ & = \operatorname*{argmax}_{\mathbf{w}} \prod_{i=1}^{n} P(y_i \vert \mathbf{x}_i , \mathbf{w}) P(\mathbf{x}_i) && \text{(} \mathbf{x}_i \text{ is independent of } \mathbf{w} \text{)} \\ & = \operatorname*{argmax}_{\mathbf{w}} \prod_{i=1}^{n} P(y_i \vert \mathbf{x}_i , \mathbf{w}) && \text{(} P(\mathbf{x}_i) \text{ is a constant --- can be dropped)} \\ & = \operatorname*{argmax}_{\mathbf{w}} \sum_{i=1}^{n} \log [P(y_i \vert \mathbf{x}_i , \mathbf{w})] && \text{log is a monotonic function} \\ \end{align*}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\Big[&space;\log&space;(\frac{1}{\sqrt{2\pi&space;\sigma^2}})&space;&plus;&space;\log&space;(e^{-\frac{(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2}{2\sigma^2}})&space;\Big]&space;&&&space;\text{Plugging&space;in&space;probability&space;distribution}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;-\frac{1}{2\sigma^2}&space;\sum_{i=1}^{n}&space;(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2&space;&&&space;\text{First&space;term&space;is&space;a&space;constant,&space;and&space;}&space;\log&space;(e^z)&space;=&space;z&space;\\&space;&&space;=&space;\operatorname*{argmin}_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2&space;&&&space;\text{Always&space;minimize;&space;}&space;\frac{1}{n}&space;\text{&space;makes&space;the&space;loss&space;interpretable&space;(average&space;squared&space;error)}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\Big[&space;\log&space;(\frac{1}{\sqrt{2\pi&space;\sigma^2}})&space;&plus;&space;\log&space;(e^{-\frac{(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2}{2\sigma^2}})&space;\Big]&space;&&&space;\text{Plugging&space;in&space;probability&space;distribution}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;-\frac{1}{2\sigma^2}&space;\sum_{i=1}^{n}&space;(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2&space;&&&space;\text{First&space;term&space;is&space;a&space;constant,&space;and&space;}&space;\log&space;(e^z)&space;=&space;z&space;\\&space;&&space;=&space;\operatorname*{argmin}_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2&space;&&&space;\text{Always&space;minimize;&space;}&space;\frac{1}{n}&space;\text{&space;makes&space;the&space;loss&space;interpretable&space;(average&space;squared&space;error)}&space;\end{align*}" title="\begin{align*} & = \operatorname*{argmax}_{\mathbf{w}} \sum_{i=1}^{n} \Big[ \log (\frac{1}{\sqrt{2\pi \sigma^2}}) + \log (e^{-\frac{(\mathbf{x}_i^{\top} \mathbf{w} - y_i)^2}{2\sigma^2}}) \Big] && \text{Plugging in probability distribution} \\ & = \operatorname*{argmax}_{\mathbf{w}} -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (\mathbf{x}_i^{\top} \mathbf{w} - y_i)^2 && \text{First term is a constant, and } \log (e^z) = z \\ & = \operatorname*{argmin}_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^{n} (\mathbf{x}_i^{\top} \mathbf{w} - y_i)^2 && \text{Always minimize; } \frac{1}{n} \text{ makes the loss interpretable (average squared error)} \end{align*}" /></a>

We are minimizing a *loss function*, <a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(\mathbf{w})&space;=&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(\mathbf{w})&space;=&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{x}_i^{\top}&space;\mathbf{w}&space;-&space;y_i)^2" title="\ell (\mathbf{w}) = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{x}_i^{\top} \mathbf{w} - y_i)^2" /></a>. This particular loss function is also known as the squared loss or Ordinary Least Squares (OLS). OLS can be optimized with gradient descent, Newton's method, or in closed form.

**Closed Form**: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}&space;=&space;(\mathbf{X}&space;\mathbf{X}^{\top})^{-1}&space;\mathbf{X}&space;\mathbf{y}^{\top}&space;\text{&space;where&space;}&space;\mathbf{X}&space;=&space;[\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_n]&space;\text{&space;and&space;}&space;\mathbf{y}&space;=&space;[y_1,&space;\ldots,&space;y_n]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}&space;=&space;(\mathbf{X}&space;\mathbf{X}^{\top})^{-1}&space;\mathbf{X}&space;\mathbf{y}^{\top}&space;\text{&space;where&space;}&space;\mathbf{X}&space;=&space;[\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_n]&space;\text{&space;and&space;}&space;\mathbf{y}&space;=&space;[y_1,&space;\ldots,&space;y_n]" title="\mathbf{w} = (\mathbf{X} \mathbf{X}^{\top})^{-1} \mathbf{X} \mathbf{y}^{\top} \text{ where } \mathbf{X} = [\mathbf{x}_1, \ldots, \mathbf{x}_n] \text{ and } \mathbf{y} = [y_1, \ldots, y_n]" /></a>.





















