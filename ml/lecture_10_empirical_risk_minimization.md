# Empirical Risk Minimization

## Recap

Remember the unconstrained SVM formulation

<a href="https://www.codecogs.com/eqnedit.php?latex=\min_{\mathbf{w}}&space;\underbrace{C&space;\sum_{i=1}^{n}&space;\max&space;[1&space;-&space;y_i&space;\underbrace{(w^\top&space;\mathbf{x}_i&space;&plus;&space;b)}_{h(\mathbf{x}_i)}&space;,&space;0]}_{\text{Hinge-Loss}}&space;&plus;&space;\underbrace{\|&space;w&space;\|_z^2}_{\ell_2-Rgularizer}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\min_{\mathbf{w}}&space;\underbrace{C&space;\sum_{i=1}^{n}&space;\max&space;[1&space;-&space;y_i&space;\underbrace{(w^\top&space;\mathbf{x}_i&space;&plus;&space;b)}_{h(\mathbf{x}_i)}&space;,&space;0]}_{\text{Hinge-Loss}}&space;&plus;&space;\underbrace{\|&space;w&space;\|_z^2}_{\ell_2-Rgularizer}" title="\min_{\mathbf{w}} \underbrace{C \sum_{i=1}^{n} \max [1 - y_i \underbrace{(w^\top \mathbf{x}_i + b)}_{h(\mathbf{x}_i)} , 0]}_{\text{Hinge-Loss}} + \underbrace{\| w \|_z^2}_{\ell_2-Rgularizer}" /></a>

The hinge loss is the SVM's error function of choice, whereas the <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_2" title="\ell_2" /></a>-regularizer reflects the complexity of the solution, and penalizes complex solutions. This is an example of empirical risk minimization with a loss function <a href="https://www.codecogs.com/eqnedit.php?latex=\ell" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell" title="\ell" /></a> and a regularizer <a href="https://www.codecogs.com/eqnedit.php?latex=r" target="_blank"><img src="https://latex.codecogs.com/gif.latex?r" title="r" /></a>,

<a href="https://www.codecogs.com/eqnedit.php?latex=\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;\underbrace{\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}_i),&space;y_i)}_{Loss}&space;&plus;&space;\underbrace{\lambda&space;r&space;(w)}_{Regularizer}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;\underbrace{\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}_i),&space;y_i)}_{Loss}&space;&plus;&space;\underbrace{\lambda&space;r&space;(w)}_{Regularizer}" title="\min_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^{n} \underbrace{\ell (h_{\mathbf{w}} (\mathbf{x}_i), y_i)}_{Loss} + \underbrace{\lambda r (w)}_{Regularizer}" /></a>,

where the loss function is a continuous function which penalizes training error, and the regularizer is a continuous function which penalizes classifier complexity. Here, we define <a href="https://www.codecogs.com/eqnedit.php?latex=\lambda" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda" title="\lambda" /></a> as <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{C}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{C}" title="\frac{1}{C}" /></a>.

## Commonly Used Binary Classification Loss Functions

Different Machine Learning algorithms use different loss functions. Here are a few examples:

| Loss <a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}_i,&space;y_i))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}_i,&space;y_i))" title="\ell (h_{\mathbf{w}} (\mathbf{x}_i, y_i))" /></a> | Usage | Comments |
| - | - | - |
| Hinge-Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\max&space;[1&space;-&space;h_{\mathbf{w}}&space;(\mathbf{x}_i)&space;y_i&space;,&space;0]^p" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\max&space;[1&space;-&space;h_{\mathbf{w}}&space;(\mathbf{x}_i)&space;y_i&space;,&space;0]^p" title="\max [1 - h_{\mathbf{w}} (\mathbf{x}_i) y_i , 0]^p" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Standard SVM <a href="https://www.codecogs.com/eqnedit.php?latex=(p=1)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(p=1)" title="(p=1)" /></a> <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> (Differentiable) Squared Hingeleess SVM <a href="https://www.codecogs.com/eqnedit.php?latex=(p=2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(p=2)" title="(p=2)" /></a> | When used for Standard SVM, the loss function denotes the size of the margin between linear separator and its closest points in either class. Only differentiable everywhere with <a href="https://www.codecogs.com/eqnedit.php?latex=p=2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p=2" title="p=2" /></a>. |
| Log-Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\log&space;(1&space;&plus;&space;e^{-h_{\mathbf{w}}&space;(\mathbf{x}_i)&space;y_i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\log&space;(1&space;&plus;&space;e^{-h_{\mathbf{w}}&space;(\mathbf{x}_i)&space;y_i})" title="\log (1 + e^{-h_{\mathbf{w}} (\mathbf{x}_i) y_i})" /></a> | Logistic Regression | One of the most popular loss functions in Machine Learning, since its outputs are well-calibrated probabilities. |
| Exponential Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=e^{-h_{\mathbf{w}}&space;(\mathbf{x}_i)&space;y_i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?e^{-h_{\mathbf{w}}&space;(\mathbf{x}_i)&space;y_i}" title="e^{-h_{\mathbf{w}} (\mathbf{x}_i) y_i}" /></a> | AdaBoost | This function is very aggressive. The loss of a misprediction increases *exponentially* with the value of <a href="https://www.codecogs.com/eqnedit.php?latex=-h_{\mathbf{w}}&space;(\mathbf{x}_i)&space;y_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-h_{\mathbf{w}}&space;(\mathbf{x}_i)&space;y_i" title="-h_{\mathbf{w}} (\mathbf{x}_i) y_i" /></a>. This can lead to nice convergence results, for example in the case of AdaBoost, but it can also cause problems with noisy data. |
| Zero-One Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;(\text{sign}(h_{\mathbf{w}}&space;(\mathbf{x}_i))&space;\neq&space;y_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;(\text{sign}(h_{\mathbf{w}}&space;(\mathbf{x}_i))&space;\neq&space;y_i)" title="\delta (\text{sign}(h_{\mathbf{w}} (\mathbf{x}_i)) \neq y_i)" /></a> | Actual Classification Loss | Non-continuous and thus impractical to optimize. |

Table of Loss Function with Classification <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;\in&space;\{&space;-1,&space;&plus;1&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;\in&space;\{&space;-1,&space;&plus;1&space;\}" title="y \in \{ -1, +1 \}" /></a>

## Commonly Used Regression Loss Functions

Regression algorithms (where a prediction can lie anywhere on the real-number line) also have their own host of loss functions:

| Loss <a href="https://www.codecogs.com/eqnedit.php?latex=\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}_i&space;,&space;y_i))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}_i&space;,&space;y_i))" title="\ell (h_{\mathbf{w}} (\mathbf{x}_i , y_i))" /></a> | Comments |
| - | - |
| Squared Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=(h(\mathbf{x}_i)&space;-&space;y_i)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(h(\mathbf{x}_i)&space;-&space;y_i)^2" title="(h(\mathbf{x}_i) - y_i)^2" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Most popular regression loss function <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Estimates *Mean* Label <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> ADVANTAGE: Differentiable everywhere <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> DISADVANTAGE: Somewhat sensitive to outliers/noise <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Aslo known as Ordinary Least Squares (OLS) |
| Absolute Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert" title="\vert h(\mathbf{x}_i) - y_i \vert" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Also a very popular loss function <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Estimates *Median* Label <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> ADVANTAGE: Less sensitive to noise <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> DISADVANTAGE: Not differentiable at <a href="https://www.codecogs.com/eqnedit.php?latex=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0" title="0" /></a> |
| Huber Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{2}&space;(h(\mathbf{x}_i)&space;-&space;y_i)^2&space;\quad\text{if}\enspace&space;\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert&space;<&space;\delta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{2}&space;(h(\mathbf{x}_i)&space;-&space;y_i)^2&space;\quad\text{if}\enspace&space;\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert&space;<&space;\delta" title="\frac{1}{2} (h(\mathbf{x}_i) - y_i)^2 \quad\text{if}\enspace \vert h(\mathbf{x}_i) - y_i \vert < \delta" /></a>, <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> otherwise <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;(\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert&space;-&space;\frac{\delta}{2})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;(\vert&space;h(\mathbf{x}_i)&space;-&space;y_i&space;\vert&space;-&space;\frac{\delta}{2})" title="\delta (\vert h(\mathbf{x}_i) - y_i \vert - \frac{\delta}{2})" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Also known as Smooth Absolute Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> ADVANTAGE: "Best of Both Worlds" of *Squared* and *Absolute* Loss <br> Once-differentiable <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Takes on behavior of Squared-Loss when loss is small, and Absolute Loss when loss is large. |
| Log-Cosh Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\log&space;(\cosh&space;(h(\mathbf{x}_i)&space;-&space;y_i))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\log&space;(\cosh&space;(h(\mathbf{x}_i)&space;-&space;y_i))" title="\log (\cosh (h(\mathbf{x}_i) - y_i))" /></a>, <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cosh&space;(x)&space;=&space;\frac{e^{x}&space;&plus;&space;e^{-x}}{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cosh&space;(x)&space;=&space;\frac{e^{x}&space;&plus;&space;e^{-x}}{2}" title="\cosh (x) = \frac{e^{x} + e^{-x}}{2}" /></a> | ADVANTAGE: Similar to Huber Loss, but twice differentiable everywhere |

Table of Loss Function with Regression, i.e. <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;\in&space;\mathbb{R}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;\in&space;\mathbb{R}" title="y \in \mathbb{R}" /></a>

## Regularizers

When we look at regularizers it helps to change the formulation of the optimization problem to obtain a better geometric intuition:

<a href="https://www.codecogs.com/eqnedit.php?latex=\min_{\mathbf{w},&space;b}&space;\sum_{i=1}^{n}&space;\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}),&space;y_i)&space;&plus;&space;\lambda&space;r&space;(\mathbf{w})&space;\iff&space;\min_{\mathbf{w},&space;b}&space;\sum_{i=1}^{n}&space;\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}),&space;y_i)&space;\enspace&space;\text{s.t.}&space;\enspace&space;r(\mathbf{w})&space;\leq&space;B" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\min_{\mathbf{w},&space;b}&space;\sum_{i=1}^{n}&space;\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}),&space;y_i)&space;&plus;&space;\lambda&space;r&space;(\mathbf{w})&space;\iff&space;\min_{\mathbf{w},&space;b}&space;\sum_{i=1}^{n}&space;\ell&space;(h_{\mathbf{w}}&space;(\mathbf{x}),&space;y_i)&space;\enspace&space;\text{s.t.}&space;\enspace&space;r(\mathbf{w})&space;\leq&space;B" title="\min_{\mathbf{w}, b} \sum_{i=1}^{n} \ell (h_{\mathbf{w}} (\mathbf{x}), y_i) + \lambda r (\mathbf{w}) \iff \min_{\mathbf{w}, b} \sum_{i=1}^{n} \ell (h_{\mathbf{w}} (\mathbf{x}), y_i) \enspace \text{s.t.} \enspace r(\mathbf{w}) \leq B" /></a>

For each <a href="https://www.codecogs.com/eqnedit.php?latex=\lambda&space;\geq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda&space;\geq&space;0" title="\lambda \geq 0" /></a>, there exists <a href="https://www.codecogs.com/eqnedit.php?latex=B&space;\geq&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?B&space;\geq&space;0" title="B \geq 0" /></a> such that the two formulations for binary classification loss functions are equivalent, and vice versa. In previous sections, <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_2" title="\ell_2" /></a>-regularizer has been introduced as the component in SVM that reflects the complexity of solutions. Besides the <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_2" title="\ell_2" /></a>-regularizer, other types of useful regularizers and their properties are list below:

| Regularizer <a href="https://www.codecogs.com/eqnedit.php?latex=r(\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?r(\mathbf{w})" title="r(\mathbf{w})" /></a> | Properties |
| - | - |
| <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_2" title="\ell_2" /></a>-Regularization <br> <a href="https://www.codecogs.com/eqnedit.php?latex=r(\mathbf{w})&space;=&space;\mathbf{w}^{\top}&space;\mathbf{w}&space;=&space;\|&space;\mathbf{w}&space;\|_2^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?r(\mathbf{w})&space;=&space;\mathbf{w}^{\top}&space;\mathbf{w}&space;=&space;\|&space;\mathbf{w}&space;\|_2^2" title="r(\mathbf{w}) = \mathbf{w}^{\top} \mathbf{w} = \| \mathbf{w} \|_2^2" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> ADVANTAGE: Strictly Convex <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> ADVANTAGE: Differentiable <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> DISADVANTAGE: Uses weights on all features, i.e. relies on all features to some degree (ideally we would like to avoid this) -- these are known as *Dense Solutions*. |
| <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_1" title="\ell_1" /></a>-Regularization <br> <a href="https://www.codecogs.com/eqnedit.php?latex=r(\mathbf{w})&space;=&space;\|&space;\mathbf{w}&space;\|_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?r(\mathbf{w})&space;=&space;\|&space;\mathbf{w}&space;\|_1" title="r(\mathbf{w}) = \| \mathbf{w} \|_1" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Convex (but not strictly) <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> DISADVANTAGE: Not differentiable at <a href="https://www.codecogs.com/eqnedit.php?latex=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0" title="0" /></a> (the point which minimization is intended to bring us to) <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Effect: *Sparse* (i.e. not *Dense*) Solutions |
| <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_p" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_p" title="\ell_p" /></a>-Norm <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\|&space;\mathbf{w}&space;\|_p&space;=&space;(\sum_{i=1}^d&space;v_i^p)^{1/p}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\|&space;\mathbf{w}&space;\|_p&space;=&space;(\sum_{i=1}^d&space;v_i^p)^{1/p}" title="\| \mathbf{w} \|_p = (\sum_{i=1}^d v_i^p)^{1/p}" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> (often <a href="https://www.codecogs.com/eqnedit.php?latex=0&space;<&space;p&space;\leq&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0&space;<&space;p&space;\leq&space;1" title="0 < p \leq 1" /></a>) <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> DISADVANTAGE: Non-convex <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> ADVANTAGE: Very sparse solutions <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Initialization dependent <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> DISADVANTAGE: Not Differentiable |

Table of Types of Regularizers

## Famous Special Cases

This section includes several special cases that deal with risk minimization, such as Ordinary Least Squares, Ridge Regression, Lasso, and Logistic Regression. The following table provides information on their loss functions, regularizers, as well as solutions.

| Loss and Regularizer | Comments |
| - | - |
| Ordinary Least Squares <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{w}^\top&space;x_i&space;-&space;y_i)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{w}^\top&space;x_i&space;-&space;y_i)^2" title="\min_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^{n} (\mathbf{w}^\top x_i - y_i)^2" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Squared Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> No Regularization <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Closed form solution: <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}&space;=&space;(\mathbf{X}&space;\mathbf{X}^{\top})^{-1}&space;\mathbf{X}&space;\mathbf{y}^{\top}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}&space;=&space;(\mathbf{X}&space;\mathbf{X}^{\top})^{-1}&space;\mathbf{X}&space;\mathbf{y}^{\top}" title="\mathbf{w} = (\mathbf{X} \mathbf{X}^{\top})^{-1} \mathbf{X} \mathbf{y}^{\top}" /></a> <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{X}&space;=&space;[\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_n]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{X}&space;=&space;[\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_n]" title="\mathbf{X} = [\mathbf{x}_1, \ldots, \mathbf{x}_n]" /></a> <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{y}&space;=&space;[y_1,&space;\ldots,&space;y_n]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{y}&space;=&space;[y_1,&space;\ldots,&space;y_n]" title="\mathbf{y} = [y_1, \ldots, y_n]" /></a> |
| Ridge Regression <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;x_i&space;-&space;y_i)^2&space;&plus;&space;\lambda&space;\|&space;w&space;\|_2^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;x_i&space;-&space;y_i)^2&space;&plus;&space;\lambda&space;\|&space;w&space;\|_2^2" title="\min_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^{n} (\mathbf{w}^{\top} x_i - y_i)^2 + \lambda \| w \|_2^2" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Squared Loss <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_2" title="\ell_2" /></a>-Regularization <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}&space;=&space;(\mathbf{X}&space;\mathbf{X}^{\top}&space;&plus;&space;\lambda&space;\mathbb{I})^{-1}&space;\mathbf{X}&space;\mathbf{y}^{\top}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}&space;=&space;(\mathbf{X}&space;\mathbf{X}^{\top}&space;&plus;&space;\lambda&space;\mathbb{I})^{-1}&space;\mathbf{X}&space;\mathbf{y}^{\top}" title="\mathbf{w} = (\mathbf{X} \mathbf{X}^{\top} + \lambda \mathbb{I})^{-1} \mathbf{X} \mathbf{y}^{\top}" /></a> |
| Lasso <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;-&space;y_i)^2&space;&plus;&space;\lambda&space;\|&space;\mathbf{w}&space;\|_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;-&space;y_i)^2&space;&plus;&space;\lambda&space;\|&space;\mathbf{w}&space;\|_1" title="\min_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^{n} (\mathbf{w}^{\top} \mathbf{x}_i - y_i)^2 + \lambda \| \mathbf{w} \|_1" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> + sparsity inducing (good for feature selection) <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> + Convex <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> - Not strictly convex (no unique solution) <br> - Not differentiable (at <a href="https://www.codecogs.com/eqnedit.php?latex=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0" title="0" /></a>) <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Solve with (sub)-gradient descent |
| Elastic Net <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;-&space;y_i)^2&space;&plus;&space;\alpha&space;\|&space;\mathbf{w}&space;\|_1&space;&plus;&space;(1&space;-&space;\alpha)&space;\|&space;\mathbf{w}&space;\|_2^2&space;\enspace&space;\text{where}&space;\enspace&space;\alpha&space;\in&space;[0,&space;1)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\min_{\mathbf{w}}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;-&space;y_i)^2&space;&plus;&space;\alpha&space;\|&space;\mathbf{w}&space;\|_1&space;&plus;&space;(1&space;-&space;\alpha)&space;\|&space;\mathbf{w}&space;\|_2^2&space;\enspace&space;\text{where}&space;\enspace&space;\alpha&space;\in&space;[0,&space;1)" title="\min_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^{n} (\mathbf{w}^{\top} \mathbf{x}_i - y_i)^2 + \alpha \| \mathbf{w} \|_1 + (1 - \alpha) \| \mathbf{w} \|_2^2 \enspace \text{where} \enspace \alpha \in [0, 1)" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> ADVANTAGE: Strictly convex (i.e. unique solution) <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> + sparsity inducing (good for feature selection) <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> + Dual of squared-loss SVM <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> DISADVANTAGE: - Non-differentiable |
| Logistic Regression <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\min_{\mathbf{w},b}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;\log&space;(1&space;&plus;&space;e^{-y_i&space;(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;&plus;&space;b)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\min_{\mathbf{w},b}&space;\frac{1}{n}&space;\sum_{i=1}^{n}&space;\log&space;(1&space;&plus;&space;e^{-y_i&space;(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;&plus;&space;b)})" title="\min_{\mathbf{w},b} \frac{1}{n} \sum_{i=1}^{n} \log (1 + e^{-y_i (\mathbf{w}^{\top} \mathbf{x}_i + b)})" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Often <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_1" title="\ell_1" /></a> or <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_2" title="\ell_2" /></a> Regularized <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Solve with gradient descent <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> <a href="https://www.codecogs.com/eqnedit.php?latex=Pr(y&space;\vert&space;x)&space;=&space;\frac{1}{1&space;&plus;&space;e^{-y&space;(\mathbf{w}^{\top}&space;x&space;&plus;&space;b)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Pr(y&space;\vert&space;x)&space;=&space;\frac{1}{1&space;&plus;&space;e^{-y&space;(\mathbf{w}^{\top}&space;x&space;&plus;&space;b)}}" title="Pr(y \vert x) = \frac{1}{1 + e^{-y (\mathbf{w}^{\top} x + b)}}" /></a> |
| Linear Support Vector Machine <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\min_{\mathbf{w},&space;b}&space;C&space;\sum_{i=1}^{n}&space;\max&space;[1&space;-&space;y_i&space;(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;&plus;&space;b),&space;0]&space;&plus;&space;\|&space;\mathbf{w}&space;\|_2^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\min_{\mathbf{w},&space;b}&space;C&space;\sum_{i=1}^{n}&space;\max&space;[1&space;-&space;y_i&space;(\mathbf{w}^{\top}&space;\mathbf{x}_i&space;&plus;&space;b),&space;0]&space;&plus;&space;\|&space;\mathbf{w}&space;\|_2^2" title="\min_{\mathbf{w}, b} C \sum_{i=1}^{n} \max [1 - y_i (\mathbf{w}^{\top} \mathbf{x}_i + b), 0] + \| \mathbf{w} \|_2^2" /></a> | <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Typically <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_2" title="\ell_2" /></a> regularized (sometimes <a href="https://www.codecogs.com/eqnedit.php?latex=\ell_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ell_1" title="\ell_1" /></a>) <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Quadratic program <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> When *kernelized* leads to **sparse** solutions <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\cdot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\cdot" title="\cdot" /></a> Kernelized version can be solved very efficiently with specialized algorithms (e.g. SMO) |

Table of Special Cases

Some additional notes on the Special Cases:

1. Ridge Regression is very fast if data isn't too high dimensional.
2. There is an interesting connection between Ordinary Least Squares and the first principal component of PCA (Principal Component Analysis). PCA also minimizes square loss, but looks at perpendicular loss (the horizontal distance between each point and the regression line) instead.
