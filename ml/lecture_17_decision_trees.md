# Decision Trees

## Motivation for Decision Trees

Let us return to the *k-nearest neighbor classifier*. In low dimensions it is actually quite powerful: It can learn non-linear decision boundaries and naturally can handle multi-class problems. There are however a few catches: kNN uses a lot of storage (as we are required to store the entire training data), the more training data we have the slower it becomes during testing (as we need to compute distances to all training inputs), and finally we need a good distance metric.

Most data that is interesting has some inherent structure. In the k-NN case we make the assumption that similar inputs have similar neighbors. This would imply that data points of various classes are not randomly sprinkled across the space, but instead appear in clusters of more or less homogeneous class assignments. Although there are *efficient data structures* enable faster nearest neighbor search, it is important to remember that the ultimate goal of the classifier is simply to give an accurate prediction. Imagine a binary classification problem with positive and negative label, you would know that its neighbors will be positive even before you compute the distances to each one of these million distances. It is therefore sufficient to simply know that the test point is an area where all neighbors are positive, its exactly identity is irrelevant.

Decision trees are exploiting exactly that. Here, we do not store the training data, instead we use the training data to build a tree structure that recursively divides the space into regions with similar labels. The root node of the tree represents the entire data set. This set is then split roughly in half along one dimension by a simple threshold <a href="https://www.codecogs.com/eqnedit.php?latex=t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?t" title="t" /></a>. All points that have a feature value <a href="https://www.codecogs.com/eqnedit.php?latex=\geq&space;t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\geq&space;t" title="\geq t" /></a> fall into the right child node, all the others into the left child node. The threshold <a href="https://www.codecogs.com/eqnedit.php?latex=t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?t" title="t" /></a> and the dimension are chosen so that the resulting child nodes are purer in terms of class membership. Ideally all positive points fall into one child node and all negative points in the other. If this is the case, the tree is done. If not, the leaf nodes are again split until eventually all leaves are pure (i.e. all its data points contain the same label) or cannot be split any further (in the rare case with two identical points of different labels).

Decision trees have several nice advantages over nearest neighbor algorithms:

1. once the tree is constructed, the training data does not need to be stored. Instead, we can simply store how many points of each label ended up in each leaf -- typically these are pure so we just have to store the label of all points.
2. decision trees are very fast during test time, as test inputs simply need to traverse down the tree to a leaf -- the prediction is the majority label of the leaf.
3. decision trees require no metric because the splits are based on feature thresholds and not distances.

*New goal*: Build a tree that is:

1. Maximally compact
2. Only has pure leaves

Quiz: Is it always possible to find a consistent tree?

Yes, if and only if no two input vectors have identical features but different labels.

*Bad news! Find a* **minimum size** *tree is NP-Hard!!*

*Good News*: we can approximate it very effectively with a greedy strategy. We keep splitting the data to minimize an *impurity function* that measures label purity amongst the children.

## Impurity Functions

Data: <a href="https://www.codecogs.com/eqnedit.php?latex=S&space;=&space;\{&space;(\mathbf{x}_1&space;,&space;y_1),&space;\ldots&space;,&space;(\mathbf{x}_n&space;,&space;y_n)&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?S&space;=&space;\{&space;(\mathbf{x}_1&space;,&space;y_1),&space;\ldots&space;,&space;(\mathbf{x}_n&space;,&space;y_n)&space;\}" title="S = \{ (\mathbf{x}_1 , y_1), \ldots , (\mathbf{x}_n , y_n) \}" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=y_i&space;\in&space;\{&space;1,&space;\ldots&space;,&space;c&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i&space;\in&space;\{&space;1,&space;\ldots&space;,&space;c&space;\}" title="y_i \in \{ 1, \ldots , c \}" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=c" target="_blank"><img src="https://latex.codecogs.com/gif.latex?c" title="c" /></a> is the number of classes

### Gini Impurity

Let <a href="https://www.codecogs.com/eqnedit.php?latex=S_k&space;\subseteq&space;S" target="_blank"><img src="https://latex.codecogs.com/gif.latex?S_k&space;\subseteq&space;S" title="S_k \subseteq S" /></a> where <a href="https://www.codecogs.com/eqnedit.php?latex=S_k&space;=&space;\{&space;(\mathbf{x}&space;,&space;y)&space;\in&space;S&space;:&space;y=k&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?S_k&space;=&space;\{&space;(\mathbf{x}&space;,&space;y)&space;\in&space;S&space;:&space;y=k&space;\}" title="S_k = \{ (\mathbf{x} , y) \in S : y=k \}" /></a> (all inputs with labels <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a>)

<a href="https://www.codecogs.com/eqnedit.php?latex=S&space;=&space;S_1&space;\cup&space;\cdots&space;\cup&space;S_c" target="_blank"><img src="https://latex.codecogs.com/gif.latex?S&space;=&space;S_1&space;\cup&space;\cdots&space;\cup&space;S_c" title="S = S_1 \cup \cdots \cup S_c" /></a>

*Define*:

<a href="https://www.codecogs.com/eqnedit.php?latex=p_k&space;=&space;\frac{\vert&space;S_k&space;\vert}{\vert&space;S&space;\vert}&space;\leftarrow&space;\text{fraction&space;of&space;inputs&space;in&space;}&space;S&space;\text{&space;with&space;label&space;}&space;k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_k&space;=&space;\frac{\vert&space;S_k&space;\vert}{\vert&space;S&space;\vert}&space;\leftarrow&space;\text{fraction&space;of&space;inputs&space;in&space;}&space;S&space;\text{&space;with&space;label&space;}&space;k" title="p_k = \frac{\vert S_k \vert}{\vert S \vert} \leftarrow \text{fraction of inputs in } S \text{ with label } k" /></a>

*Note*: This is different from Gini coefficient. See Gini impurity (not to be confused with the Gini Coefficient) of a leaf:

<a href="https://www.codecogs.com/eqnedit.php?latex=G(S)&space;=&space;\sum_{k=1}^{c}&space;p_k&space;(1&space;-&space;p_k)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?G(S)&space;=&space;\sum_{k=1}^{c}&space;p_k&space;(1&space;-&space;p_k)" title="G(S) = \sum_{k=1}^{c} p_k (1 - p_k)" /></a>

Gini impurity of a tree:

<a href="https://www.codecogs.com/eqnedit.php?latex=G^{T}&space;(S)&space;=&space;\frac{\vert&space;S_L&space;\vert}{\vert&space;S&space;\vert}&space;G^{T}&space;(S_L)&space;&plus;&space;\frac{\vert&space;S_R&space;\vert}{\vert&space;S&space;\vert}&space;G^{T}&space;(S_R)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?G^{T}&space;(S)&space;=&space;\frac{\vert&space;S_L&space;\vert}{\vert&space;S&space;\vert}&space;G^{T}&space;(S_L)&space;&plus;&space;\frac{\vert&space;S_R&space;\vert}{\vert&space;S&space;\vert}&space;G^{T}&space;(S_R)" title="G^{T} (S) = \frac{\vert S_L \vert}{\vert S \vert} G^{T} (S_L) + \frac{\vert S_R \vert}{\vert S \vert} G^{T} (S_R)" /></a>

where:

- <a href="https://www.codecogs.com/eqnedit.php?latex=(S&space;=&space;S_L&space;\cup&space;S_R)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(S&space;=&space;S_L&space;\cup&space;S_R)" title="(S = S_L \cup S_R)" /></a>
- <a href="https://www.codecogs.com/eqnedit.php?latex=S_L&space;\cap&space;S_R&space;=&space;\varnothing" target="_blank"><img src="https://latex.codecogs.com/gif.latex?S_L&space;\cap&space;S_R&space;=&space;\varnothing" title="S_L \cap S_R = \varnothing" /></a>
- <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\vert&space;S_L&space;\vert}{S}&space;\leftarrow&space;\text{fraction&space;of&space;inputs&space;in&space;left&space;substree}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\vert&space;S_L&space;\vert}{S}&space;\leftarrow&space;\text{fraction&space;of&space;inputs&space;in&space;left&space;substree}" title="\frac{\vert S_L \vert}{S} \leftarrow \text{fraction of inputs in left substree}" /></a>
- <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\vert&space;S_R&space;\vert}{\vert&space;S&space;\vert}&space;\leftarrow&space;\text{fraction&space;of&space;inputs&space;in&space;right&space;subtree}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\vert&space;S_R&space;\vert}{\vert&space;S&space;\vert}&space;\leftarrow&space;\text{fraction&space;of&space;inputs&space;in&space;right&space;subtree}" title="\frac{\vert S_R \vert}{\vert S \vert} \leftarrow \text{fraction of inputs in right subtree}" /></a>

### Entropy

Let <a href="https://www.codecogs.com/eqnedit.php?latex=p_1,&space;\ldots&space;,&space;p_k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_1,&space;\ldots&space;,&space;p_k" title="p_1, \ldots , p_k" /></a> be defined as before. We know what we don't want (Uniform Distribution): <a href="https://www.codecogs.com/eqnedit.php?latex=p_1&space;=&space;p_2&space;=&space;\cdots&space;=&space;p_c&space;=&space;\frac{1}{c}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_1&space;=&space;p_2&space;=&space;\cdots&space;=&space;p_c&space;=&space;\frac{1}{c}" title="p_1 = p_2 = \cdots = p_c = \frac{1}{c}" /></a> This is the worst case since each leaf is equally likely. Prediction is random guessing. Define the impurity as how close we are to uniform. Use KL-Divergence to compute "closeness".

Note: KL-Divergence is not a metric because it is not symmetric, i.e., <a href="https://www.codecogs.com/eqnedit.php?latex=KL(p&space;\|&space;q)&space;\neq&space;KL(q&space;\|&space;p)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?KL(p&space;\|&space;q)&space;\neq&space;KL(q&space;\|&space;p)" title="KL(p \| q) \neq KL(q \| p)" /></a>.

Let <a href="https://www.codecogs.com/eqnedit.php?latex=q_1&space;,&space;\ldots&space;,&space;q_c" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q_1&space;,&space;\ldots&space;,&space;q_c" title="q_1 , \ldots , q_c" /></a> be the uniform label/distribution. i.e. <a href="https://www.codecogs.com/eqnedit.php?latex=q_k&space;=&space;\frac{1}{c}&space;\forall&space;k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q_k&space;=&space;\frac{1}{c}&space;\forall&space;k" title="q_k = \frac{1}{c} \forall k" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;KL(p&space;\|&space;q)&space;&&space;=&space;\sum_{k=1}^{c}&space;p_k&space;\log&space;\frac{p_k}{q_k}&space;\geq&space;0&space;\leftarrow&space;\text{KL-Divergence}&space;\\&space;&&space;=&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;-&space;p_k&space;\log&space;(q_k)&space;\enspace&space;\text{where}&space;\enspace&space;q_k&space;=&space;\frac{1}{c}&space;\\&space;&&space;=&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;&plus;&space;p_k&space;\log&space;(c)&space;\\&space;&&space;=&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;&plus;&space;\log&space;(c)&space;\sum_{k}&space;p_k&space;\enspace&space;\text{where}&space;\enspace&space;\log&space;(c)&space;\leftarrow&space;\text{constant,&space;}&space;\sum_{k}&space;p_k&space;=&space;1&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;KL(p&space;\|&space;q)&space;&&space;=&space;\sum_{k=1}^{c}&space;p_k&space;\log&space;\frac{p_k}{q_k}&space;\geq&space;0&space;\leftarrow&space;\text{KL-Divergence}&space;\\&space;&&space;=&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;-&space;p_k&space;\log&space;(q_k)&space;\enspace&space;\text{where}&space;\enspace&space;q_k&space;=&space;\frac{1}{c}&space;\\&space;&&space;=&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;&plus;&space;p_k&space;\log&space;(c)&space;\\&space;&&space;=&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;&plus;&space;\log&space;(c)&space;\sum_{k}&space;p_k&space;\enspace&space;\text{where}&space;\enspace&space;\log&space;(c)&space;\leftarrow&space;\text{constant,&space;}&space;\sum_{k}&space;p_k&space;=&space;1&space;\end{align*}" title="\begin{align*} KL(p \| q) & = \sum_{k=1}^{c} p_k \log \frac{p_k}{q_k} \geq 0 \leftarrow \text{KL-Divergence} \\ & = \sum_{k} p_k \log (p_k) - p_k \log (q_k) \enspace \text{where} \enspace q_k = \frac{1}{c} \\ & = \sum_{k} p_k \log (p_k) + p_k \log (c) \\ & = \sum_{k} p_k \log (p_k) + \log (c) \sum_{k} p_k \enspace \text{where} \enspace \log (c) \leftarrow \text{constant, } \sum_{k} p_k = 1 \end{align*}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\max_{p}&space;\text{KL}&space;(p\|q)&space;&&space;=&space;\max_{p}&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;\\&space;&&space;=&space;\min_{p}&space;-&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;\\&space;&&space;=&space;\min_{p}&space;H(s)&space;\leftarrow&space;\text{Entropy}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;\max_{p}&space;\text{KL}&space;(p\|q)&space;&&space;=&space;\max_{p}&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;\\&space;&&space;=&space;\min_{p}&space;-&space;\sum_{k}&space;p_k&space;\log&space;(p_k)&space;\\&space;&&space;=&space;\min_{p}&space;H(s)&space;\leftarrow&space;\text{Entropy}&space;\end{align*}" title="\begin{align*} \max_{p} \text{KL} (p\|q) & = \max_{p} \sum_{k} p_k \log (p_k) \\ & = \min_{p} - \sum_{k} p_k \log (p_k) \\ & = \min_{p} H(s) \leftarrow \text{Entropy} \end{align*}" /></a>

*Entropy over tree*:

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;H(S)&space;&&space;=&space;p^L&space;H(S^L)&space;&plus;&space;p^R&space;H(S^R)&space;\\&space;p^L&space;&&space;=&space;\frac{\vert&space;S^L&space;\vert}{\vert&space;S&space;\vert},&space;p^R&space;=&space;\frac{\vert&space;S^R&space;\vert}{\vert&space;S&space;\vert}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;H(S)&space;&&space;=&space;p^L&space;H(S^L)&space;&plus;&space;p^R&space;H(S^R)&space;\\&space;p^L&space;&&space;=&space;\frac{\vert&space;S^L&space;\vert}{\vert&space;S&space;\vert},&space;p^R&space;=&space;\frac{\vert&space;S^R&space;\vert}{\vert&space;S&space;\vert}&space;\end{align*}" title="\begin{align*} H(S) & = p^L H(S^L) + p^R H(S^R) \\ p^L & = \frac{\vert S^L \vert}{\vert S \vert}, p^R = \frac{\vert S^R \vert}{\vert S \vert} \end{align*}" /></a>

## ID3-Algorithm

*Base Cases*:

<a href="https://www.codecogs.com/eqnedit.php?latex=\text{ID3}(S):&space;\begin{array}{ll}&space;\text{if&space;}&space;\exists&space;\bar{y}&space;\text{&space;s.t.&space;}&space;\forall&space;(x,&space;y)&space;\in&space;S,&space;y=\bar{y}&space;\Rightarrow&space;\text{return&space;leaf&space;with&space;label&space;}&space;\bar{y}&space;\\&space;\text{if&space;}&space;\exists&space;\bar{x}&space;\text{&space;s.t.&space;}&space;\forall&space;(x,&space;y)&space;\in&space;S,&space;x=\bar{x}&space;\Rightarrow&space;\text{return&space;leaf&space;with&space;mode}(y:&space;(x,&space;y)&space;\in&space;S)&space;\text{&space;or&space;mean&space;(regression)}&space;\end{array}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\text{ID3}(S):&space;\begin{array}{ll}&space;\text{if&space;}&space;\exists&space;\bar{y}&space;\text{&space;s.t.&space;}&space;\forall&space;(x,&space;y)&space;\in&space;S,&space;y=\bar{y}&space;\Rightarrow&space;\text{return&space;leaf&space;with&space;label&space;}&space;\bar{y}&space;\\&space;\text{if&space;}&space;\exists&space;\bar{x}&space;\text{&space;s.t.&space;}&space;\forall&space;(x,&space;y)&space;\in&space;S,&space;x=\bar{x}&space;\Rightarrow&space;\text{return&space;leaf&space;with&space;mode}(y:&space;(x,&space;y)&space;\in&space;S)&space;\text{&space;or&space;mean&space;(regression)}&space;\end{array}" title="\text{ID3}(S): \begin{array}{ll} \text{if } \exists \bar{y} \text{ s.t. } \forall (x, y) \in S, y=\bar{y} \Rightarrow \text{return leaf with label } \bar{y} \\ \text{if } \exists \bar{x} \text{ s.t. } \forall (x, y) \in S, x=\bar{x} \Rightarrow \text{return leaf with mode}(y: (x, y) \in S) \text{ or mean (regression)} \end{array}" /></a>

The equation above indicates the ID3 algorithm stop under two cases. The first case is that all the data points in a subset have the same label. If this happens, we should stop splitting the subset and create a leaf with label <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a>. The other case is there are no more attributes could be used to split the subset. Then we create a leaf and label it with the most common <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a>.

Try all features and all possible splits. Pick the split that minimizes impurity (e.g. <a href="https://www.codecogs.com/eqnedit.php?latex=s>t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?s>t" title="s>t" /></a>) where <a href="https://www.codecogs.com/eqnedit.php?latex=f&space;\leftarrow&space;\text{feature}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f&space;\leftarrow&space;\text{feature}" title="f \leftarrow \text{feature}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=t&space;\leftarrow&space;\text{threshold}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?t&space;\leftarrow&space;\text{threshold}" title="t \leftarrow \text{threshold}" /></a>

*Recursion*:

<a href="https://www.codecogs.com/eqnedit.php?latex=\text{Define}:&space;\begin{bmatrix}&space;S^L&space;=&space;\{&space;(x,&space;y)&space;\in&space;S:&space;x_f&space;\leq&space;t&space;\}&space;\\&space;S^R&space;=&space;\{&space;(x,&space;y)&space;\in&space;S:&space;x_f&space;>&space;t&space;\}&space;\end{bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\text{Define}:&space;\begin{bmatrix}&space;S^L&space;=&space;\{&space;(x,&space;y)&space;\in&space;S:&space;x_f&space;\leq&space;t&space;\}&space;\\&space;S^R&space;=&space;\{&space;(x,&space;y)&space;\in&space;S:&space;x_f&space;>&space;t&space;\}&space;\end{bmatrix}" title="\text{Define}: \begin{bmatrix} S^L = \{ (x, y) \in S: x_f \leq t \} \\ S^R = \{ (x, y) \in S: x_f > t \} \end{bmatrix}" /></a>

*Quiz*: Why don't we stop if no split can improve impurity?

*Example*: **XOR**

- First split does not improve impurity
- Decision trees are myopic

## Regression Trees

### CART: Classification and Regression Trees

Assume labels are continuous: <a href="https://www.codecogs.com/eqnedit.php?latex=y_i&space;\in&space;\mathbb{R}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i&space;\in&space;\mathbb{R}" title="y_i \in \mathbb{R}" /></a>

*Impurity: Squared Loss*

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;L(S)&space;=&space;\frac{1}{\vert&space;S&space;\vert}&space;\sum_{(x,y)\in&space;S}&space;(y&space;-&space;\bar{y}_S)^2&space;&&space;\leftarrow&space;\text{Average&space;squared&space;difference&space;from&space;average&space;label}&space;\\&space;\text{where&space;}&space;\bar{y}_S&space;=&space;\frac{1}{\vert&space;S&space;\vert}&space;\sum_{(x,y)\in&space;S}&space;y&space;&&space;\leftarrow&space;\text{Average&space;label}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;L(S)&space;=&space;\frac{1}{\vert&space;S&space;\vert}&space;\sum_{(x,y)\in&space;S}&space;(y&space;-&space;\bar{y}_S)^2&space;&&space;\leftarrow&space;\text{Average&space;squared&space;difference&space;from&space;average&space;label}&space;\\&space;\text{where&space;}&space;\bar{y}_S&space;=&space;\frac{1}{\vert&space;S&space;\vert}&space;\sum_{(x,y)\in&space;S}&space;y&space;&&space;\leftarrow&space;\text{Average&space;label}&space;\end{align*}" title="\begin{align*} L(S) = \frac{1}{\vert S \vert} \sum_{(x,y)\in S} (y - \bar{y}_S)^2 & \leftarrow \text{Average squared difference from average label} \\ \text{where } \bar{y}_S = \frac{1}{\vert S \vert} \sum_{(x,y)\in S} y & \leftarrow \text{Average label} \end{align*}" /></a>

At leaves, predict <a href="https://www.codecogs.com/eqnedit.php?latex=\bar{y}_S" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bar{y}_S" title="\bar{y}_S" /></a>. Finding best split only costs <a href="https://www.codecogs.com/eqnedit.php?latex=O(n&space;\log&space;n)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(n&space;\log&space;n)" title="O(n \log n)" /></a>.

*CART summary*:

- CART are very light weight classifiers
- Very fast during testing
- Usually not competitive in accuracy but can become very strong through *bagging* (Random Forests) and *boosting* (Gradient Boosted Trees)

## Parametric vs. Non-Parametric Algorithms

So far we have introduced a variety of algorithms. One can categorize these into different families, such as generative vs. discriminative, or probabilistic vs. non-probabilistic. Here we will introduce another one, *parametric* vs. *non-parametric*.

A *parametric* algorithm is one that has a constant set of parameters, which is independent of the number of training samples. You can think of it as the amount of much space you need to store the trained classifier. An examples for a parametric algorithm is the Perceptron algorithm, or logistic regression. Their parameters consist of <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b" title="b" /></a>, which define the separating hyperplane. The dimension of <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> depends on the dimension of the training data, but not on how many training samples you use for training.

In contrast, the number of parameters of a *non-parametric* algorithm scales as a function of the training samples. An example of a non-parametric algorithm is the <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a>-Nearest Neighbors classifier. Here, during "training" we store the entire training data -- so the parameters that we learn are identical to the training set and the number of parameters (the storage we require) grows linearly with the training set size.

An interesting edge case is kernel-SVM. Here it depends very much which kernel we are using. E.g. linear SVMs are parametric (for the same reason as the Perceptron or logistic regression). So if the kernel is linear the algorithm is clearly parametric. However, if we use an RBF kernel then we cannot represent the classifier of a hyper-plane of finite dimensions. Instead we have to store the support vectors and their corresponding dual variables <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha_i" title="\alpha_i" /></a> -- the number of which is a function of the data set size (and complexity). Hence, the kernel-SVM with an RBF kernel is non-parametric. A strange in-between case is the polynomial kernel. It represents a hyper-plane in an extremely high but still finite-dimensional space. So technically one could represent any solution of an SVM with a polynomial kernel as a hyperplane in an extremely high dimensional space with a fixed number of parameters, and the algorithm is therefore (technically) parametric. However, in practice this is not practical. Instead, it is almost always more economical to store the support vectors and their corresponding dual variables (just like with the RBF kernel). It therefore is technically parametric but for all means and purposes behaves like a non-parametric algorithm.

Decision Trees are also an interesting case. It they are trained to full depth they are non-parametric, as the depth of a decision tree scales as a function of the training data (in practice <a href="https://www.codecogs.com/eqnedit.php?latex=O(\log_2&space;(n))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(\log_2&space;(n))" title="O(\log_2 (n))" /></a>). If we however limit the tree depth by a maximum value they become parametric (as an upper bound of the model size is now known prior to observing the training data).
