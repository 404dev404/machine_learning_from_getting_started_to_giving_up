# KD Trees

## Time Complexity of k-NN

Let's look at the time complexity of k-NN. We are in a <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a>-dimensional space. To make it easier, let's assume we've already processed some number of inputs, and we want to know the time complexity of adding one more data point. When training, k-NN simply memorizes the labels of each data point it sees. This means adding one more data point is <a href="https://www.codecogs.com/eqnedit.php?latex=O(d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(d)" title="O(d)" /></a>. When testing, we need to compute the distance between our new data point and all of the data points we trained on. If <a href="https://www.codecogs.com/eqnedit.php?latex=n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n" title="n" /></a> is the number of data points we have trained on, then our time complexity for training is <a href="https://www.codecogs.com/eqnedit.php?latex=O(dn)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(dn)" title="O(dn)" /></a>. Classifying one test input is also <a href="https://www.codecogs.com/eqnedit.php?latex=O(dn)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(dn)" title="O(dn)" /></a>. To achieve the best accuracy we can, we would like our training data set to be very large (<a href="https://www.codecogs.com/eqnedit.php?latex=n&space;\gg&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n&space;\gg&space;0" title="n \gg 0" /></a>), but this will soon become a serious bottleneck during test time.

**Goal**: Can we make k-NN faster during testing? We can if we use clever data structures.

## k-Dimensional Trees

The general idea of KD-trees is to partition the feature space. We want to discard lots of data points immediately because their partition is further away than our <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a> closest neighbors. We partition in the following way:

1. Divide your data into two halves, e.g. left and right, along one feature.
2. For each training input, remember the half it lies in.

How can this partitioning speed up testing? Let's think about it for the one neighbor case.

1. Identify which side the test point lies in, e.g. the right side.
2. Find the nearest neighbor <a href="https://www.codecogs.com/eqnedit.php?latex=x_{NN}^{R}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_{NN}^{R}" title="x_{NN}^{R}" /></a> of <a href="https://www.codecogs.com/eqnedit.php?latex=x_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_t" title="x_t" /></a> in the same side. The <a href="https://www.codecogs.com/eqnedit.php?latex=R" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R" title="R" /></a> denotes that our nearest neighbor is also on the right side.
3. Compute the distance between <a href="https://www.codecogs.com/eqnedit.php?latex=x_y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_y" title="x_y" /></a> and the dividing "wall". Denote this as <a href="https://www.codecogs.com/eqnedit.php?latex=d_w" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d_w" title="d_w" /></a>. If <a href="https://www.codecogs.com/eqnedit.php?latex=d_w&space;>&space;d(x_t&space;,&space;x_{NN}^{R})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d_w&space;>&space;d(x_t&space;,&space;x_{NN}^{R})" title="d_w > d(x_t , x_{NN}^{R})" /></a> you are done, and we get a 2x speedup.

In other words: if the distance to the partition is larger than the distance to our closest neighbor, we know that none of the data points *inside* that partition can be closer. We can avoid computing the distance to any of the points in that entire partition. We can prove this formally with the triangular inequality. Let <a href="https://www.codecogs.com/eqnedit.php?latex=d(x_t&space;,&space;x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d(x_t&space;,&space;x)" title="d(x_t , x)" /></a> denote the distance between our test point <a href="https://www.codecogs.com/eqnedit.php?latex=x_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_t" title="x_t" /></a> and a candidate <a href="https://www.codecogs.com/eqnedit.php?latex=x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x" title="x" /></a>. We know that <a href="https://www.codecogs.com/eqnedit.php?latex=x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x" title="x" /></a> lies on the other side of the wall, so this distance is dissected into two parts <a href="https://www.codecogs.com/eqnedit.php?latex=d(x_t&space;,&space;x)&space;=&space;d_1&space;&plus;&space;d_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d(x_t&space;,&space;x)&space;=&space;d_1&space;&plus;&space;d_2" title="d(x_t , x) = d_1 + d_2" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=d_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d_1" title="d_1" /></a> is the part of the distance on <a href="https://www.codecogs.com/eqnedit.php?latex=x_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_t" title="x_t" /></a>'s side of the wall and <a href="https://www.codecogs.com/eqnedit.php?latex=d_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d_2" title="d_2" /></a> is the part of the distance on <a href="https://www.codecogs.com/eqnedit.php?latex=x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x" title="x" /></a>'s side of the wall. Also let <a href="https://www.codecogs.com/eqnedit.php?latex=d_w" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d_w" title="d_w" /></a> denote the shortest distance from <a href="https://www.codecogs.com/eqnedit.php?latex=x_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_t" title="x_t" /></a> to the wall. We know that <a href="https://www.codecogs.com/eqnedit.php?latex=d_1&space;>&space;d_w" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d_1&space;>&space;d_w" title="d_1 > d_w" /></a> and therefore it follows that

<a href="https://www.codecogs.com/eqnedit.php?latex=d(x_t&space;,&space;x)&space;=&space;d_1&space;&plus;&space;d_2&space;\geq&space;d_w&space;&plus;&space;d_2&space;\geq&space;d_w" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d(x_t&space;,&space;x)&space;=&space;d_1&space;&plus;&space;d_2&space;\geq&space;d_w&space;&plus;&space;d_2&space;\geq&space;d_w" title="d(x_t , x) = d_1 + d_2 \geq d_w + d_2 \geq d_w" /></a>

This implies that if <a href="https://www.codecogs.com/eqnedit.php?latex=d_w" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d_w" title="d_w" /></a> is already larger than the distance to the current best candidate point for the nearest neighbor, we can safely discard <a href="https://www.codecogs.com/eqnedit.php?latex=x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x" title="x" /></a> as a candidate.

## KD-Tree Data Structure

*Tree Construction*:

1. Split data recursively in half on exactly one feature.
2. Rotate through features.

When rotating through features, a good heuristic is to pick the feature with maximum variance.

Which partitions can be pruned?

Which must be searched and in what order?

*Pros*:

- Exact.
- Easy to build.

*Cons*:

- Curse of dimensionality makes KD-Trees ineffective for higher number of dimensions.
- All splits are axis aligned.

*Approximation*: Limit search to <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> leafs only.

## Ball-Trees

Similar to KD-trees, but instead of boxes use hyper-spheres (balls). As before we can dissect the distance and use the triangular inequality

<a href="https://www.codecogs.com/eqnedit.php?latex=d(x_t&space;,&space;x)&space;=&space;d_1&space;&plus;&space;d_2&space;\geq&space;d_b&space;&plus;&space;d_2&space;\geq&space;d_b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d(x_t&space;,&space;x)&space;=&space;d_1&space;&plus;&space;d_2&space;\geq&space;d_b&space;&plus;&space;d_2&space;\geq&space;d_b" title="d(x_t , x) = d_1 + d_2 \geq d_b + d_2 \geq d_b" /></a>

If the distance to the ball, <a href="https://www.codecogs.com/eqnedit.php?latex=d_b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d_b" title="d_b" /></a>, is larger than distance to the currently closest neighbor, we can safely ignore the ball and all points within. The ball structure allows us to partition the data along an underlying manifold that our points are on, instead of repeatedly dissecting the entire feature space (as in KD-Trees).

### Ball-Tree Construction

Input: set <a href="https://www.codecogs.com/eqnedit.php?latex=S" target="_blank"><img src="https://latex.codecogs.com/gif.latex?S" title="S" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=n&space;=&space;\vert&space;S&space;\vert" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n&space;=&space;\vert&space;S&space;\vert" title="n = \vert S \vert" /></a>, <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;&&space;\textbf{Algorithm}&space;\text{&space;Ball-Tree&space;Pseudo-Code}&space;\\&space;&&space;\textbf{procedure&space;}&space;\text{BALLTREE}(S,&space;k)&space;\\&space;&&space;\qquad&space;\textbf{if&space;}&space;\vert&space;S&space;\vert&space;<&space;k&space;\textbf{&space;then&space;}&space;\text{stop}&space;\textbf{&space;end&space;if}&space;&&&space;\text{Return&space;leaf&space;containing&space;}&space;S&space;\\&space;&&space;\qquad&space;\text{pick&space;}&space;x_0&space;\in&space;S&space;\text{&space;uniformly&space;at&space;random}&space;\\&space;&&space;\qquad&space;\text{pick&space;}&space;x_1&space;=&space;\operatorname*{argmax}_{x&space;\in&space;S}&space;d(x_0&space;,&space;x)&space;\\&space;&&space;\qquad&space;\text{pick&space;}&space;x_2&space;=&space;\operatorname*{argmax}_{x&space;\in&space;S}&space;d(x_1&space;,&space;x)&space;\\&space;&&space;\qquad&space;\forall&space;i&space;=&space;1&space;\ldots&space;\vert&space;S&space;\vert,&space;z_i&space;=&space;(x_1&space;-&space;x_2)^{\top}&space;x_i&space;\qquad&space;\leftarrow&space;\text{project&space;data&space;onto&space;}&space;(x_1&space;-&space;x_2)&space;\\&space;&&space;\qquad&space;m&space;=&space;\text{median}(z_1&space;,&space;\ldots&space;,&space;z_{\vert&space;S&space;\vert})&space;\\&space;&&space;\qquad&space;S_L&space;=&space;\{&space;x&space;\in&space;S:&space;z_i&space;<&space;m&space;\}&space;\\&space;&&space;\qquad&space;S_R&space;=&space;\{&space;x&space;\in&space;S:&space;z_i&space;\geq&space;m&space;\}&space;\\&space;&&space;\qquad&space;\textbf{Return&space;}&space;\text{tree:}&space;\\&space;&&space;\qquad&space;\text{-&space;center&space;}&space;c&space;=&space;\text{mean}(S)&space;\\&space;&&space;\qquad&space;\text{-&space;radius&space;}&space;r&space;=&space;\max_{x&space;\in&space;S}&space;d&space;(x,&space;c)&space;\\&space;&&space;\qquad&space;\text{children:&space;Balltree}(S_L&space;,&space;k)&space;\text{&space;and&space;Balltree}(S_R&space;,&space;k)&space;\\&space;&&space;\textbf{end&space;procedure}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;&&space;\textbf{Algorithm}&space;\text{&space;Ball-Tree&space;Pseudo-Code}&space;\\&space;&&space;\textbf{procedure&space;}&space;\text{BALLTREE}(S,&space;k)&space;\\&space;&&space;\qquad&space;\textbf{if&space;}&space;\vert&space;S&space;\vert&space;<&space;k&space;\textbf{&space;then&space;}&space;\text{stop}&space;\textbf{&space;end&space;if}&space;&&&space;\text{Return&space;leaf&space;containing&space;}&space;S&space;\\&space;&&space;\qquad&space;\text{pick&space;}&space;x_0&space;\in&space;S&space;\text{&space;uniformly&space;at&space;random}&space;\\&space;&&space;\qquad&space;\text{pick&space;}&space;x_1&space;=&space;\operatorname*{argmax}_{x&space;\in&space;S}&space;d(x_0&space;,&space;x)&space;\\&space;&&space;\qquad&space;\text{pick&space;}&space;x_2&space;=&space;\operatorname*{argmax}_{x&space;\in&space;S}&space;d(x_1&space;,&space;x)&space;\\&space;&&space;\qquad&space;\forall&space;i&space;=&space;1&space;\ldots&space;\vert&space;S&space;\vert,&space;z_i&space;=&space;(x_1&space;-&space;x_2)^{\top}&space;x_i&space;\qquad&space;\leftarrow&space;\text{project&space;data&space;onto&space;}&space;(x_1&space;-&space;x_2)&space;\\&space;&&space;\qquad&space;m&space;=&space;\text{median}(z_1&space;,&space;\ldots&space;,&space;z_{\vert&space;S&space;\vert})&space;\\&space;&&space;\qquad&space;S_L&space;=&space;\{&space;x&space;\in&space;S:&space;z_i&space;<&space;m&space;\}&space;\\&space;&&space;\qquad&space;S_R&space;=&space;\{&space;x&space;\in&space;S:&space;z_i&space;\geq&space;m&space;\}&space;\\&space;&&space;\qquad&space;\textbf{Return&space;}&space;\text{tree:}&space;\\&space;&&space;\qquad&space;\text{-&space;center&space;}&space;c&space;=&space;\text{mean}(S)&space;\\&space;&&space;\qquad&space;\text{-&space;radius&space;}&space;r&space;=&space;\max_{x&space;\in&space;S}&space;d&space;(x,&space;c)&space;\\&space;&&space;\qquad&space;\text{children:&space;Balltree}(S_L&space;,&space;k)&space;\text{&space;and&space;Balltree}(S_R&space;,&space;k)&space;\\&space;&&space;\textbf{end&space;procedure}&space;\end{align*}" title="\begin{align*} & \textbf{Algorithm} \text{ Ball-Tree Pseudo-Code} \\ & \textbf{procedure } \text{BALLTREE}(S, k) \\ & \qquad \textbf{if } \vert S \vert < k \textbf{ then } \text{stop} \textbf{ end if} && \text{Return leaf containing } S \\ & \qquad \text{pick } x_0 \in S \text{ uniformly at random} \\ & \qquad \text{pick } x_1 = \operatorname*{argmax}_{x \in S} d(x_0 , x) \\ & \qquad \text{pick } x_2 = \operatorname*{argmax}_{x \in S} d(x_1 , x) \\ & \qquad \forall i = 1 \ldots \vert S \vert, z_i = (x_1 - x_2)^{\top} x_i \qquad \leftarrow \text{project data onto } (x_1 - x_2) \\ & \qquad m = \text{median}(z_1 , \ldots , z_{\vert S \vert}) \\ & \qquad S_L = \{ x \in S: z_i < m \} \\ & \qquad S_R = \{ x \in S: z_i \geq m \} \\ & \qquad \textbf{Return } \text{tree:} \\ & \qquad \text{- center } c = \text{mean}(S) \\ & \qquad \text{- radius } r = \max_{x \in S} d (x, c) \\ & \qquad \text{children: Balltree}(S_L , k) \text{ and Balltree}(S_R , k) \\ & \textbf{end procedure} \end{align*}" /></a>

### Ball-Tree Use

Same as KD-Trees.

Slower than KD-Trees in low dimensions (<a href="https://www.codecogs.com/eqnedit.php?latex=d&space;\leq&space;3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d&space;\leq&space;3" title="d \leq 3" /></a>) but a lot faster in high dimensions. Both are affected by the curse of dimensionality, but Ball-Trees tend to still work if data exhibits local structure (e.g. lies on a low-dimensional manifold).

## Summary

- <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a>-NN is slow during testing because it does a lot of unnecessary work.
- KD-trees partition the feature space so we can rule out whole partitions that are further away than our closest <a href="https://www.codecogs.com/eqnedit.php?latex=k" target="_blank"><img src="https://latex.codecogs.com/gif.latex?k" title="k" /></a> neighbors. However, the splits are axis aligned which does not extend well to higher dimensions.
- Ball-trees partition the manifold the points are on, as opposed to the whole space. This allows it to perform much better in higher dimensions.
