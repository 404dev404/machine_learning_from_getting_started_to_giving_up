# KD Trees

## Time Complexity of k-NN

Let's look at the time complexity of k-NN. We are in a <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a>-dimensional space. To make it easier, let's assume we've already processed some number of inputs, and we want to know the time complexity of adding one more data point. When training, k-NN simply memorizes the labels of each data point it sees. This means adding one more data point is <a href="https://www.codecogs.com/eqnedit.php?latex=O(d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(d)" title="O(d)" /></a>. When testing, we need to compute the distance between our new data point and all of the data points we trained on. If <a href="https://www.codecogs.com/eqnedit.php?latex=n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n" title="n" /></a> is the number of data points we have trained on, then our time complexity for training is <a href="https://www.codecogs.com/eqnedit.php?latex=O(dn)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(dn)" title="O(dn)" /></a>. Classifying one test input is also <a href="https://www.codecogs.com/eqnedit.php?latex=O(dn)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O(dn)" title="O(dn)" /></a>. To achieve the best accuracy we can, we would like our training data set to be very large (<a href="https://www.codecogs.com/eqnedit.php?latex=n&space;\gg&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?n&space;\gg&space;0" title="n \gg 0" /></a>), but this will soon become a serious bottleneck during test time.

**Goal**: Can we make k-NN faster during testing? We can if we use clever data structures.
