# Bayes Classifier and Naive Bayes

Our training consists of the set <a href="https://www.codecogs.com/eqnedit.php?latex=D&space;=&space;\{&space;(\mathbf{x}_1,&space;y_1),&space;\ldots,&space;(\mathbf{x}_n,&space;y_n)&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D&space;=&space;\{&space;(\mathbf{x}_1,&space;y_1),&space;\ldots,&space;(\mathbf{x}_n,&space;y_n)&space;\}" title="D = \{ (\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n) \}" /></a> drawn from some unknown distribution <a href="https://www.codecogs.com/eqnedit.php?latex=P(X,&space;Y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(X,&space;Y)" title="P(X, Y)" /></a>. Because all pairs are sampled i.i.d., we obtain

<a href="https://www.codecogs.com/eqnedit.php?latex=P(D)&space;=&space;P((\mathbf{x}_1,&space;y_1),&space;\ldots,&space;(\mathbf{x}_n,&space;y_n))&space;=&space;\prod_{\alpha=1}^{n}&space;P(\mathbf{x}_{\alpha},&space;y_{\alpha})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(D)&space;=&space;P((\mathbf{x}_1,&space;y_1),&space;\ldots,&space;(\mathbf{x}_n,&space;y_n))&space;=&space;\prod_{\alpha=1}^{n}&space;P(\mathbf{x}_{\alpha},&space;y_{\alpha})" title="P(D) = P((\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)) = \prod_{\alpha=1}^{n} P(\mathbf{x}_{\alpha}, y_{\alpha})" /></a>.

If we do have enough data, we could estimate <a href="https://www.codecogs.com/eqnedit.php?latex=P(X,&space;Y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(X,&space;Y)" title="P(X, Y)" /></a> similar to the coin example, where we imagine a *gigantic* die that has one side for each possible value of <a href="https://www.codecogs.com/eqnedit.php?latex=(\mathbf{x},&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\mathbf{x},&space;y)" title="(\mathbf{x}, y)" /></a>. We can estimate the probability that one specific side comes up through counting:

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{P}(\mathbf{x},&space;y)&space;=&space;\frac{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x}&space;\wedge&space;y_i&space;=&space;y)}{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{P}(\mathbf{x},&space;y)&space;=&space;\frac{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x}&space;\wedge&space;y_i&space;=&space;y)}{n}" title="\hat{P}(\mathbf{x}, y) = \frac{\sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x} \wedge y_i = y)}{n}" /></a>,

where <a href="https://www.codecogs.com/eqnedit.php?latex=I(\mathbf{x}_i&space;=&space;\mathbf{x}&space;\wedge&space;y_i&space;=&space;y)&space;=&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?I(\mathbf{x}_i&space;=&space;\mathbf{x}&space;\wedge&space;y_i&space;=&space;y)&space;=&space;1" title="I(\mathbf{x}_i = \mathbf{x} \wedge y_i = y) = 1" /></a> if <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i&space;=&space;\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i&space;=&space;\mathbf{x}" title="\mathbf{x}_i = \mathbf{x}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=y_i&space;=&space;y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i&space;=&space;y" title="y_i = y" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0" title="0" /></a> otherwise.

Of course, if we are primarily interested in predicting the label <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a> from the feature <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}" title="\mathbf{x}" /></a>, we may estimate <a href="https://www.codecogs.com/eqnedit.php?latex=P(Y&space;\vert&space;X)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(Y&space;\vert&space;X)" title="P(Y \vert X)" /></a> directly instead of <a href="https://www.codecogs.com/eqnedit.php?latex=P(X,&space;Y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(X,&space;Y)" title="P(X, Y)" /></a>. We can then use the Bayes Optimal Classifier for a specific <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{P}(y&space;\vert&space;\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{P}(y&space;\vert&space;\mathbf{x})" title="\hat{P}(y \vert \mathbf{x})" /></a> to make predictions.

So how can we estimate <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{P}(y&space;\vert&space;\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{P}(y&space;\vert&space;\mathbf{x})" title="\hat{P}(y \vert \mathbf{x})" /></a>? Previously we have derived that <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{P}(y)&space;=&space;\frac{\sum_{i=1}^{n}&space;I(y_i&space;=&space;y)}{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{P}(y)&space;=&space;\frac{\sum_{i=1}^{n}&space;I(y_i&space;=&space;y)}{n}" title="\hat{P}(y) = \frac{\sum_{i=1}^{n} I(y_i = y)}{n}" /></a>. Similarly, <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{P}(\mathbf{x})&space;=&space;\frac{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x})}{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{P}(\mathbf{x})&space;=&space;\frac{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x})}{n}" title="\hat{P}(\mathbf{x}) = \frac{\sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x})}{n}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{P}(y,&space;\mathbf{x})&space;=&space;\frac{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x}&space;\wedge&space;y_i&space;=&space;y)}{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{P}(y,&space;\mathbf{x})&space;=&space;\frac{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x}&space;\wedge&space;y_i&space;=&space;y)}{n}" title="\hat{P}(y, \mathbf{x}) = \frac{\sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x} \wedge y_i = y)}{n}" /></a>. We can put these two together

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{P}(y&space;\vert&space;\mathbf{x})&space;=&space;\frac{\hat{P}(y,&space;\mathbf{x})}{P(\mathbf{x})}&space;=&space;\frac{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x}&space;\wedge&space;y_i&space;=&space;y)}{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x})}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{P}(y&space;\vert&space;\mathbf{x})&space;=&space;\frac{\hat{P}(y,&space;\mathbf{x})}{P(\mathbf{x})}&space;=&space;\frac{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x}&space;\wedge&space;y_i&space;=&space;y)}{\sum_{i=1}^{n}&space;I(\mathbf{x}_i&space;=&space;\mathbf{x})}" title="\hat{P}(y \vert \mathbf{x}) = \frac{\hat{P}(y, \mathbf{x})}{P(\mathbf{x})} = \frac{\sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x} \wedge y_i = y)}{\sum_{i=1}^{n} I(\mathbf{x}_i = \mathbf{x})}" /></a>

*Problem*: But there is a big problem with this method. The MLE estimate is only good if there are many training vectors with the **same identical** features as <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}" title="\mathbf{x}" /></a>! In **high dimensional spaces** (or with continuous <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}" title="\mathbf{x}" /></a>), this never happens!

# Naive Bayes

We can approach this dilemma with a simple trick, and an additional assumption. The trick part is to estimate <a href="https://www.codecogs.com/eqnedit.php?latex=P(y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y)" title="P(y)" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)" title="P(\mathbf{x} \vert y)" /></a> instead, since, by Bayes rule,

<a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x})&space;=&space;\frac{P(\mathbf{x}&space;\vert&space;y)&space;P(y)}{P(\mathbf{x})}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x})&space;=&space;\frac{P(\mathbf{x}&space;\vert&space;y)&space;P(y)}{P(\mathbf{x})}" title="P(y \vert \mathbf{x}) = \frac{P(\mathbf{x} \vert y) P(y)}{P(\mathbf{x})}" /></a>.

Recall from *Estimating Probabilities from Data* that estimating <a href="https://www.codecogs.com/eqnedit.php?latex=P(y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y)" title="P(y)" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)" title="P(\mathbf{x} \vert y)" /></a> is called *generative learning*.

Estimating <a href="https://www.codecogs.com/eqnedit.php?latex=P(y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y)" title="P(y)" /></a> is easy. For example, if <a href="https://www.codecogs.com/eqnedit.php?latex=Y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y" title="Y" /></a> takes on discrete binary values estimating <a href="https://www.codecogs.com/eqnedit.php?latex=P(Y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(Y)" title="P(Y)" /></a> reduces to coin tossing. We simply need to count how many times we observe each outcome (in this case each class):

<a href="https://www.codecogs.com/eqnedit.php?latex=P(y=c)&space;=&space;\frac{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)}{n}&space;=&space;\hat{\pi}_c" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y=c)&space;=&space;\frac{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)}{n}&space;=&space;\hat{\pi}_c" title="P(y=c) = \frac{\sum_{i=1}^{n} I(y_i = c)}{n} = \hat{\pi}_c" /></a>

Estimating <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)" title="P(\mathbf{x} \vert y)" /></a>, however, is not easy! The additional assumption that we make is the *Naive Bayes assumption*.

*Naive Bayes Assumption*:

<a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)&space;=&space;\prod_{\alpha&space;=&space;1}^{d}&space;P(x_{\alpha}&space;\vert&space;y)\text{,&space;where&space;}&space;x_{\alpha}&space;=&space;[\mathbf{x}]_{\alpha}&space;\text{&space;is&space;the&space;value&space;for&space;feature&space;}&space;\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)&space;=&space;\prod_{\alpha&space;=&space;1}^{d}&space;P(x_{\alpha}&space;\vert&space;y)\text{,&space;where&space;}&space;x_{\alpha}&space;=&space;[\mathbf{x}]_{\alpha}&space;\text{&space;is&space;the&space;value&space;for&space;feature&space;}&space;\alpha" title="P(\mathbf{x} \vert y) = \prod_{\alpha = 1}^{d} P(x_{\alpha} \vert y)\text{, where } x_{\alpha} = [\mathbf{x}]_{\alpha} \text{ is the value for feature } \alpha" /></a>

i.e., feature values are **independent given the label**! This is a very **bold** assumption.

For example, a setting where the Naive Bayes classifier is often used is spam filtering. Here, the data are emails and the labels are *spam* or *not-spam*. The Naive Bayes assumption implies that the words in an email are conditionally independent, given that you know that an email is spam or not. Clearly this is not true. Neither the words of spam or not-spam emails are drawn independently at random. However, the resulting classifiers can work well in practice even if this assumption is violated.

So, for now, let's pretend the Naive Bayes assumption holds. Then the Bayes Classifier can be defined as

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;h(\mathbf{x})&space;&&space;=&space;\operatorname*{argmax}_{y}&space;P(y&space;\vert&space;\mathbf{x})&space;\\&space;&&space;=&space;\operatorname*{argmax}_{y}&space;\frac{P(\mathbf{x}&space;\vert&space;y)&space;P(y)}{P(\mathbf{x})}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{y}&space;P(\mathbf{x}&space;\vert&space;y)&space;P(y)&space;&&&space;\text{(}&space;P(\mathbf{x})&space;\text{&space;does&space;not&space;depend&space;on&space;}&space;y&space;\text{)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{y}&space;\prod_{\alpha&space;=&space;1}^{d}&space;P(x_{\alpha}&space;\vert&space;y)&space;P(y)&space;&&&space;\text{(by&space;the&space;naive&space;Bayes&space;assumption)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{y}&space;\sum_{\alpha&space;=&space;1}^{d}&space;\log&space;(P(x_{\alpha}&space;\vert&space;y))&space;&plus;&space;\log&space;(P(y))&space;&&&space;\text{(as&space;log&space;is&space;a&space;monotonic&space;function)}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;h(\mathbf{x})&space;&&space;=&space;\operatorname*{argmax}_{y}&space;P(y&space;\vert&space;\mathbf{x})&space;\\&space;&&space;=&space;\operatorname*{argmax}_{y}&space;\frac{P(\mathbf{x}&space;\vert&space;y)&space;P(y)}{P(\mathbf{x})}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{y}&space;P(\mathbf{x}&space;\vert&space;y)&space;P(y)&space;&&&space;\text{(}&space;P(\mathbf{x})&space;\text{&space;does&space;not&space;depend&space;on&space;}&space;y&space;\text{)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{y}&space;\prod_{\alpha&space;=&space;1}^{d}&space;P(x_{\alpha}&space;\vert&space;y)&space;P(y)&space;&&&space;\text{(by&space;the&space;naive&space;Bayes&space;assumption)}&space;\\&space;&&space;=&space;\operatorname*{argmax}_{y}&space;\sum_{\alpha&space;=&space;1}^{d}&space;\log&space;(P(x_{\alpha}&space;\vert&space;y))&space;&plus;&space;\log&space;(P(y))&space;&&&space;\text{(as&space;log&space;is&space;a&space;monotonic&space;function)}&space;\end{align*}" title="\begin{align*} h(\mathbf{x}) & = \operatorname*{argmax}_{y} P(y \vert \mathbf{x}) \\ & = \operatorname*{argmax}_{y} \frac{P(\mathbf{x} \vert y) P(y)}{P(\mathbf{x})} \\ & = \operatorname*{argmax}_{y} P(\mathbf{x} \vert y) P(y) && \text{(} P(\mathbf{x}) \text{ does not depend on } y \text{)} \\ & = \operatorname*{argmax}_{y} \prod_{\alpha = 1}^{d} P(x_{\alpha} \vert y) P(y) && \text{(by the naive Bayes assumption)} \\ & = \operatorname*{argmax}_{y} \sum_{\alpha = 1}^{d} \log (P(x_{\alpha} \vert y)) + \log (P(y)) && \text{(as log is a monotonic function)} \end{align*}" /></a>

Estimating <a href="https://www.codecogs.com/eqnedit.php?latex=\log&space;(P(x_{\alpha}&space;\vert&space;y))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\log&space;(P(x_{\alpha}&space;\vert&space;y))" title="\log (P(x_{\alpha} \vert y))" /></a> is easy as we only need to consider one dimension. And estimating <a href="https://www.codecogs.com/eqnedit.php?latex=P(y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y)" title="P(y)" /></a> is not affected by the assumption.

## Estimating <a href="https://www.codecogs.com/eqnedit.php?latex=P([\mathbf{x}]_{\alpha}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P([\mathbf{x}]_{\alpha}&space;\vert&space;y)" title="P([\mathbf{x}]_{\alpha} \vert y)" /></a>

Now that we know how we can use our assumption to make the estimation of <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x})" title="P(y \vert \mathbf{x})" /></a> tractable. There are 3 notable cases in which we can use our naive Bayes classifier.

### Case #1: Categorical Features

*Features*:

<a href="https://www.codecogs.com/eqnedit.php?latex=[\mathbf{x}]_{\alpha}&space;\in&space;\{&space;f_1,&space;f_2,&space;\ldots,&space;f_{K_{\alpha}}&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?[\mathbf{x}]_{\alpha}&space;\in&space;\{&space;f_1,&space;f_2,&space;\ldots,&space;f_{K_{\alpha}}&space;\}" title="[\mathbf{x}]_{\alpha} \in \{ f_1, f_2, \ldots, f_{K_{\alpha}} \}" /></a>

Each feature <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> falls into one of <a href="https://www.codecogs.com/eqnedit.php?latex=K_{\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K_{\alpha}" title="K_{\alpha}" /></a> categories. (Note that the case with binary features is just a specific case of this, where <a href="https://www.codecogs.com/eqnedit.php?latex=K_{\alpha}&space;=&space;2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K_{\alpha}&space;=&space;2" title="K_{\alpha} = 2" /></a>.) An example of such a setting may be medical data where one feature could be *gender* (male/female) or *marital status* (single/married/widowed).

*Model <a href="https://www.codecogs.com/eqnedit.php?latex=P(x_{\alpha}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(x_{\alpha}&space;\vert&space;y)" title="P(x_{\alpha} \vert y)" /></a>*:

<a href="https://www.codecogs.com/eqnedit.php?latex=P(x_{\alpha}&space;=&space;j&space;\vert&space;y&space;=&space;c)&space;=&space;[\theta_{jc}]_{\alpha}&space;\text{&space;and&space;}&space;\sum_{j=1}^{K_{\alpha}}&space;[\theta_{jc}]_{\alpha}&space;=&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(x_{\alpha}&space;=&space;j&space;\vert&space;y&space;=&space;c)&space;=&space;[\theta_{jc}]_{\alpha}&space;\text{&space;and&space;}&space;\sum_{j=1}^{K_{\alpha}}&space;[\theta_{jc}]_{\alpha}&space;=&space;1" title="P(x_{\alpha} = j \vert y = c) = [\theta_{jc}]_{\alpha} \text{ and } \sum_{j=1}^{K_{\alpha}} [\theta_{jc}]_{\alpha} = 1" /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=[\theta_{jc}]_{\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?[\theta_{jc}]_{\alpha}" title="[\theta_{jc}]_{\alpha}" /></a> is the probability of feature <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> having the value <a href="https://www.codecogs.com/eqnedit.php?latex=j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?j" title="j" /></a>, given that the label is <a href="https://www.codecogs.com/eqnedit.php?latex=c" target="_blank"><img src="https://latex.codecogs.com/gif.latex?c" title="c" /></a>. And the constraint indicates that <a href="https://www.codecogs.com/eqnedit.php?latex=x_{\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_{\alpha}" title="x_{\alpha}" /></a> must have one of the categories <a href="https://www.codecogs.com/eqnedit.php?latex=\{&space;1,&space;\ldots,&space;K_{\alpha}&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\{&space;1,&space;\ldots,&space;K_{\alpha}&space;\}" title="\{ 1, \ldots, K_{\alpha} \}" /></a>.

*Parameter Estimation*:

<a href="https://www.codecogs.com/eqnedit.php?latex=[\hat{\theta}_{jc}]_{\alpha}&space;=&space;\frac{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)&space;I(x_{i&space;\alpha}&space;=&space;j)&space;&plus;&space;l}{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)&space;&plus;&space;l&space;K_\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?[\hat{\theta}_{jc}]_{\alpha}&space;=&space;\frac{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)&space;I(x_{i&space;\alpha}&space;=&space;j)&space;&plus;&space;l}{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)&space;&plus;&space;l&space;K_\alpha}" title="[\hat{\theta}_{jc}]_{\alpha} = \frac{\sum_{i=1}^{n} I(y_i = c) I(x_{i \alpha} = j) + l}{\sum_{i=1}^{n} I(y_i = c) + l K_\alpha}" /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=x_{i&space;\alpha}&space;=&space;[\mathbf{x}_{i}]_{\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_{i&space;\alpha}&space;=&space;[\mathbf{x}_{i}]_{\alpha}" title="x_{i \alpha} = [\mathbf{x}_{i}]_{\alpha}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=l" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l" title="l" /></a> is a smoothing parameter. By setting <a href="https://www.codecogs.com/eqnedit.php?latex=l&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l&space;=&space;0" title="l = 0" /></a> we get an MLE estimator, <a href="https://www.codecogs.com/eqnedit.php?latex=l&space;>&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l&space;>&space;0" title="l > 0" /></a> leads to MAP. If we set <a href="https://www.codecogs.com/eqnedit.php?latex=l&space;=&space;&plus;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l&space;=&space;&plus;1" title="l = +1" /></a> we get *Laplace smoothing*.

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\text{number&space;of&space;samples&space;with&space;label&space;}&space;c&space;\text{&space;that&space;have&space;feature&space;}&space;\alpha&space;\text{&space;with&space;value&space;}&space;j}{\text{number&space;of&space;samples&space;with&space;label&space;}&space;c}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\text{number&space;of&space;samples&space;with&space;label&space;}&space;c&space;\text{&space;that&space;have&space;feature&space;}&space;\alpha&space;\text{&space;with&space;value&space;}&space;j}{\text{number&space;of&space;samples&space;with&space;label&space;}&space;c}" title="\frac{\text{number of samples with label } c \text{ that have feature } \alpha \text{ with value } j}{\text{number of samples with label } c}" /></a>.

Essentially the categorical feature model associate a special coin with each feature and label. The generative model that we are assuming is that the data was generated by first choosing the label (e.g. "healthy person"). That label comes with a set of <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> "dice", for each dimension one. The generator picks each die, tosses it and fills in the feature value with the outcome of the coin toss. So if there are <a href="https://www.codecogs.com/eqnedit.php?latex=C" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C" title="C" /></a> possible labels and <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> dimensions we are estimating <a href="https://www.codecogs.com/eqnedit.php?latex=d&space;\times&space;C" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d&space;\times&space;C" title="d \times C" /></a> "dice" from the data. However, per data point only <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> dice are tossed (one for each dimension). Die <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> (for any label) has <a href="https://www.codecogs.com/eqnedit.php?latex=K_\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?K_\alpha" title="K_\alpha" /></a> possible "sides". Of course this is not how the data is generated in reality -- but it is a modeling assumption that we make. We then learn these models from the data and during test time see which model is more likely given the sample.

*Prediction*:

<a href="https://www.codecogs.com/eqnedit.php?latex=\operatorname*{argmax}_{y}&space;P(y=c&space;\vert&space;\mathbf{x})&space;\propto&space;\operatorname*{argmax}_{y}&space;\hat{\pi}_c&space;\prod_{\alpha&space;=&space;1}^{d}&space;[\hat{\theta}_{jc}]_{\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\operatorname*{argmax}_{y}&space;P(y=c&space;\vert&space;\mathbf{x})&space;\propto&space;\operatorname*{argmax}_{y}&space;\hat{\pi}_c&space;\prod_{\alpha&space;=&space;1}^{d}&space;[\hat{\theta}_{jc}]_{\alpha}" title="\operatorname*{argmax}_{y} P(y=c \vert \mathbf{x}) \propto \operatorname*{argmax}_{y} \hat{\pi}_c \prod_{\alpha = 1}^{d} [\hat{\theta}_{jc}]_{\alpha}" /></a>

### Case #2: Multinomial Features

If feature values don't represent categories (e.g. male/female) but counts we need to use a different model. E.g. in the text document categorization, feature value <a href="https://www.codecogs.com/eqnedit.php?latex=x_\alpha&space;=&space;j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_\alpha&space;=&space;j" title="x_\alpha = j" /></a> means that in this particular document <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}" title="\mathbf{x}" /></a> the <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha^{th}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha^{th}" title="\alpha^{th}" /></a> word in my dictionary appears <a href="https://www.codecogs.com/eqnedit.php?latex=j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?j" title="j" /></a> times. Let us consider the example of spam filtering. Imagine the <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha^{th}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha^{th}" title="\alpha^{th}" /></a> word is indicative towards "spam". Then if <a href="https://www.codecogs.com/eqnedit.php?latex=x_\alpha&space;=&space;10" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_\alpha&space;=&space;10" title="x_\alpha = 10" /></a> means that this email is likely spam (as word <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> appears <a href="https://www.codecogs.com/eqnedit.php?latex=10" target="_blank"><img src="https://latex.codecogs.com/gif.latex?10" title="10" /></a> times in it). And another email with <a href="https://www.codecogs.com/eqnedit.php?latex=x_\alpha'&space;=&space;20" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_\alpha'&space;=&space;20" title="x_\alpha' = 20" /></a> should be even more likely to be spam (as the spammy word appears twice as often). With categorical features this is not guaranteed. It could be that the training set does not contain any email that contain word <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> exactly <a href="https://www.codecogs.com/eqnedit.php?latex=20" target="_blank"><img src="https://latex.codecogs.com/gif.latex?20" title="20" /></a> times. In this case you would simply get the hallucinated smoothing values for both spam and not-spam -- and the signal is lost. We need a model that incorporates our knowledge that features are counts -- this will help us during estimation (you don't have to see a training email with exactly the same number of word occurances) and during inference/testing (as you will obtain these monotonicities that one might expect). The multinomial distribution does exactly that.

*Features*:

<a href="https://www.codecogs.com/eqnedit.php?latex=x_\alpha&space;\in&space;\{&space;0,&space;1,&space;2,&space;\ldots,&space;m&space;\}&space;\text{&space;and&space;}&space;m&space;=&space;\sum_{\alpha&space;=&space;1}^{d}&space;x_\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_\alpha&space;\in&space;\{&space;0,&space;1,&space;2,&space;\ldots,&space;m&space;\}&space;\text{&space;and&space;}&space;m&space;=&space;\sum_{\alpha&space;=&space;1}^{d}&space;x_\alpha" title="x_\alpha \in \{ 0, 1, 2, \ldots, m \} \text{ and } m = \sum_{\alpha = 1}^{d} x_\alpha" /></a>

Each feature <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> represents a count and <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> is the length of the sequence. An example of this could be the count of a specific word <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> in a document of length <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> is the size of the vocabulary.

*Model* <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)" title="P(\mathbf{x} \vert y)" /></a>: Use the multinomial distribution

<a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;m,&space;y&space;=&space;c)&space;=&space;\frac{m&space;!}{x_1&space;!&space;\cdot&space;x_2&space;!&space;\cdot&space;\ldots&space;\cdot&space;x_d&space;!}&space;\prod_{\alpha&space;=&space;1}^{d}&space;(\theta_{ac})^{x_\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;m,&space;y&space;=&space;c)&space;=&space;\frac{m&space;!}{x_1&space;!&space;\cdot&space;x_2&space;!&space;\cdot&space;\ldots&space;\cdot&space;x_d&space;!}&space;\prod_{\alpha&space;=&space;1}^{d}&space;(\theta_{ac})^{x_\alpha}" title="P(\mathbf{x} \vert m, y = c) = \frac{m !}{x_1 ! \cdot x_2 ! \cdot \ldots \cdot x_d !} \prod_{\alpha = 1}^{d} (\theta_{ac})^{x_\alpha}" /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=\theta_{ac}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_{ac}" title="\theta_{ac}" /></a> is the probability of selecting <a href="https://www.codecogs.com/eqnedit.php?latex=x_\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_\alpha" title="x_\alpha" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{\alpha&space;=&space;1}^{d}&space;\theta_{ac}&space;=&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{\alpha&space;=&space;1}^{d}&space;\theta_{ac}&space;=&space;1" title="\sum_{\alpha = 1}^{d} \theta_{ac} = 1" /></a>. So, we can use this to generate a spam email, i.e., a document <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}" title="\mathbf{x}" /></a> of class <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;\text{spam}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;\text{spam}" title="y = \text{spam}" /></a> by picking <a href="https://www.codecogs.com/eqnedit.php?latex=m" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m" title="m" /></a> words independently at random from the vocabulary of <a href="https://www.codecogs.com/eqnedit.php?latex=d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d" title="d" /></a> words using <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y&space;=&space;\text{spam})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y&space;=&space;\text{spam})" title="P(\mathbf{x} \vert y = \text{spam})" /></a>.

*Parameter Estimation*:

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{\theta}_{ac}&space;=&space;\frac{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)&space;x_{i\alpha}&space;&plus;&space;l}{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)&space;m_i&space;&plus;&space;l&space;\cdot&space;d}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{\theta}_{ac}&space;=&space;\frac{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)&space;x_{i\alpha}&space;&plus;&space;l}{\sum_{i=1}^{n}&space;I(y_i&space;=&space;c)&space;m_i&space;&plus;&space;l&space;\cdot&space;d}" title="\hat{\theta}_{ac} = \frac{\sum_{i=1}^{n} I(y_i = c) x_{i\alpha} + l}{\sum_{i=1}^{n} I(y_i = c) m_i + l \cdot d}" /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=m_i&space;=&space;\sum_{\beta&space;=&space;1}^{d}&space;x_{i\beta}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m_i&space;=&space;\sum_{\beta&space;=&space;1}^{d}&space;x_{i\beta}" title="m_i = \sum_{\beta = 1}^{d} x_{i\beta}" /></a> denotes the number of words in document <a href="https://www.codecogs.com/eqnedit.php?latex=i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i" title="i" /></a>. The numerator sums up all counts for feature <a href="https://www.codecogs.com/eqnedit.php?latex=x_\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_\alpha" title="x_\alpha" /></a> and the denominator sums up all counts of all features across all data points. E.g.,

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\text{number&space;of&space;times&space;word&space;}\alpha&space;\text{&space;appears&space;in&space;all&space;spam&space;emails}}{\text{number&space;of&space;words&space;in&space;all&space;spam&space;emails&space;combined}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\text{number&space;of&space;times&space;word&space;}\alpha&space;\text{&space;appears&space;in&space;all&space;spam&space;emails}}{\text{number&space;of&space;words&space;in&space;all&space;spam&space;emails&space;combined}}" title="\frac{\text{number of times word }\alpha \text{ appears in all spam emails}}{\text{number of words in all spam emails combined}}" /></a>

Again, <a href="https://www.codecogs.com/eqnedit.php?latex=l" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l" title="l" /></a> is the smoothing parameter.

*Prediction*:

<a href="https://www.codecogs.com/eqnedit.php?latex=\operatorname*{argmax}_{c}&space;P(y=c&space;\vert&space;\mathbf{x})&space;\propto&space;\operatorname*{argmax}_{c}&space;\hat{\pi_c}&space;\prod_{\alpha&space;=&space;1}^{d}&space;\hat{\theta}_{\alpha&space;c}^{x_\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\operatorname*{argmax}_{c}&space;P(y=c&space;\vert&space;\mathbf{x})&space;\propto&space;\operatorname*{argmax}_{c}&space;\hat{\pi_c}&space;\prod_{\alpha&space;=&space;1}^{d}&space;\hat{\theta}_{\alpha&space;c}^{x_\alpha}" title="\operatorname*{argmax}_{c} P(y=c \vert \mathbf{x}) \propto \operatorname*{argmax}_{c} \hat{\pi_c} \prod_{\alpha = 1}^{d} \hat{\theta}_{\alpha c}^{x_\alpha}" /></a>

### Case #3: Continuous Features (Gaussian Naive Bayes)

*Features*:

<a href="https://www.codecogs.com/eqnedit.php?latex=x_\alpha&space;\in&space;\mathbb{R}&space;\quad&space;\text{(each&space;feature&space;takes&space;on&space;a&space;real&space;value)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_\alpha&space;\in&space;\mathbb{R}&space;\quad&space;\text{(each&space;feature&space;takes&space;on&space;a&space;real&space;value)}" title="x_\alpha \in \mathbb{R} \quad \text{(each feature takes on a real value)}" /></a>

*Model* <a href="https://www.codecogs.com/eqnedit.php?latex=P(x_\alpha&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(x_\alpha&space;\vert&space;y)" title="P(x_\alpha \vert y)" /></a>: Use Gaussian distribution

<a href="https://www.codecogs.com/eqnedit.php?latex=P(x_\alpha&space;\vert&space;y&space;=&space;c)&space;=&space;\mathcal{N}(\mu_{\alpha&space;c},&space;\sigma^2_{\alpha&space;c})&space;=&space;\frac{1}{\sqrt{2&space;\pi}&space;\sigma_{\alpha&space;c}}&space;e^{-\frac{1}{2}(\frac{x_{\alpha}&space;-&space;\mu_{\alpha&space;c}}{\sigma_{\alpha&space;c}})^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(x_\alpha&space;\vert&space;y&space;=&space;c)&space;=&space;\mathcal{N}(\mu_{\alpha&space;c},&space;\sigma^2_{\alpha&space;c})&space;=&space;\frac{1}{\sqrt{2&space;\pi}&space;\sigma_{\alpha&space;c}}&space;e^{-\frac{1}{2}(\frac{x_{\alpha}&space;-&space;\mu_{\alpha&space;c}}{\sigma_{\alpha&space;c}})^2}" title="P(x_\alpha \vert y = c) = \mathcal{N}(\mu_{\alpha c}, \sigma^2_{\alpha c}) = \frac{1}{\sqrt{2 \pi} \sigma_{\alpha c}} e^{-\frac{1}{2}(\frac{x_{\alpha} - \mu_{\alpha c}}{\sigma_{\alpha c}})^2}" /></a>

Note that the model specified above is based on our assumption about the data -- that each feature <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> comes from a class-conditional Gaussian distribution. The full distribution <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)&space;\sim&space;\mathcal{N}(\mu_y,&space;\Sigma_y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)&space;\sim&space;\mathcal{N}(\mu_y,&space;\Sigma_y)" title="P(\mathbf{x} \vert y) \sim \mathcal{N}(\mu_y, \Sigma_y)" /></a>, where <a href="https://www.codecogs.com/eqnedit.php?latex=\Sigma_y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Sigma_y" title="\Sigma_y" /></a> is a diagonal covariance matrix with <a href="https://www.codecogs.com/eqnedit.php?latex=[\Sigma_y]_{\alpha,&space;\alpha}&space;=&space;\sigma_{\alpha,y}^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?[\Sigma_y]_{\alpha,&space;\alpha}&space;=&space;\sigma_{\alpha,y}^2" title="[\Sigma_y]_{\alpha, \alpha} = \sigma_{\alpha,y}^2" /></a>.

*Parameter Estimation*: As always, we estimate the parameters of the distributions for each dimension and class independently. Gaussian distributions only have two parameters, the mean and variance. The mean <a href="https://www.codecogs.com/eqnedit.php?latex=\mu_{\alpha,&space;y}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mu_{\alpha,&space;y}" title="\mu_{\alpha, y}" /></a> is estimated by the average feature value of dimension <a href="https://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> from all samples with label <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a>. The (squared) standard deviation is simply the variance of this estimate.

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\mu_{\alpha&space;c}&space;&&space;\leftarrow&space;\frac{1}{n_c}&space;\sum_{i=1}^{n}&space;I&space;(y_i&space;=&space;c)&space;x_{i&space;\alpha}&space;&&&space;\text{where&space;}&space;n_c&space;=&space;\sum_{i=1}^{n}&space;I&space;(y_i&space;=&space;c)&space;\\&space;\sigma_{\alpha&space;c}^2&space;&&space;\leftarrow&space;\frac{1}{n_c}&space;\sum_{i=1}^{n}&space;I&space;(y_i&space;=&space;c)&space;(x_{i&space;\alpha}&space;-&space;\mu_{\alpha&space;c})^2&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;\mu_{\alpha&space;c}&space;&&space;\leftarrow&space;\frac{1}{n_c}&space;\sum_{i=1}^{n}&space;I&space;(y_i&space;=&space;c)&space;x_{i&space;\alpha}&space;&&&space;\text{where&space;}&space;n_c&space;=&space;\sum_{i=1}^{n}&space;I&space;(y_i&space;=&space;c)&space;\\&space;\sigma_{\alpha&space;c}^2&space;&&space;\leftarrow&space;\frac{1}{n_c}&space;\sum_{i=1}^{n}&space;I&space;(y_i&space;=&space;c)&space;(x_{i&space;\alpha}&space;-&space;\mu_{\alpha&space;c})^2&space;\end{align*}" title="\begin{align*} \mu_{\alpha c} & \leftarrow \frac{1}{n_c} \sum_{i=1}^{n} I (y_i = c) x_{i \alpha} && \text{where } n_c = \sum_{i=1}^{n} I (y_i = c) \\ \sigma_{\alpha c}^2 & \leftarrow \frac{1}{n_c} \sum_{i=1}^{n} I (y_i = c) (x_{i \alpha} - \mu_{\alpha c})^2 \end{align*}" /></a>

## Naive Bayes is a Linear Classifier

1. Suppose that <a href="https://www.codecogs.com/eqnedit.php?latex=y_i&space;\in&space;\{&space;-1,&space;&plus;1&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i&space;\in&space;\{&space;-1,&space;&plus;1&space;\}" title="y_i \in \{ -1, +1 \}" /></a> and features are *multinomial*

We can show that

<a href="https://www.codecogs.com/eqnedit.php?latex=h(\mathbf{x})&space;=&space;\operatorname*{argmax}_{y}&space;P(y)&space;\prod_{\alpha&space;-&space;1}^{d}&space;P(x_\alpha&space;\vert&space;y)&space;=&space;\text{sign}(\mathbf{w}^\top&space;\mathbf{x}&space;&plus;&space;b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\mathbf{x})&space;=&space;\operatorname*{argmax}_{y}&space;P(y)&space;\prod_{\alpha&space;-&space;1}^{d}&space;P(x_\alpha&space;\vert&space;y)&space;=&space;\text{sign}(\mathbf{w}^\top&space;\mathbf{x}&space;&plus;&space;b)" title="h(\mathbf{x}) = \operatorname*{argmax}_{y} P(y) \prod_{\alpha - 1}^{d} P(x_\alpha \vert y) = \text{sign}(\mathbf{w}^\top \mathbf{x} + b)" /></a>

That is,

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}^{\top}\mathbf{x}&space;&plus;&space;b&space;>&space;0&space;\iff&space;h(\mathbf{x})&space;=&space;&plus;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}^{\top}\mathbf{x}&space;&plus;&space;b&space;>&space;0&space;\iff&space;h(\mathbf{x})&space;=&space;&plus;1" title="\mathbf{w}^{\top}\mathbf{x} + b > 0 \iff h(\mathbf{x}) = +1" /></a>

As before, we define <a href="https://www.codecogs.com/eqnedit.php?latex=P(x_\alpha&space;\vert&space;y&space;=&space;&plus;1)&space;\propto&space;\theta_{\alpha^{&plus;}}^{x_\alpha}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(x_\alpha&space;\vert&space;y&space;=&space;&plus;1)&space;\propto&space;\theta_{\alpha^{&plus;}}^{x_\alpha}" title="P(x_\alpha \vert y = +1) \propto \theta_{\alpha^{+}}^{x_\alpha}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=P(Y=&plus;1)&space;=&space;\pi_{&plus;}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(Y=&plus;1)&space;=&space;\pi_{&plus;}" title="P(Y=+1) = \pi_{+}" /></a>:

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;[\mathbf{w}]_\alpha&space;&&space;=&space;\log&space;(\theta_{\alpha^{&plus;}})&space;-&space;\log&space;(\theta_{\alpha^{-}})&space;\\&space;b&space;&&space;=&space;\log&space;(\pi^{&plus;})&space;-&space;\log&space;(\pi^{-})&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;[\mathbf{w}]_\alpha&space;&&space;=&space;\log&space;(\theta_{\alpha^{&plus;}})&space;-&space;\log&space;(\theta_{\alpha^{-}})&space;\\&space;b&space;&&space;=&space;\log&space;(\pi^{&plus;})&space;-&space;\log&space;(\pi^{-})&space;\end{align*}" title="\begin{align*} [\mathbf{w}]_\alpha & = \log (\theta_{\alpha^{+}}) - \log (\theta_{\alpha^{-}}) \\ b & = \log (\pi^{+}) - \log (\pi^{-}) \end{align*}" /></a>

If we use the above to do classification, we can compute for <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}^{\top}\cdot&space;\mathbf{x}&space;&plus;&space;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}^{\top}\cdot&space;\mathbf{x}&space;&plus;&space;b" title="\mathbf{w}^{\top}\cdot \mathbf{x} + b" /></a>

Simplying this further leads to

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\mathbf{w}^{\top}&space;\mathbf{x}&space;&plus;&space;b&space;>&space;0&space;&&space;\iff&space;\sum_{\alpha&space;=&space;1}^{d}&space;[\mathbf{x}]_{\alpha}&space;\overbrace{(\log&space;(\theta_{\alpha^{&plus;}})&space;-&space;\log&space;(\theta_{\alpha^{-}}))}^{[\mathbf{w}]_{\alpha}}&space;&plus;&space;\overbrace{\log&space;(\pi^{&plus;})&space;-&space;\log&space;(\pi^{-})}^{b}&space;>&space;0&space;&&&space;\text{(Plugging&space;in&space;definition&space;of&space;}&space;\mathbf{w},&space;b&space;\text{.)}&space;\\&space;&&space;\iff&space;\exp{\Big(&space;\sum_{\alpha=1}^{d}&space;[\mathbf{x}]_{\alpha}&space;(\log&space;(\theta_{\alpha^{&plus;}})&space;-&space;\log&space;(\theta_{\alpha^{-}}))&space;&plus;&space;\log&space;(\pi^{&plus;})&space;-&space;\log&space;(\pi^{-})&space;\Big)}&space;>&space;1&space;&&&space;\text{(exponentiating&space;both&space;sides)}&space;\\&space;&&space;\iff&space;\prod_{\alpha&space;=&space;1}^{d}&space;\frac{\exp{\big(&space;\log&space;\theta_{\alpha^{&plus;}}^{[\mathbf{x}]_{\alpha}}&space;&plus;&space;\log&space;(\pi^{&plus;})&space;\big)}}{\exp{\big(&space;\log&space;\theta_{\alpha^{-}}^{[\mathbf{x}]_{\alpha}}&space;&plus;&space;\log&space;(\pi^{-})&space;\big)}}&space;&&&space;\text{Because&space;}&space;a&space;\log&space;(b)&space;=&space;\log&space;(b^a)&space;\text{&space;and&space;}&space;\exp{(a-b)}&space;=&space;\frac{e^a}{e^b}&space;\text{&space;operations}&space;\end{align*}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;\mathbf{w}^{\top}&space;\mathbf{x}&space;&plus;&space;b&space;>&space;0&space;&&space;\iff&space;\sum_{\alpha&space;=&space;1}^{d}&space;[\mathbf{x}]_{\alpha}&space;\overbrace{(\log&space;(\theta_{\alpha^{&plus;}})&space;-&space;\log&space;(\theta_{\alpha^{-}}))}^{[\mathbf{w}]_{\alpha}}&space;&plus;&space;\overbrace{\log&space;(\pi^{&plus;})&space;-&space;\log&space;(\pi^{-})}^{b}&space;>&space;0&space;&&&space;\text{(Plugging&space;in&space;definition&space;of&space;}&space;\mathbf{w},&space;b&space;\text{.)}&space;\\&space;&&space;\iff&space;\exp{\Big(&space;\sum_{\alpha=1}^{d}&space;[\mathbf{x}]_{\alpha}&space;(\log&space;(\theta_{\alpha^{&plus;}})&space;-&space;\log&space;(\theta_{\alpha^{-}}))&space;&plus;&space;\log&space;(\pi^{&plus;})&space;-&space;\log&space;(\pi^{-})&space;\Big)}&space;>&space;1&space;&&&space;\text{(exponentiating&space;both&space;sides)}&space;\\&space;&&space;\iff&space;\prod_{\alpha&space;=&space;1}^{d}&space;\frac{\exp{\big(&space;\log&space;\theta_{\alpha^{&plus;}}^{[\mathbf{x}]_{\alpha}}&space;&plus;&space;\log&space;(\pi^{&plus;})&space;\big)}}{\exp{\big(&space;\log&space;\theta_{\alpha^{-}}^{[\mathbf{x}]_{\alpha}}&space;&plus;&space;\log&space;(\pi^{-})&space;\big)}}&space;&&&space;\text{Because&space;}&space;a&space;\log&space;(b)&space;=&space;\log&space;(b^a)&space;\text{&space;and&space;}&space;\exp{(a-b)}&space;=&space;\frac{e^a}{e^b}&space;\text{&space;operations}&space;\end{align*}" title="\begin{align*} \mathbf{w}^{\top} \mathbf{x} + b > 0 & \iff \sum_{\alpha = 1}^{d} [\mathbf{x}]_{\alpha} \overbrace{(\log (\theta_{\alpha^{+}}) - \log (\theta_{\alpha^{-}}))}^{[\mathbf{w}]_{\alpha}} + \overbrace{\log (\pi^{+}) - \log (\pi^{-})}^{b} > 0 && \text{(Plugging in definition of } \mathbf{w}, b \text{.)} \\ & \iff \exp{\Big( \sum_{\alpha=1}^{d} [\mathbf{x}]_{\alpha} (\log (\theta_{\alpha^{+}}) - \log (\theta_{\alpha^{-}})) + \log (\pi^{+}) - \log (\pi^{-}) \Big)} > 1 && \text{(exponentiating both sides)} \\ & \iff \prod_{\alpha = 1}^{d} \frac{\exp{\big( \log \theta_{\alpha^{+}}^{[\mathbf{x}]_{\alpha}} + \log (\pi^{+}) \big)}}{\exp{\big( \log \theta_{\alpha^{-}}^{[\mathbf{x}]_{\alpha}} + \log (\pi^{-}) \big)}} && \text{Because } a \log (b) = \log (b^a) \text{ and } \exp{(a-b)} = \frac{e^a}{e^b} \text{ operations} \end{align*}" /></a>




































































