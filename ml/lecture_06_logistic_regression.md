# Logistic Regression

Machine learning algorithms can be (roughly) categorized into two categories:

- **Generative** algorithms, that estimate <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i,&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i,&space;y)" title="P(\mathbf{x}_i, y)" /></a> (often they model <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i&space;\vert&space;y)" title="P(\mathbf{x}_i \vert y)" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=P(y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y)" title="P(y)" /></a> separately).
- **Discriminative** algorithms, that model <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x}_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x}_i)" title="P(y \vert \mathbf{x}_i)" /></a>

The Naive Bayes algorithm is **generative**. It models <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i&space;\vert&space;y)" title="P(\mathbf{x}_i \vert y)" /></a> and makes explicit assumptions on its distribution (e.g. multinomial, categorical, Gaussian, ...). The parameters of this distributions are estimated with MLE or MAP. We showed previously that for the Gaussian Naive Bayes <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" title="P(y \vert \mathbf{x}_i)=\frac{1}{1+e^{-y(\mathbf{w}^T \mathbf{x}_i + b)}}" /></a> for <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;\in&space;\{&space;&plus;1,&space;-1&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;\in&space;\{&space;&plus;1,&space;-1&space;\}" title="y \in \{ +1, -1 \}" /></a> for specific vectors <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b" title="b" /></a> that are uniquely determined through the particular choice of <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i&space;\vert&space;y)" title="P(\mathbf{x}_i \vert y)" /></a>.

Logistic Regression is often referred to as the **discriminative** counterpart of Naive Bayes. Here, we model <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x}_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x}_i)" title="P(y \vert \mathbf{x}_i)" /></a> and assume that it takes on exactly this form

<a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" title="P(y \vert \mathbf{x}_i)=\frac{1}{1+e^{-y(\mathbf{w}^T \mathbf{x}_i + b)}}" /></a>.

We make little assumptions on <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}_i&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}_i&space;\vert&space;y)" title="P(\mathbf{x}_i \vert y)" /></a>, e.g. it could be Gaussian or Multinomial. Ultimately it doesn't matter, because we estimate the vector <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b" title="b" /></a> directly with MLE or MAP to maximize the conditional likelihood of <a href="https://www.codecogs.com/eqnedit.php?latex=\prod_i&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;;&space;\mathbf{w},&space;b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\prod_i&space;P(y_i&space;\vert&space;\mathbf{x}_i&space;;&space;\mathbf{w},&space;b)" title="\prod_i P(y_i \vert \mathbf{x}_i ; \mathbf{w}, b)" /></a>.

We can absorbed the parameter <a href="https://www.codecogs.com/eqnedit.php?latex=b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b" title="b" /></a> into <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> through an additional constant dimension.

## Maximum Likelihood Estimate (MLE)

In MLE we choose parrameters that **maximize the conditional likelihood**. The conditional data likelihood <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})" title="P(\mathbf{y} \vert X, \mathbf{w})" /></a> is the probability of the observed value <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{y}&space;\in&space;\mathbb{R}^{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{y}&space;\in&space;\mathbb{R}^{n}" title="\mathbf{y} \in \mathbb{R}^{n}" /></a> in the training data conditioned on the feature value <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a>. Note that <a href="https://www.codecogs.com/eqnedit.php?latex=X&space;=&space;[\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_i,&space;\ldots,&space;\mathbf{x}_n]&space;\in&space;\mathbb{R}^{d&space;\times&space;n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X&space;=&space;[\mathbf{x}_1,&space;\ldots,&space;\mathbf{x}_i,&space;\ldots,&space;\mathbf{x}_n]&space;\in&space;\mathbb{R}^{d&space;\times&space;n}" title="X = [\mathbf{x}_1, \ldots, \mathbf{x}_i, \ldots, \mathbf{x}_n] \in \mathbb{R}^{d \times n}" /></a>. We choose the parameters that maximize this function and we assume that the <a href="https://www.codecogs.com/eqnedit.php?latex=y_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i" title="y_i" /></a>'s are independent given the input features <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{x}_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{x}_i" title="\mathbf{x}_i" /></a> and <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>. So,

<a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})&space;=&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i,&space;\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})&space;=&space;\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i,&space;\mathbf{w})" title="P(\mathbf{y} \vert X, \mathbf{w}) = \prod_{i=1}^{n} P(y_i \vert \mathbf{x}_i, \mathbf{w})" /></a>.

Now if we take the log, we obtain

<a href="https://www.codecogs.com/eqnedit.php?latex=\log&space;\Big(\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i,&space;\mathbf{w})\Big)=-\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\log&space;\Big(\prod_{i=1}^{n}&space;P(y_i&space;\vert&space;\mathbf{x}_i,&space;\mathbf{w})\Big)=-\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" title="\log \Big(\prod_{i=1}^{n} P(y_i \vert \mathbf{x}_i, \mathbf{w})\Big)=-\sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i})" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{\mathbf{w}}_{MLE}=\operatorname*{argmax}_{\mathbf{w}}&space;\Big(-&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})\Big)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{\mathbf{w}}_{MLE}=\operatorname*{argmax}_{\mathbf{w}}&space;\Big(-&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})\Big)" title="\hat{\mathbf{w}}_{MLE}=\operatorname*{argmax}_{\mathbf{w}} \Big(- \sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i})\Big)" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\operatorname*{argmin}_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\operatorname*{argmin}_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" title="\operatorname*{argmin}_{\mathbf{w}} \sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i})" /></a>

We need to estimate the parameter <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>. To find the values of the parameters at minimum, we can try to find solutions for <a href="https://www.codecogs.com/eqnedit.php?latex=\nabla_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\nabla_{\mathbf{w}}&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})&space;=&space;0" title="\nabla_{\mathbf{w}} \sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i}) = 0" /></a>. This equation has no closed form solution, so we use **Gradient Descent** on the **negative log likelihood** <a href="https://www.codecogs.com/eqnedit.php?latex=l(\mathbf{w})=\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\mathbf{w})=\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})" title="l(\mathbf{w})=\sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i})" /></a>.

# Maximum a Posteriori (MAP) Estimate

In the MAP estimate we treat <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a> as a random variable and can specify a prior belief distribution over it. We may use: <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}&space;\sim&space;\mathcal{N}(\mathbf{0},\sigma^2&space;I)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}&space;\sim&space;\mathcal{N}(\mathbf{0},\sigma^2&space;I)" title="\mathbf{w} \sim \mathcal{N}(\mathbf{0},\sigma^2 I)" /></a>. This is the Gaussian approximation for LR.

Our goal in MAP is to find the *most likely* model parameters *given the data*, i.e., the parameters that **maximize the posterior**.

<a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{w}&space;\vert&space;D)&space;=&space;P(\mathbf{w}&space;\vert&space;X,&space;\mathbf{y})&space;\propto&space;P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})&space;P(\mathbf{w})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{w}&space;\vert&space;D)&space;=&space;P(\mathbf{w}&space;\vert&space;X,&space;\mathbf{y})&space;\propto&space;P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})&space;P(\mathbf{w})" title="P(\mathbf{w} \vert D) = P(\mathbf{w} \vert X, \mathbf{y}) \propto P(\mathbf{y} \vert X, \mathbf{w}) P(\mathbf{w})" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{\mathbf{w}}_{MAP}&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\log&space;\Big(&space;P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})&space;P(\mathbf{w})&space;\Big)&space;\\&space;=&space;\operatorname*{argmin}_{\mathbf{w}}&space;\Big(&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})&plus;\lambda&space;\mathbf{w}^T&space;\mathbf{w}&space;\Big)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{\mathbf{w}}_{MAP}&space;=&space;\operatorname*{argmax}_{\mathbf{w}}&space;\log&space;\Big(&space;P(\mathbf{y}&space;\vert&space;X,&space;\mathbf{w})&space;P(\mathbf{w})&space;\Big)&space;\\&space;=&space;\operatorname*{argmin}_{\mathbf{w}}&space;\Big(&space;\sum_{i=1}^{n}&space;\log&space;(1&plus;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})&plus;\lambda&space;\mathbf{w}^T&space;\mathbf{w}&space;\Big)" title="\hat{\mathbf{w}}_{MAP} = \operatorname*{argmax}_{\mathbf{w}} \log \Big( P(\mathbf{y} \vert X, \mathbf{w}) P(\mathbf{w}) \Big) \\ = \operatorname*{argmin}_{\mathbf{w}} \Big( \sum_{i=1}^{n} \log (1+e^{-y_i \mathbf{w}^T \mathbf{x}_i})+\lambda \mathbf{w}^T \mathbf{w} \Big)" /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=\lambda&space;=&space;\frac{1}{2&space;\sigma^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda&space;=&space;\frac{1}{2&space;\sigma^2}" title="\lambda = \frac{1}{2 \sigma^2}" /></a>. Once again, this function has no closed form solution, but we can use **Gradient Descent** on the *negative log posterior* <a href="https://www.codecogs.com/eqnedit.php?latex=l(\mathbf{w})=\sum_{i=1}^{n}&space;\log&space;(1&space;&plus;&space;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})&space;&plus;&space;\lambda&space;\mathbf{w}^T&space;\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(\mathbf{w})=\sum_{i=1}^{n}&space;\log&space;(1&space;&plus;&space;e^{-y_i&space;\mathbf{w}^T&space;\mathbf{x}_i})&space;&plus;&space;\lambda&space;\mathbf{w}^T&space;\mathbf{w}" title="l(\mathbf{w})=\sum_{i=1}^{n} \log (1 + e^{-y_i \mathbf{w}^T \mathbf{x}_i}) + \lambda \mathbf{w}^T \mathbf{w}" /></a> to find the optimal parameters <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{w}" title="\mathbf{w}" /></a>.

# Summary

Logistic Regression is the discriminative counterpart to Naive Bayes. In Naive Bayes, we first model <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)" title="P(\mathbf{x} \vert y)" /></a> for each label <a href="https://www.codecogs.com/eqnedit.php?latex=y" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y" title="y" /></a>, and then obtain the decision boundary that best discriminates between these two distributions. In Logistic Regression we do not attempt to model the data distribution <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)" title="P(\mathbf{x} \vert y)" /></a>, instead, we model <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x})" title="P(y \vert \mathbf{x})" /></a> directly. We assume the same probabilistic form <a href="https://www.codecogs.com/eqnedit.php?latex=P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(y&space;\vert&space;\mathbf{x}_i)=\frac{1}{1&plus;e^{-y(\mathbf{w}^T&space;\mathbf{x}_i&space;&plus;&space;b)}}" title="P(y \vert \mathbf{x}_i)=\frac{1}{1+e^{-y(\mathbf{w}^T \mathbf{x}_i + b)}}" /></a>, but we do not restrict ourselves in any way by making assumptions about <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)" title="P(\mathbf{x} \vert y)" /></a>. This allows logistic regression to be more flexible, but such flexibility also requires more data to avoid overfitting. Typically, in scenarios with little data and if the modelling assumption is appropriate, Naive Bayes tends to outperform Logistic Regression. However, as data sets becomes large Logistic Regression often outperforms Naive Bayes, which suffers from the fact that the assumptions made on <a href="https://www.codecogs.com/eqnedit.php?latex=P(\mathbf{x}&space;\vert&space;y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(\mathbf{x}&space;\vert&space;y)" title="P(\mathbf{x} \vert y)" /></a> are probably not exactly correct. If the assumptions hold exactly, i.e. the data is truly drawn from the distribution that we assumed in Naive Bayes, then Logistic Regression and Naive Bayes converge to the exact same result in the limit (but NB will be faster).




